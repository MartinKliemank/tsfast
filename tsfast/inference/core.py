# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/05_inference/00_core.ipynb.

# %% auto 0
__all__ = ['InferenceWrapper']

# %% ../../nbs/05_inference/00_core.ipynb 2
from ..datasets.core import extract_mean_std_from_dls
from ..data.loader import reset_model_state
from ..models.layers import NormalizedModel
import numpy as np
import torch

# %% ../../nbs/05_inference/00_core.ipynb 3
class InferenceWrapper:
    """
    A wrapper class to simplify inference with a trained tsfast/fastai Learner
    on NumPy data. Handles normalization and state reset automatically.
    """
    def __init__(self, learner):
        """
        Initializes the inferencer.

        Args:
            learner: The trained tsfast/fastai Learner object.
        """
        if not hasattr(learner, 'model') or not hasattr(learner, 'dls'):
            raise TypeError("Input 'learner' object does not appear to be a valid fastai/tsfast Learner.")

        self.device = learner.dls.device
        self.core_model = learner.model.to(self.device)

        # Extract normalization stats
        mean, std = extract_mean_std_from_dls(learner.dls)
        if mean is None or std is None:
             raise ValueError("Could not extract mean/std from learner's DataLoaders. Ensure normalization was used during training.")

        # Create and store the NormalizedModel
        self.norm_model = NormalizedModel(self.core_model, mean, std).to(self.device)
        self.norm_model.eval() # Set to evaluation mode

    def inference(self, numpy_data: np.ndarray) -> np.ndarray:
        # Add batch dimension if needed
        if numpy_data.ndim == 2:
            numpy_data_batched = np.expand_dims(numpy_data, axis=0)
        elif numpy_data.ndim == 3 and numpy_data.shape[0] == 1:
            numpy_data_batched = numpy_data
        else:
             raise ValueError(f"Input data should have 2 dimensions [seq_len, features] or 3 dimensions [1, seq_len, features]. Provided shape: {numpy_data.shape}")

        input_tensor = torch.from_numpy(numpy_data_batched).float().to(self.device)

        output_tensor = None
        with torch.no_grad():
            reset_model_state(self.core_model)
            model_output = self.norm_model(input_tensor)

            # Handle tuple outputs
            if isinstance(model_output, tuple):
                output_tensor = model_output[0]
            else:
                output_tensor = model_output
        if output_tensor is None:
            raise RuntimeError("Model did not return a valid output tensor.")

        return output_tensor.squeeze(0).cpu().numpy()

    def __call__(self, numpy_data: np.ndarray) -> np.ndarray:
        """Allows calling the predictor instance like a function."""
        return self.inference(numpy_data)
