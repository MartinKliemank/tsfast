{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext line_profiler\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corefunctions\n",
    "> Corefunctionality for data preparation of sequential data for pytorch, fastai models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Structure\n",
    "The data will be extracted and prepared via transforms. Those are grouped in:\n",
    "- Type Transforms: Those extraxt the needed components from the source items, like input sequences or target scalar values. The work on single tensors.\n",
    "- Item Transforms: Those Transforms may work on tuple level and therefore may process relationships between input and output.\n",
    "- Batch Transform: Those transforms work on batch level. They receive batched tensors and may apply lazy transforms like normalization very effeciently.\n",
    "\n",
    "An application example may look like the following:\n",
    "- sourceitems: \n",
    "    - path extraction with hdf5 file endings\n",
    "    - create pandas dataframe with information for type transforms, like slices\n",
    "    - filter items in pandas dataframe\n",
    "- type transforms: \n",
    "    - extract hdf5 input and output sequence\n",
    "    - create windows\n",
    "- item transforms: \n",
    "    - filter sequence by value\n",
    "    - shift output sequence by 1 element\n",
    "- batch transforms: \n",
    "    - noise injection\n",
    "    - normalization\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.data.all import *\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def obj_in_lst(lst,cls):\n",
    "    '''retrieve first object of type cls from a list'''\n",
    "    return next(o for o in lst if type(o) is cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def count_parameters(model):\n",
    "    '''retrieve number of trainable parameters of a model'''\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Source Items\n",
    "The file paths may be extracted with `get_files` of fastai2. `get_hdf_files` removes the need of writing the hdf5 file extension.\n",
    "\n",
    "Then a pandas dataframe may be created in case further information for the source items need to be stored like slices for the windowing function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Extract File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, Path('test_data/train/Sim_RealisticCycle1.hdf5'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_path = 'test_data/'\n",
    "hdf_files = get_files(f_path,extensions='.hdf5',recurse=True)\n",
    "len(hdf_files),hdf_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "hdf_extensions = ['.hdf5']\n",
    "def get_hdf_files(path,recurse=True, folders=None):\n",
    "    \"Get hdf5 files in `path` recursively, only in `folders`, if specified.\"\n",
    "    return get_files(path, extensions=hdf_extensions, recurse=recurse, folders=folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, Path('test_data/train/Sim_RealisticCycle1.hdf5'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf_files = get_hdf_files(f_path)\n",
    "len(hdf_files),hdf_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create Source Dictionaries\n",
    "In order to extract mulitple realizations of one file with different modifications, we create a list of properties. Pandas Dataframes are to slow for iteration but very fast and convenient for creations. So after creation of the pandas Dataframe we convert it to a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def apply_df_tfms(f_list,pd_tfms = None):\n",
    "    '''Create Pandas Dataframe out of a list of items, with a list of df transforms applied'''\n",
    "    df = pd.DataFrame(data=f_list.items,columns=['path'],dtype=str)\n",
    "    if pd_tfms is not None:\n",
    "        for t in pd_tfms:\n",
    "            df = t(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path\n",
       "0  test_data/train/Sim_RealisticCycle1.hdf5\n",
       "1  test_data/train/Sim_RealisticCycle2.hdf5\n",
       "2  test_data/valid/Sim_RealisticCycle3.hdf5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = apply_df_tfms(hdf_files)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def CreateDict(pd_tfms = None):\n",
    "    '''Create List of Dictionarys out of a list of items, with a list of df transforms applied'''\n",
    "    def _inner(f_list):\n",
    "        return apply_df_tfms(f_list,pd_tfms).to_dict(orient='records')\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': 'test_data/train/Sim_RealisticCycle1.hdf5'},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle2.hdf5'},\n",
       " {'path': 'test_data/valid/Sim_RealisticCycle3.hdf5'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_dict =CreateDict()(hdf_files)\n",
    "l_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ValidClmContains(lst_valid):\n",
    "    '''add validation column using a list of strings that are part of the validation frames'''\n",
    "    def _inner(df):\n",
    "        re_valid = '|'.join([re.escape(f) for f in lst_valid])\n",
    "        df['valid'] = df.path.str.contains(re_valid)\n",
    "        return df\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 2.95 ms, total: 2.95 ms\n",
      "Wall time: 2.94 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'path': 'test_data/train/Sim_RealisticCycle1.hdf5', 'valid': False},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle2.hdf5', 'valid': False},\n",
       " {'path': 'test_data/valid/Sim_RealisticCycle3.hdf5', 'valid': True}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lst_valid = ['valid']\n",
    "CreateDict([ValidClmContains(lst_valid)])(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ValidClmIs(lst_valid):\n",
    "    '''adds validation column using a list of validation filenames'''\n",
    "    def _inner(df):\n",
    "        df['valid'] = df.path.isin([str(f) for f in lst_valid])\n",
    "        return df\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.84 ms, sys: 864 µs, total: 2.71 ms\n",
      "Wall time: 2.68 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'path': 'test_data/train/Sim_RealisticCycle1.hdf5', 'valid': False},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle2.hdf5', 'valid': True},\n",
       " {'path': 'test_data/valid/Sim_RealisticCycle3.hdf5', 'valid': True}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lst_valid = ['test_data/train/Sim_RealisticCycle2.hdf5','test_data/valid/Sim_RealisticCycle3.hdf5']\n",
    "CreateDict([ValidClmIs(lst_valid)])(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def FilterClm(clm_name,func = lambda x:x):\n",
    "    '''adds validation column using a list of validation filenames'''\n",
    "    def _inner(df):\n",
    "        return df[func(df[clm_name])]\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': 'test_data/train/Sim_RealisticCycle2.hdf5', 'valid': True},\n",
       " {'path': 'test_data/valid/Sim_RealisticCycle3.hdf5', 'valid': True}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CreateDict([ValidClmIs(lst_valid),FilterClm('valid')])(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def DfHDFCreateWindows(win_sz,stp_sz, clm, fixed_start = False, fixed_end = False,keep_filter=None):\n",
    "    '''create windows of sequences, splits sequence into multiple items'''\n",
    "    def _inner(df):\n",
    "        if fixed_start and fixed_end: raise Exception\n",
    "            \n",
    "        pd.options.mode.chained_assignment = None #every row is a reference so we need to suppress the warning messages while copying\n",
    "            \n",
    "        df['l_slc'] = None\n",
    "        df['r_slc'] = None\n",
    "\n",
    "        lst_df = [] #new dataframe for every row\n",
    "        if keep_filter is not None:\n",
    "            keep_idxs = ~df.path.astype(str).str.contains(keep_filter)\n",
    "        \n",
    "        \n",
    "        for idx, row in (df if keep_filter is None else df[keep_idxs]).iterrows():\n",
    "            with h5py.File(row.path,'r') as f:\n",
    "                #TODO make clm optional\n",
    "#                 if clm == '': \n",
    "#                     clm = list(f.keys())[0]\n",
    "                f_len = max(f[clm].shape)\n",
    "\n",
    "                n_win = ((f_len-win_sz)//stp_sz)+1\n",
    "                tmp_df = df.iloc[[idx]*n_win]; #duplicate the row of the df multiple times by reference\n",
    "                lst_idx = np.arange(n_win)\n",
    "\n",
    "                \n",
    "                if not fixed_start: tmp_df['l_slc'] = lst_idx*stp_sz\n",
    "                if not fixed_end: tmp_df['r_slc'] = lst_idx*stp_sz + win_sz\n",
    "\n",
    "                lst_df.append(tmp_df)\n",
    "                \n",
    "        \n",
    "        pd.options.mode.chained_assignment = 'warn'\n",
    "                \n",
    "        if keep_filter is not None: \n",
    "            raw_df = df[~keep_idxs].copy()\n",
    "            lst_df.append(raw_df)\n",
    "        \n",
    "        res_df = pd.concat(lst_df,sort=False)\n",
    "        return res_df\n",
    "    \n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>400</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265000</td>\n",
       "      <td>265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265100</td>\n",
       "      <td>265200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265200</td>\n",
       "      <td>265300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265300</td>\n",
       "      <td>265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265400</td>\n",
       "      <td>265500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7966 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        path   l_slc   r_slc\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5       0     100\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5     100     200\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5     200     300\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5     300     400\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5     400     500\n",
       "..                                       ...     ...     ...\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5  265000  265100\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5  265100  265200\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5  265200  265300\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5  265300  265400\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5  265400  265500\n",
       "\n",
       "[7966 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "create_win = DfHDFCreateWindows(win_sz=100,stp_sz=100,clm='current')\n",
    "win_df = create_win(df)\n",
    "win_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path  l_slc  r_slc\n",
       "0  test_data/train/Sim_RealisticCycle1.hdf5      0    100\n",
       "0  test_data/train/Sim_RealisticCycle1.hdf5    100    200\n",
       "0  test_data/train/Sim_RealisticCycle1.hdf5    200    300\n",
       "1  test_data/train/Sim_RealisticCycle2.hdf5      0    100\n",
       "1  test_data/train/Sim_RealisticCycle2.hdf5    100    200\n",
       "1  test_data/train/Sim_RealisticCycle2.hdf5    200    300\n",
       "2  test_data/valid/Sim_RealisticCycle3.hdf5      0    100\n",
       "2  test_data/valid/Sim_RealisticCycle3.hdf5    100    200\n",
       "2  test_data/valid/Sim_RealisticCycle3.hdf5    200    300"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_expr = 'l_slc <= 200'\n",
    "filt_df = win_df.query(query_expr)\n",
    "filt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def DfFilterQuery(query):\n",
    "    def _inner(df):\n",
    "        return df.query(query)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(DfFilterQuery(query_expr)(win_df),filt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 528 ms, sys: 4.73 ms, total: 533 ms\n",
      "Wall time: 531 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'path': 'test_data/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 0,\n",
       "  'r_slc': 101},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 10,\n",
       "  'r_slc': 111},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 20,\n",
       "  'r_slc': 121},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 30,\n",
       "  'r_slc': 131},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 40,\n",
       "  'r_slc': 141}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tfm_src = CreateDict([ValidClmContains(['valid']),DfHDFCreateWindows(win_sz=100+1,stp_sz=10,clm='current')])\n",
    "src_dicts = tfm_src(hdf_files)\n",
    "src_dicts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert Paths to Sequence Objects\n",
    "Der Pfad wird unter Angabe der Spaltennamen in Sequenzen und Skalare Werte umgewandelt, um so am Ende ein 3-Tupel zu erhalten aus:\n",
    "- (Sequence, Scalar, Sequence) <-> (input,input,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extract sequential data from hdf5-files\n",
    "Two different functions, based on pandas df and on lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Shift time Series\n",
    "Sometimes we need to shift columns of a sequence by a specific value. Then we cant simply slice the array but have to handle each column individually. First a performance test has to be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calc_shift_offsets(clm_shift):\n",
    "    clm_shift = array(clm_shift)\n",
    "    l_offs = -min(clm_shift.min(),0)\n",
    "    r_offs = -max(clm_shift.max(),0)\n",
    "    l_shift = clm_shift+l_offs\n",
    "    r_shift = clm_shift+r_offs\n",
    "    dim_red = l_offs-r_offs\n",
    "    return l_shift,r_shift,dim_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 0, 2]), array([-1, -1, -2,  0]), 2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shft = [0,0,-1,1]\n",
    "calc_shift_offsets(shft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both shifting methods have their own performance character. vstack needs double the time on short sequences, while the creation of a seperate array with copy becomes worse starting at around 5000 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ta = array([[1,2,3]*2]*10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# y = np.vstack([ta[i:-ta.shape[1]+i,i] for i in range(ta.shape[1])]).T   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# x = np.zeros((ta.shape[0]-ta.shape[1],ta.shape[1]))\n",
    "# for i in range(ta.shape[1]):\n",
    "#     x[:,i] = ta[i:-ta.shape[1]+i,i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 HDF2Sequence\n",
    "HDF5 performance is massively affected by the dtype of the signals. f4 (32 bit floating point) Numbers are faster to load and lead to smaller files then f8 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0,axis=0),axis=0) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def downsample_mean(x,N):\n",
    "    shp = x.shape\n",
    "    trunc = -(x.shape[0] % N)\n",
    "    trunc = trunc if trunc != 0 else None\n",
    "    return x[:trunc,:].reshape((-1,N,x.shape[-1])).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hdf_extract_sequence(hdf_path,clms,dataset = None, l_slc = None, r_slc= None):\n",
    "    with h5py.File(hdf_path,'r') as f:\n",
    "        ds = f if dataset is None else f[dataset]\n",
    "        l_array = [ds[n][l_slc:r_slc] for n in clms]\n",
    "        seq = np.stack(l_array,axis=-1)\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Memoize:\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn\n",
    "        self.memo = {}\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        if args not in self.memo:\n",
    "            self.memo[args] = self.fn(*args)\n",
    "        return self.memo[args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HDF2Sequence(Transform):\n",
    "    \n",
    "    def __init__(self, clm_names,clm_shift=None,truncate_sz=None,to_cls=noop,cached=True):\n",
    "        if not clm_shift is None:\n",
    "            assert len(clm_shift)==len(clm_names) and all(isinstance(n, int) for n in clm_shift)\n",
    "            self.l_shift,self.r_shift,_ = calc_shift_offsets(clm_shift)\n",
    "        \n",
    "        self._exseq = Memoize(self._hdf_extract_sequence) if cached else self._hdf_extract_sequence\n",
    "        \n",
    "        store_attr(self,'clm_names,clm_shift,truncate_sz,to_cls,cached')\n",
    "        \n",
    "    def _hdf_extract_sequence(self,hdf_path,dataset = None, l_slc = None, r_slc= None):\n",
    "        with h5py.File(hdf_path,'r') as f:\n",
    "            ds = f if dataset is None else f[dataset]\n",
    "            l_array = [(ds[n][l_slc:r_slc]) for n in self.clm_names]\n",
    "            seq = np.stack(l_array,axis=-1)\n",
    "            return seq.astype('f8')#workaround for random bug, that mitigates convergence if the numpy array is an f4 array. Seems to make no sense because the result does not change.\n",
    "    \n",
    "    def _extract_dict_sequence(self,item):\n",
    "        if isinstance(item,dict):\n",
    "            path = item['path']\n",
    "            dataset = item['dataset'] if 'dataset' in item else None\n",
    "            l_slc = item['l_slc'] if 'l_slc' in item else None\n",
    "            r_slc = item['r_slc'] if 'r_slc' in item else None\n",
    "\n",
    "            if self.cached:\n",
    "                seq = self._exseq(path,dataset,None,None)[l_slc:r_slc]\n",
    "            else:\n",
    "                seq = self._exseq(path,dataset,l_slc,r_slc)\n",
    "        else:\n",
    "            seq = self._exseq(str(item),None,None,None)\n",
    "\n",
    "        #shift clms of result by given value \n",
    "        if not self.clm_shift is None:\n",
    "            l_seq = seq.shape[0]\n",
    "            seq = np.stack([seq[self.l_shift[i]:l_seq+self.r_shift[i],i] for i in range(seq.shape[1])],axis=-1)\n",
    "            \n",
    "        if not self.truncate_sz is None:\n",
    "            seq = seq[truncate_sz:]\n",
    "        \n",
    "        #it is important to slice first and then do the class conversion\n",
    "        return self.to_cls(seq) \n",
    "\n",
    "    def encodes(self, item)->None: \n",
    "        return self._extract_dict_sequence(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  4.18735027],\n",
       "       [-0.1       ,  4.18935013],\n",
       "       [-0.1       ,  4.18969536],\n",
       "       ...,\n",
       "       [ 8.83880043,  3.39321232],\n",
       "       [ 8.84599972,  3.39287138],\n",
       "       [ 8.85309982,  3.3925302 ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "hdf2seq = HDF2Sequence(['current','voltage'],cached=False)\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1       ,  4.18935013],\n",
       "       [-0.1       ,  4.18969536],\n",
       "       [-0.1       ,  4.19003677],\n",
       "       ...,\n",
       "       [ 8.83880043,  3.39321232],\n",
       "       [ 8.84599972,  3.39287138],\n",
       "       [ 8.85309982,  3.3925302 ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf2seq = HDF2Sequence(['current','voltage'],clm_shift=[1,1])\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSequencesInput([[ 0.0000,  4.1874],\n",
       "        [-0.1000,  4.1894],\n",
       "        [-0.1000,  4.1897],\n",
       "        ...,\n",
       "        [ 8.8388,  3.3932],\n",
       "        [ 8.8460,  3.3929],\n",
       "        [ 8.8531,  3.3925]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf2seq = HDF2Sequence(['current','voltage'],cached=False,to_cls=TensorSequencesInput)\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.1 ms ± 283 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf2seq = HDF2Sequence(['current','voltage'],cached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  4.18735027],\n",
       "       [-0.1       ,  4.18935013],\n",
       "       [-0.1       ,  4.18969536],\n",
       "       ...,\n",
       "       [ 8.83880043,  3.39321232],\n",
       "       [ 8.84599972,  3.39287138],\n",
       "       [ 8.85309982,  3.3925302 ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion lässt sich mittels Pipeline auf eine Liste von Quellobjekten (hier Pfade) anwenden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265607, 1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf2seq = HDF2Sequence(['current'])\n",
    "hdf2seq(hdf_files[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(HDF2Sequence(['current','voltage']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_pipe = pipe(hdf_files)\n",
    "# len(res_pipe), res_pipe[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hdf2scalars(hdf_path,c_names):\n",
    "    with h5py.File(hdf_path,'r') as f:\n",
    "#         import pdb; pdb.set_trace()\n",
    "#         l_array = [f[n][:][:,None] for n in c_names]\n",
    "#         seq = np.concatenate(l_array,axis=1)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Test\n",
    "Caching stores the arrays for future use at every function call. Very usefull, especially for windows. Should allways be turned. Only explicitly turn it off when there is not enough memory for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms=[  [HDF2Sequence(['current','voltage'],cached=False)],\n",
    "        [HDF2Sequence(['voltage'],cached=False)]]\n",
    "dsrc = Datasets(src_dicts[:1000],tfms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dsrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for x in dsrc:\n",
    "#     x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms=[  [HDF2Sequence(['current','voltage'],cached=True,clm_shift=[1,2])],\n",
    "        [HDF2Sequence(['voltage'],cached=True)]]\n",
    "dsrc = Datasets(src_dicts[:1000],tfms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "for x in dsrc:\n",
    "    x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching is way faster because every file gets loaded multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Datatypes for Sequences and Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TensorSequences(TensorBase):#TensorBase\n",
    "#     def __init__(self,x,c_names=None, **kwargs):\n",
    "#         super().__init__()\n",
    "#         self.c_names = c_names\n",
    "    \n",
    "    def show(self, ctx=None, **kwargs):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        ax = ctx\n",
    "        if ax is None: _,ax = plt.subplots()\n",
    "        ax.plot(self)\n",
    "#         if title is not None: ax.set_title(title)\n",
    "        return ax\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(HDF2Sequence, keep=True)\n",
    "    def from_hdf(cls,clm_names,**kwargs):\n",
    "        return HDF2Sequence(clm_names,**kwargs)\n",
    "    \n",
    "class TensorSequencesInput(TensorSequences): pass\n",
    "class TensorSequencesOutput(TensorSequences): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = TensorSequencesInput.from_hdf(['current'])\n",
    "type(f(hdf_files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorSequences(np.ones((30,2))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@Transform\n",
    "def toTensorSequencesInput(o): return TensorSequencesInput(o)\n",
    "@Transform\n",
    "def toTensorSequencesOutput(o): return TensorSequencesOutput(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TensorScalars(Tensor): pass\n",
    "class TensorScalarsInput(TensorScalars): pass\n",
    "class TensorScalarsOutput(TensorScalars): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Sequence Slicing Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class SeqSlice(Transform):\n",
    "    '''Take a slice from an array-like object. Useful for e.g. shifting input and output'''\n",
    "    def __init__(self, l_slc=None,r_slc=None):\n",
    "        self.l_slc,self.r_slc = l_slc,r_slc\n",
    "        \n",
    "    def encodes(self, o): return o[self.l_slc:self.r_slc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_shift = SeqSlice(r_slc=-1)\n",
    "arr = np.ones((5))\n",
    "test_eq(l_shift(arr),arr[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sequence Noise Injection Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqNoiseInjection(Transform):\n",
    "    split_idx=0\n",
    "    '''Adds normal distributed noise to the tensor sequence with seperate mean and std for every signal'''\n",
    "    def __init__(self, std=1e-1,mean=0.):\n",
    "        self.std,self.mean = tensor(std),tensor(mean)\n",
    "        \n",
    "    def setups(self, dl:DataLoader):\n",
    "        #check the tensor type of your input\n",
    "        #TODO: include scalar type case\n",
    "        x,*_ = dl.one_batch()\n",
    "        self.std = to_device(self.std,x.device)\n",
    "        self.mean = to_device(self.mean,x.device)\n",
    "        \n",
    "    def encodes(self, o:TensorSequencesInput): \n",
    "        #expand creates a view on a tensor and is therefore very fast compared to copy\n",
    "        return o+torch.normal(mean=self.mean.expand_as(o), \n",
    "                              std=self.std.expand_as(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSequencesInput([[ 1.,  1.,  1.],\n",
       "         [-1., -1., -1.]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = TensorSequencesInput(tensor([[1,1,1],[-1,-1,-1.0]]))\n",
    "ns_mean = tensor([0.,10.1,3.1])\n",
    "ns_std = tensor([1.,1.1,0.1])\n",
    "x,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSequencesInput([[ 1.,  1.,  1.],\n",
       "        [-1., -1., -1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_noise = SeqNoiseInjection(std=ns_std,mean=ns_std)\n",
    "seq_noise(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSequencesInput([[ 1.,  1.,  1.],\n",
       "        [-1., -1., -1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_noise = SeqNoiseInjection(std=ns_std*10)\n",
    "seq_noise(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sequence Bias Injection Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqBiasInjection(Transform):\n",
    "    split_idx=0\n",
    "    '''Adds a normal distributed offset to the tensor sequence with seperate mean and std for every signal'''\n",
    "    def __init__(self, std=1e-1,mean=0.):\n",
    "        self.std,self.mean = tensor(std),tensor(mean)\n",
    "        \n",
    "    def setups(self, dl:DataLoader):\n",
    "        #check the tensor type of your input\n",
    "        #TODO: include scalar type case\n",
    "        x,*_ = dl.one_batch()\n",
    "        self.std = to_device(self.std,x.device)\n",
    "        self.mean = to_device(self.mean,x.device)\n",
    "        \n",
    "    def encodes(self, o:TensorSequencesInput): \n",
    "        #expand creates a view on a tensor and is therefore very fast compared to copy\n",
    "        mean=self.mean.repeat((o.shape[0],1,1)).expand((o.shape[0],1,o.shape[2]))\n",
    "        std= self.std.repeat((o.shape[0],1,1)).expand((o.shape[0],1,o.shape[2]))\n",
    "        n = torch.normal(mean=mean, std=std).expand_as(o)\n",
    "        return o+n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1., -1.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_bias = SeqBiasInjection(std=ns_std,mean=ns_std)\n",
    "seq_bias(x)[...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.1000, 0.1000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_bias.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSequencesInput([[ 1.,  1.,  1.],\n",
       "        [-1., -1., -1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_bias = SeqBiasInjection(std=ns_std*10)\n",
    "seq_bias(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Normalization\n",
    "`Normalize` is programmed for `TensorImage` as an input tensor. It gets. At init the variable axes need to be chosen correspondingly to the shape of your tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@Normalize\n",
    "def encodes(self, x:TensorSequencesInput): \n",
    "    return (x-self.mean) / self.std\n",
    "\n",
    "@Normalize\n",
    "def decodes(self, x:TensorSequencesInput):\n",
    "    f = to_cpu if x.device.type=='cpu' else noop\n",
    "    return (x*f(self.std) + f(self.mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSequencesInput([[ 1.,  1.,  1.],\n",
       "         [-1., -1., -1.]]),\n",
       " TensorSequencesInput([[  1.0000,  -8.2727, -21.0000],\n",
       "         [ -1.0000, -10.0909, -41.0000]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = Normalize.from_stats(mean=ns_mean,std=ns_std,dim=1,ndim=2,cuda=False)\n",
    "x,norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split in Training, Validation\n",
    "Splitting kann anhand von vorher bekannten Indizes, dem Dateipfad oder anderen allgemeinen Funktion durchgeführt werden.\n",
    "\n",
    "Splitting innerhalb einer Sequenzen sollte in der Praxis nur dann geschehen wenn eine einzige Sequenz vorhanden ist. Diese kann dann vorher manuell geteilt werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Splitting mit vorgegebenem Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = IndexSplitter([1,2])\n",
    "test_eq(splitter(hdf_files),[[0],[1,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Splitting mit allgemeiner Funktion\n",
    "Items, bei denen die definierte Funktion `True` zurück gibt, werden den Validierungsdatensatz zugeordnet, der Rest dem Training. In diesem Fall wird nach dem Übergeordneten Ordnernamen gesucht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = FuncSplitter(lambda o: Path(o).parent.name == 'valid')\n",
    "test_eq(splitter(hdf_files),[[0,1],[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Splitting anhand des Parent-Folders\n",
    "Splitter, der Explizit Training und Validierungsordner den Datensätzen zuordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _parent_idxs(items, name): return mask2idxs(Path(o).parent.name == name for o in items)\n",
    "\n",
    "def ParentSplitter(train_name='train', valid_name='valid'):\n",
    "    \"Split `items` from the parent folder names (`train_name` and `valid_name`).\"\n",
    "    def _inner(o, **kwargs):\n",
    "        return _parent_idxs(o, train_name),_parent_idxs(o, valid_name)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ParentSplitter()\n",
    "test_eq(splitter(hdf_files),[[0,1],[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Percentage Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def PercentageSplitter(pct=0.8):\n",
    "    \"Split `items` in order in relative quantity.\"\n",
    "    def _inner(o, **kwargs):\n",
    "        split_idx=int(len(o)*pct)\n",
    "        return L(range(split_idx)),L(range(split_idx,len(o)))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = PercentageSplitter(0.7)\n",
    "test_eq(splitter(hdf_files),[[0,1],[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Apply To Dictionary\n",
    "In Case of the Datablock API your items are a list of dictionaries. If you want to apply a Splitter to the path stored within you need a wrapper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ApplyToDict(fn,key='path'):\n",
    "    return lambda x:fn([i[key] for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Valid Column\n",
    "Using the 'valid' column of the Dataframe that has been created by a transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "valid_clm_splitter =  FuncSplitter(lambda o:o['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#53101) [0,1,2,3,4,5,6,7,8,9...],\n",
       " (#26550) [53101,53102,53103,53104,53105,53106,53107,53108,53109,53110...])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_clm_splitter(src_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataloaders Creation\n",
    "A Datasets combines all implemented components on item level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pad_sequence(batch):\n",
    "    '''collate_fn for padding of sequences of different lengths, use in before_batch of databunch, still quite slow'''\n",
    "    #takes list of tuples as input, returns list of tuples\n",
    "    sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n",
    "\n",
    "    pad_func = partial(torch.nn.utils.rnn.pad_sequence,batch_first=True)\n",
    "    padded_tensors = [pad_func([x[tup] for x in sorted_batch]) for tup in range(len(batch[0]))]\n",
    "    padded_list = [retain_types(tuple([tup[entry] for tup in padded_tensors]),batch[0]) for entry in range(len(batch))]\n",
    "    #retain types is important for decoding later back to source items\n",
    "#     import pdb; pdb.set_trace()\n",
    "    \n",
    "    return padded_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Low-Level with Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms=[  [HDF2Sequence(['current','voltage']),SeqSlice(l_slc=1),toTensorSequencesInput],\n",
    "        [HDF2Sequence(['voltage']),SeqSlice(r_slc=-1),toTensorSequencesOutput]]\n",
    "splits = splitter([x['path'] for x in src_dicts])\n",
    "dsrc = Datasets(src_dicts,tfms=tfms,splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# dsrc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 100, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = dsrc.dataloaders(bs=128,after_batch=[SeqNoiseInjection(std=[1.1,0.01]),Normalize(axes=[0,1])],before_batch=pad_sequence)\n",
    "db.one_batch()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Mid-Level with Datablock API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequenceBlock(TransformBlock):\n",
    "    def __init__(self, seq_extract,padding=False):\n",
    "        return super().__init__(type_tfms=[seq_extract],\n",
    "                                batch_tfms=[Normalize(axes=[0,1])],\n",
    "                                dls_kwargs={} if not padding else {'before_batch': pad_sequence})\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(HDF2Sequence, keep=True)\n",
    "    def from_hdf(cls, clm_names, seq_cls=TensorSequencesInput,padding=False, **kwargs):\n",
    "        return cls(HDF2Sequence(clm_names,to_cls=seq_cls,**kwargs), padding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = DataBlock(blocks=(SequenceBlock.from_hdf(['current','voltage'],TensorSequencesInput,padding=True,cached=False),\n",
    "                        SequenceBlock.from_hdf(['voltage'],TensorSequencesOutput,cached=False)),\n",
    "                get_items=tfm_src,\n",
    "                splitter=ApplyToDict(ParentSplitter()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = seq.dataloaders(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = pickle.dumps(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 High-Level with SequenceDataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.utils.data import Dataset,ConcatDataset\n",
    "class Seq2SeqDS(Dataset):\n",
    "    #Datensatz aus HDF5 Datei \n",
    "\n",
    "    def __init__(self, hdf_file, u_clms,y_clms, win_sz,stp_sz):\n",
    "        super().__init__()\n",
    "        store_attr(self,'u_clms,y_clms,win_sz,stp_sz')\n",
    "        \n",
    "        self.u = hdf_extract_sequence(hdf_file,u_clms)\n",
    "        self.y = hdf_extract_sequence(hdf_file,y_clms)\n",
    "        \n",
    "#         if clm_shift is not None:\n",
    "#             assert len(clm_shift[0])==len(u_clms) and len(clm_shift[1])==len(y_clms)\n",
    "#             l_shift,r_shift,dim_red = calc_shift_offsets(clm_shift)\n",
    "        \n",
    "        self.len = (self.u.shape[0]-self.win_sz)//self.stp_sz or 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not (0 <= idx < len(self)): raise IndexError\n",
    "#         import pdb; pdb.set_trace()\n",
    "        i = idx*self.stp_sz + self.win_sz\n",
    "        return (TensorSequencesInput(self.u[(i-self.win_sz+1):i+1]),\n",
    "                TensorSequencesOutput(self.y[(i-self.win_sz+1):i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(TfmdDL, keep=True)\n",
    "def Seq2SeqDataloaders(items,u_clms,y_clms, win_sz,stp_sz=1,splitter=None,**kwargs):\n",
    "    if splitter is None:\n",
    "        lst_items = [items]\n",
    "    else:\n",
    "        lst_items = [items[idx] for idx in splitter(items)]\n",
    "        \n",
    "    dss = [ConcatDataset([Seq2SeqDS(f,u_clms,y_clms, win_sz,stp_sz) for f in itms])\n",
    "          for itms in lst_items]\n",
    "    if not 'after_batch' in kwargs: kwargs['after_batch'] = [to_device,Normalize(axes=[0,1])]\n",
    "    \n",
    "    dls = [TfmdDL(ds,shuffle=(i == 0),**kwargs) for i,ds in enumerate(dss)]\n",
    "    return DataLoaders(*dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls=Seq2SeqDataloaders(hdf_files,['current','voltage'],['voltage'],100,10,\n",
    "                       splitter=ParentSplitter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Show Batches and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_sequence(axs,in_sig,targ_sig,out_sig=None,**kwargs):\n",
    "    n_targ = targ_sig.shape[1]\n",
    "    for j,ax in  enumerate(axs[:-1]):\n",
    "        ax.plot(targ_sig[:,j])\n",
    "        if out_sig is not None: \n",
    "            ax.plot(out_sig[:,j])\n",
    "            ax.legend(['y','ŷ'])\n",
    "            if 'ref' in kwargs:\n",
    "                ax.plot(kwargs['ref'][:,j]) \n",
    "        ax.label_outer()\n",
    "    axs[-1].plot(in_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_seqs_single_figure(n_samples,n_targ,samples,plot_func,outs=None,**kwargs):\n",
    "    rows=max(1,((n_samples-1) // 3)+1)\n",
    "    cols=min(3,n_samples)\n",
    "    fig = plt.figure(figsize=(9,2*cols))\n",
    "    outer_grid = fig.add_gridspec(rows, cols)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    for i in range(n_samples):\n",
    "        in_sig = samples[i][0]\n",
    "        targ_sig = samples[i][1]\n",
    "        if outs is not None: out_sig = outs[i][0]\n",
    "        inner_grid = outer_grid[i].subgridspec(n_targ+1, 1)\n",
    "        axs = [fig.add_subplot(inner_grid[j]) for j in range(n_targ+1)]\n",
    "        plot_func(axs,in_sig,targ_sig,out_sig=out_sig if outs is not None else None,**kwargs)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_seqs_multi_figures(n_samples,n_targ,samples,plot_func,outs=None,**kwargs):\n",
    "    for i in range(n_samples):\n",
    "        fig = plt.figure(figsize=(9,3))\n",
    "        axs = fig.subplots(nrows=n_targ+1,sharex=True)\n",
    "        in_sig = samples[i][0]\n",
    "        targ_sig = samples[i][1]\n",
    "        if outs is not None:  out_sig = outs[i][0]\n",
    "            \n",
    "        plot_func(axs,in_sig,targ_sig,out_sig=out_sig if outs is not None else None,**kwargs)\n",
    "        \n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x:TensorSequences, y:TensorSequences, samples, ctxs=None, max_n=6, **kwargs):\n",
    "    n_samples = min(len(samples), max_n)\n",
    "    n_targ = samples[0][1].shape[1]\n",
    "    if n_samples > 3:\n",
    "        #if there are more then 3 samples to plot then put them in a single figure\n",
    "        plot_seqs_single_figure(n_samples,n_targ,samples,plot_sequence, **kwargs)\n",
    "    else:\n",
    "        #if there are less then 3 samples to plot then put each in its own figure\n",
    "        plot_seqs_multi_figures(n_samples,n_targ,samples,plot_sequence, **kwargs)\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5aa9d215cc4cb88acf5ff6ec3f36e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5face94445420c88d81089ca12b74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d6c2a27b6a4e9bb22cc98c64432b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x:TensorSequences, y:TensorSequences, samples, outs, ctxs=None, max_n=2, **kwargs):\n",
    "    n_samples = min(len(samples), max_n)\n",
    "    n_targ = samples[0][1].shape[1]\n",
    "    if n_samples > 3:\n",
    "        #if there are more then 3 samples to plot then put them in a single figure\n",
    "        plot_seqs_single_figure(n_samples,n_targ,samples,plot_sequence,outs, **kwargs)\n",
    "    else:\n",
    "        #if there are less then 3 samples to plot then put each in its own figure\n",
    "        plot_seqs_multi_figures(n_samples,n_targ,samples,plot_sequence,outs, **kwargs)\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_model.ipynb.\n",
      "Converted 02_learner.ipynb.\n",
      "Converted 03_dataloaders.ipynb.\n",
      "Converted 11_dualrnn.ipynb.\n",
      "Converted 12_TensorQuaternions.ipynb.\n",
      "Converted 13_HPOpt.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
