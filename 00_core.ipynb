{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codestructure\n",
    "\n",
    "1. Extract File Paths\n",
    "2. Convert Paths to Sequence Objects (TensorSequence, TensorScalars, TensorSequence) <-> (input,input,output)\n",
    "2. Data Manipulation\n",
    "    1. Sequence pruning\n",
    "    2. Sequence Resampling\n",
    "4. Split Sequence Objects in Train, Validation\n",
    "3. (Create Windows out of Sequence Objects)\n",
    "5. Noise Injection in Training Dataset Input\n",
    "5. Normalize Input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.data.all import *\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract File Paths\n",
    "Der erste Schritt kann mit get_files von fastai2 erledigt werden. \n",
    "\n",
    "Alternativ kann dies mit weniger Code mit der get_hdf_files() Funktion erledigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, PosixPath('test_data/train/Sim_RealisticCycle2.hdf5'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_path = 'test_data/'\n",
    "hdf_files = get_files(f_path,extensions='.hdf5',recurse=True)\n",
    "len(hdf_files),hdf_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "hdf_extensions = ['.hdf5']\n",
    "def get_hdf_files(path,recurse=True, folders=None):\n",
    "    \"Get hdf5 files in `path` recursively, only in `folders`, if specified.\"\n",
    "    return get_files(path, extensions=hdf_extensions, recurse=recurse, folders=folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, PosixPath('test_data/train/Sim_RealisticCycle2.hdf5'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf_files = get_hdf_files(f_path)\n",
    "len(hdf_files),hdf_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert Paths to Sequence Objects\n",
    "Der Pfad wird unter Angabe der Spaltennamen in Sequenzen und Skalare Werte umgewandelt, um so am Ende ein 3-Tupel zu erhalten aus:\n",
    "- (Sequence, Scalar, Sequence) <-> (input,input,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hdf2sequence(hdf_path,c_names):\n",
    "    with h5py.File(hdf_path,'r') as f:\n",
    "#         import pdb; pdb.set_trace()\n",
    "        l_array = [f[n][:][:,None] for n in c_names]\n",
    "        seq = np.concatenate(l_array,axis=1)\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265598, 2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf2sequence(hdf_files[0],['current','voltage']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion lässt sich mittels Pipeline auf eine Liste von Quellobjekten (hier Pfade) anwenden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(partial(hdf2sequence,c_names=['current','voltage']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, array([0.       , 4.1873503], dtype=float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_pipe = pipe(hdf_files)\n",
    "len(res_pipe), res_pipe[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hdf2scalars(hdf_path,c_names):\n",
    "    with h5py.File(hdf_path,'r') as f:\n",
    "#         import pdb; pdb.set_trace()\n",
    "#         l_array = [f[n][:][:,None] for n in c_names]\n",
    "#         seq = np.concatenate(l_array,axis=1)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Tuple erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Hdf2SeqSeq(Transform):\n",
    "    def __init__(self, seq_inp, seq_out): self.seq_inp,self.seq_out = seq_inp,seq_out\n",
    "    def encodes(self, o): return (hdf2sequence(o,self.seq_inp),\n",
    "                                  hdf2sequence(o,self.seq_out))\n",
    "    def decodes(self, x): return SequenceItem(x)\n",
    "\n",
    "class Hdf2SeqScal(Transform):\n",
    "    def __init__(self, seq_inp, scal_out): self.seq_inp,self.scal_out = seq_inp,scal_out\n",
    "    def encodes(self, o): return (hdf2sequence(o,self.seq_inp),\n",
    "                                  hdf2scalars(o,self.scal_out))\n",
    "    def decodes(self, x): return SequenceItem(x) \n",
    "class Hdf2SeqScalSeq(Transform):\n",
    "    def __init__(self, seq_inp,scal_inp, seq_out): self.seq_inp,self.scal_inp,self.seq_out = seq_inp,scal_inp,seq_out\n",
    "    def encodes(self, o): return (hdf2sequence(o,self.seq_inp),\n",
    "                                  hdf2scalars(o,self.scal_inp),\n",
    "                                  hdf2sequence(o,self.seq_out))\n",
    "    def decodes(self, x): return SequenceItem(x)\n",
    "\n",
    "class Hdf2SeqScalScal(Transform):\n",
    "    def __init__(self, seq_inp,scal_inp, scal_out): self.seq_inp,self.scal_inp,self.scal_out = seq_inp,scal_inp,scal_out\n",
    "    def encodes(self, o): return (hdf2sequence(o,self.seq_inp),\n",
    "                                  hdf2scalars(o,self.scal_inp),\n",
    "                                  hdf2scalars(o,self.scal_out))\n",
    "    def decodes(self, x): return SequenceItem(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, (265598, 2))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf2seq = Pipeline(Hdf2SeqSeq(['current','voltage'],['voltage']))\n",
    "\n",
    "items = hdf2seq(hdf_files)\n",
    "len(items),items[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SequenceItem\n",
    "Damit die Sequenz visualisiert werden kann und auch dritte Informationen gespeichert werden können, wird eine Klasse erstellt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#TODO: Fallunterscheidung der Sequenzen\n",
    "class SequenceItem(Tuple):\n",
    "    def show(self, ctx=None, **kwargs): \n",
    "        plt.figure()\n",
    "        plt.plot(self[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SequenceItem ist nur für die Darstellung eines Tupels von Sequenzen zuständig. Es muss zwischen Skalaren und Vektoriellen Zielgrößen unterschieden werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqTfm(Transform):\n",
    "    def decodes(self, x): return SequenceItem(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SequenceTfm erstellt ein SequenceItem beim decoding für die spätere Darstellung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split in Training, Validation\n",
    "Splitting kann anhand von vorher bekannten Indizes, dem Dateipfad oder anderen allgemeinen Funktion durchgeführt werden.\n",
    "\n",
    "Splitting innerhalb einer Sequenzen sollte in der Praxis nur dann geschehen wenn eine einzige Sequenz vorhanden ist. Diese kann dann vorher manuell geteilt werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Splitting mit vorgegebenem Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = IndexSplitter([1,2])\n",
    "test_eq(splitter(items),[[0],[1,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Splitting mit allgemeiner Funktion\n",
    "Items, bei denen die definierte Funktion `True` zurück gibt, werden den Validierungsdatensatz zugeordnet, der Rest dem Training. In diesem Fall wird nach dem Übergeordneten Ordnernamen gesucht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = FuncSplitter(lambda o: Path(o).parent.name == 'valid')\n",
    "test_eq(splitter(hdf_files),[[0,1],[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Splitting anhand des Parent-Folders\n",
    "Splitter, der Explizit Training und Validierungsordner den Datensätzen zuordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _parent_idxs(items, name): return mask2idxs(Path(o).parent.name == name for o in items)\n",
    "\n",
    "def ParentSplitter(train_name='train', valid_name='valid'):\n",
    "    \"Split `items` from the parent folder names (`train_name` and `valid_name`).\"\n",
    "    def _inner(o, **kwargs):\n",
    "        return _parent_idxs(o, train_name),_parent_idxs(o, valid_name)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ParentSplitter()\n",
    "split_idxs = splitter(hdf_files)\n",
    "test_eq(split_idxs,[[0,1],[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc = DataSource(items,splits=split_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dsrc.train),len(dsrc.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[ 0.       ,  4.1873503],\n",
       "         [-0.0052   ,  4.187454 ],\n",
       "         [-0.009    ,  4.187548 ],\n",
       "         ...,\n",
       "         [ 1.0783   ,  3.7160358],\n",
       "         [ 1.0739   ,  3.716139 ],\n",
       "         [ 1.0706   ,  3.7162225]], dtype=float32), array([[4.1873503],\n",
       "         [4.187454 ],\n",
       "         [4.187548 ],\n",
       "         ...,\n",
       "         [3.7160358],\n",
       "         [3.716139 ],\n",
       "         [3.7162225]], dtype=float32)),)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsrc.train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Windows\n",
    "Aus einer langen Sequenz werden mehrere kurze Sequenzen extrahiert um so verschiedene Teile zu gleicher Zeit dem Model zu zeigen.\n",
    "\n",
    "Dies geschieht auf Tuple level und ist deshalb kein TupleTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64866, array([[3.735982 ],\n",
       "        [3.7363622],\n",
       "        [3.7367425],\n",
       "        ...,\n",
       "        [2.505974 ],\n",
       "        [2.502833 ],\n",
       "        [2.4996917]], dtype=float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = items[0]\n",
    "len(obj[2]),obj[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testen ob bei vervielfältigung sich der Speicherbedarf massiv ändert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [obj]*100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untersuchung mittels Speicherbedarf bei htop zeigt, dass sich der Speicherbedarf selbst bei 10^5 facher Liste nicht um mehr als 10 MB ändert. Das ist die Darstellungsgrenze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def createWindows(x):\n",
    "    win_size = 100\n",
    "    x_seq = x[0]\n",
    "    n_win = x_seq.shape[0]//win_size\n",
    "    win_list = [x]*n_win\n",
    "    for i,win in enumerate(win_list):\n",
    "        win_list[i]=(\n",
    "            win[0][i*win_size:(i+1)*win_size],\n",
    "            win[1][i*win_size:(i+1)*win_size])\n",
    "    return L(win_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = createWindows(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.5 ms ± 1.67 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "pipe_tst = Pipeline(createWindows)\n",
    "lst = pipe_tst(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
