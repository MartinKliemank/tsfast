{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Corefunctionality for data preparation of sequential data for pytorch,\n",
    "  fastai models\n",
    "output-file: core.html\n",
    "title: Corefunctions\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core\n",
    "#| default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "# %load_ext line_profiler\n",
    "%matplotlib widget"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Structure\n",
    "The data will be extracted and prepared via transforms. Those are grouped in:\n",
    "- Type Transforms: Those extraxt the needed components from the source items, like input sequences or target scalar values. The work on single tensors.\n",
    "- Item Transforms: Those Transforms may work on tuple level and therefore may process relationships between input and output.\n",
    "- Batch Transform: Those transforms work on batch level. They receive batched tensors and may apply lazy transforms like normalization very effeciently.\n",
    "\n",
    "An application example may look like the following:\n",
    "- sourceitems: \n",
    "    - path extraction with hdf5 file endings\n",
    "    - create pandas dataframe with information for type transforms, like slices\n",
    "    - filter items in pandas dataframe\n",
    "- type transforms: \n",
    "    - extract hdf5 input and output sequence\n",
    "    - create windows\n",
    "- item transforms: \n",
    "    - filter sequence by value\n",
    "    - shift output sequence by 1 element\n",
    "- batch transforms: \n",
    "    - noise injection\n",
    "    - normalization\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.augment import RandTransform\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def obj_in_lst(lst,cls):\n",
    "    '''retrieve first object of type cls from a list'''\n",
    "    return next(o for o in lst if type(o) is cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def count_parameters(model):\n",
    "    '''retrieve number of trainable parameters of a model'''\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Source Items\n",
    "The file paths may be extracted with `get_files` of fastai2. `get_hdf_files` removes the need of writing the hdf5 file extension.\n",
    "\n",
    "Then a pandas dataframe may be created in case further information for the source items need to be stored like slices for the windowing function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Extract File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " Path('/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_path = Path.cwd() / 'test_data/battery'\n",
    "hdf_files = get_files(f_path,extensions='.hdf5',recurse=True)\n",
    "len(hdf_files),hdf_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "hdf_extensions = ['.hdf5']\n",
    "def get_hdf_files(path,recurse=True, folders=None):\n",
    "    \"Get hdf5 files in `path` recursively, only in `folders`, if specified.\"\n",
    "    return get_files(path, extensions=hdf_extensions, recurse=recurse, folders=folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " Path('/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf_files = get_hdf_files(f_path)\n",
    "len(hdf_files),hdf_files[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create Source Dictionaries\n",
    "In order to extract mulitple realizations of one file with different modifications, we create a list of properties. Pandas Dataframes are to slow for iteration but very fast and convenient for creations. So after creation of the pandas Dataframe we convert it to a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def apply_df_tfms(src,pd_tfms = None):\n",
    "    '''Create Pandas Dataframe out of a list of items, with a list of df transforms applied'''\n",
    "    if type(src) is pd.DataFrame:\n",
    "        df = src\n",
    "    else:\n",
    "        df = pd.DataFrame(data=src.items,columns=['path'],dtype=str)\n",
    "    if pd_tfms is not None:\n",
    "        for t in pd_tfms:\n",
    "            df = t(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle2.hdf5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 path\n",
       "0  /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5\n",
       "1  /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle2.hdf5\n",
       "2  /home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = apply_df_tfms(hdf_files)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(apply_df_tfms(hdf_files),apply_df_tfms(apply_df_tfms(hdf_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def CreateDict(pd_tfms = None):\n",
    "    '''Create List of Dictionarys out of a list of items, with a list of df transforms applied'''\n",
    "    def _inner(src):\n",
    "        df = apply_df_tfms(src,pd_tfms)\n",
    "#         df_dict_list = df.to_dict(orient='records') native to_dict is slower than self written approach\n",
    "        df_values = df.values\n",
    "        df_dict = {name:list(df_values[:,i]) for (i,name) in enumerate(df.columns)}\n",
    "        df_dict_list = [{name: df_dict[name][i] for name in df_dict} for i in range(len(df))]\n",
    "        return df_dict_list\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': '/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5'},\n",
       " {'path': '/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle2.hdf5'},\n",
       " {'path': '/home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_dict =CreateDict()(hdf_files)\n",
    "l_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ValidClmContains(lst_valid):\n",
    "    '''add validation column using a list of strings that are part of the validation frames'''\n",
    "    def _inner(df):\n",
    "        re_valid = '|'.join([re.escape(f) for f in lst_valid])\n",
    "        df['valid'] = df.path.str.contains(re_valid)\n",
    "        return df\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.31 ms, sys: 133 µs, total: 1.45 ms\n",
      "Wall time: 1.43 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'path': '/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False},\n",
       " {'path': '/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle2.hdf5',\n",
       "  'valid': False},\n",
       " {'path': '/home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5',\n",
       "  'valid': True}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lst_valid = ['valid']\n",
    "CreateDict([ValidClmContains(lst_valid)])(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ValidClmIs(lst_valid):\n",
    "    '''adds validation column using a list of validation filenames'''\n",
    "    def _inner(df):\n",
    "        df['valid'] = df.path.isin([str(f) for f in lst_valid])\n",
    "        return df\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.11 ms, sys: 113 µs, total: 1.22 ms\n",
      "Wall time: 1.22 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'path': '/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False},\n",
       " {'path': '/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle2.hdf5',\n",
       "  'valid': False},\n",
       " {'path': '/home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5',\n",
       "  'valid': False}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lst_valid = ['test_data/battery/train/Sim_RealisticCycle2.hdf5','test_data/battery/valid/Sim_RealisticCycle3.hdf5']\n",
    "CreateDict([ValidClmIs(lst_valid)])(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def FilterClm(clm_name,func = lambda x:x):\n",
    "    '''adds validation column using a list of validation filenames'''\n",
    "    def _inner(df):\n",
    "        return df[func(df[clm_name])]\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CreateDict([ValidClmIs(lst_valid),FilterClm('valid')])(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_hdf_seq_len(df,clm,ds=None):\n",
    "    '''extract the sequence length of the dataset with the 'clm' name and 'f_path' path  '''\n",
    "    with h5py.File(df['path'],'r') as f:\n",
    "        ds = f if 'dataset' not in df else f[df['dataset']]\n",
    "        f_len = max(ds[clm].shape)\n",
    "    return f_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def df_get_hdf_seq_len(df,clm,ds=None):\n",
    "    '''extracts the sequence length of every file in advance to prepare repeated window extractions with 'DfHDFCreateWindows' '''\n",
    "#     df['seq_len'] = ([get_hdf_seq_len(row.path,clm) for (idx, row) in df.iterrows()])\n",
    "    df['seq_len'] = df.apply(lambda x: get_hdf_seq_len(x,clm),axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def DfHDFGetSeqLen(clm):\n",
    "    def _inner(df):\n",
    "        return df_get_hdf_seq_len(df,clm)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>265598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 path  \\\n",
       "0  /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5   \n",
       "1  /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle2.hdf5   \n",
       "2  /home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5   \n",
       "\n",
       "   seq_len  \n",
       "0   265607  \n",
       "1   265598  \n",
       "2   265593  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_get_hdf_seq_len(df,'current')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>265598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 path  \\\n",
       "0  /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5   \n",
       "1  /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle2.hdf5   \n",
       "2  /home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5   \n",
       "\n",
       "   seq_len  \n",
       "0   265607  \n",
       "1   265598  \n",
       "2   265593  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DfHDFGetSeqLen('current')(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numbers\n",
    "def DfResamplingFactor(src_fs,lst_targ_fs):\n",
    "    if not isinstance(src_fs, numbers.Number) and not type(src_fs) is str: \n",
    "        raise ValueError('src_fs has to be a column name or a fixed number')\n",
    "    \n",
    "    def _inner(df):\n",
    "        np_targ_fs = array(lst_targ_fs)\n",
    "        pd.options.mode.chained_assignment = None #every row is a reference so we need to suppress the warning messages while copying\n",
    "\n",
    "        #repeat entries for every target fs\n",
    "        res_df = df.iloc[np.repeat(np.arange(len(df)),len(np_targ_fs))] \n",
    "        targ_fs = np.tile(np_targ_fs,len(df))\n",
    "        res_df['targ_fs'] = targ_fs\n",
    "        \n",
    "        if isinstance(src_fs, numbers.Number):\n",
    "            #src_fs is a fixed number\n",
    "            res_df['resampling_factor'] = targ_fs/src_fs\n",
    "        else:\n",
    "            #src_fs is a column name of the df\n",
    "            res_df['resampling_factor'] = targ_fs/res_df[src_fs]\n",
    "\n",
    "        pd.options.mode.chained_assignment = 'warn'\n",
    "        \n",
    "        return res_df\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_fs = [50,100,300]\n",
    "test_eq(len(DfResamplingFactor(100,targ_fs)(df)),9)   \n",
    "df['src_fs'] = 200.\n",
    "test_eq(len(DfResamplingFactor('src_fs',targ_fs)(df)),9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def DfHDFCreateWindows(win_sz,stp_sz, clm, fixed_start = False, fixed_end = False):\n",
    "    '''create windows of sequences, splits sequence into multiple items'''\n",
    "    def _inner(df):\n",
    "        if fixed_start and fixed_end: raise Exception\n",
    "        \n",
    "        if 'seq_len' in df:\n",
    "            np_f_len = df.seq_len.values\n",
    "        else:\n",
    "            np_f_len = np.array([get_hdf_seq_len(row,clm) for (idx, row) in df.iterrows()])\n",
    "            \n",
    "        if 'resampling_factor' in df: np_f_len =(np_f_len*df.resampling_factor.values).astype(int)\n",
    "            \n",
    "        n_win = ((np_f_len-win_sz)//stp_sz)+1\n",
    "        #cast array n_win to int and clip negative values to 0\n",
    "        n_win = n_win.astype(int)\n",
    "        n_win = np.clip(n_win,a_min=0,a_max=None) #remove negative values at instances where the winsize is smaller than the seq_len\n",
    "        lst_idx = np.arange(len(np_f_len))\n",
    "        \n",
    "        pd.options.mode.chained_assignment = None #every row is a reference so we need to suppress the warning messages while copying\n",
    "        \n",
    "        res_df = df.iloc[np.repeat(lst_idx,n_win)]\n",
    "#         res_df = df.loc[np.repeat(lst_idx,n_win)] #the loc variant as a little bit slower because it creates copies and returns wrong values with redundant indexes, but is more robust\n",
    "\n",
    "        step_idx = np.concatenate([np.arange(x) for x in n_win])\n",
    "    \n",
    "        \n",
    "        res_df['l_slc'] = step_idx*stp_sz if not fixed_start else None\n",
    "        res_df['r_slc'] = step_idx*stp_sz + win_sz if not fixed_end else None\n",
    "            \n",
    "        pd.options.mode.chained_assignment = 'warn'\n",
    "            \n",
    "        return res_df\n",
    "    \n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.28 ms, sys: 0 ns, total: 2.28 ms\n",
      "Wall time: 2.11 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>src_fs</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>100</td>\n",
       "      <td>200.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200</td>\n",
       "      <td>300.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>300</td>\n",
       "      <td>400.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>400</td>\n",
       "      <td>500.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>200.0</td>\n",
       "      <td>265000</td>\n",
       "      <td>265100.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>200.0</td>\n",
       "      <td>265100</td>\n",
       "      <td>265200.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>200.0</td>\n",
       "      <td>265200</td>\n",
       "      <td>265300.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>200.0</td>\n",
       "      <td>265300</td>\n",
       "      <td>265400.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>200.0</td>\n",
       "      <td>265400</td>\n",
       "      <td>265500.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7966 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  path  \\\n",
       "0   /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5   \n",
       "0   /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5   \n",
       "0   /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5   \n",
       "0   /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5   \n",
       "0   /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5   \n",
       "..                                                                                 ...   \n",
       "2   /home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5   \n",
       "2   /home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5   \n",
       "2   /home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5   \n",
       "2   /home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5   \n",
       "2   /home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5   \n",
       "\n",
       "    seq_len  src_fs   l_slc     r_slc  \n",
       "0    265607   200.0       0     100.2  \n",
       "0    265607   200.0     100     200.2  \n",
       "0    265607   200.0     200     300.2  \n",
       "0    265607   200.0     300     400.2  \n",
       "0    265607   200.0     400     500.2  \n",
       "..      ...     ...     ...       ...  \n",
       "2    265593   200.0  265000  265100.2  \n",
       "2    265593   200.0  265100  265200.2  \n",
       "2    265593   200.0  265200  265300.2  \n",
       "2    265593   200.0  265300  265400.2  \n",
       "2    265593   200.0  265400  265500.2  \n",
       "\n",
       "[7966 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "create_win = DfHDFCreateWindows(win_sz=100.2,stp_sz=100,clm='current')\n",
    "win_df = create_win(df)\n",
    "win_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>src_fs</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>265594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>265598</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>265594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 path  \\\n",
       "0  /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5   \n",
       "1  /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle2.hdf5   \n",
       "\n",
       "   seq_len  src_fs  l_slc   r_slc  \n",
       "0   265607   200.0      0  265594  \n",
       "1   265598   200.0      0  265594  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_df = DfHDFCreateWindows(win_sz=265594,stp_sz=100,clm='current')(df)\n",
    "test_eq(len(win_df),2)\n",
    "win_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(create_win(df_get_hdf_seq_len(df,'current')) , create_win(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>src_fs</th>\n",
       "      <th>targ_fs</th>\n",
       "      <th>resampling_factor</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0</td>\n",
       "      <td>100.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>265598</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0</td>\n",
       "      <td>100.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0</td>\n",
       "      <td>100.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 path  \\\n",
       "0  /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5   \n",
       "1  /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle2.hdf5   \n",
       "2  /home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5   \n",
       "\n",
       "   seq_len  src_fs  targ_fs  resampling_factor  l_slc  r_slc  \n",
       "0   265607   200.0      0.1             0.0005      0  100.2  \n",
       "1   265598   200.0      0.1             0.0005      0  100.2  \n",
       "2   265593   200.0      0.1             0.0005      0  100.2  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_win_df = create_win(DfResamplingFactor(200,[0.1])(df))\n",
    "res_win_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(res_win_df),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>src_fs</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>265594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>265598</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>265594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 path  \\\n",
       "0  /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5   \n",
       "1  /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle2.hdf5   \n",
       "\n",
       "   seq_len  src_fs  l_slc   r_slc  \n",
       "0   265607   200.0      0  265594  \n",
       "1   265598   200.0      0  265594  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_expr = 'l_slc <= 200'\n",
    "filt_df = win_df.query(query_expr)\n",
    "filt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def DfApplyFuncSplit(split_func,func1,func2):\n",
    "    '''apply two different functions on the dataframe, func1 on the first indices of split_func, func2 on the second indices.\n",
    "        Split_func is a Training, Validation split function'''\n",
    "    def _inner(df):\n",
    "        (idxs1,idxs2) = split_func(df.path)\n",
    "        df1= func1(df.iloc[idxs1])\n",
    "        df2= func2(df.iloc[idxs2])\n",
    "        return pd.concat((df1,df2))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>src_fs</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>200.0</td>\n",
       "      <td>210000</td>\n",
       "      <td>220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>200.0</td>\n",
       "      <td>220000</td>\n",
       "      <td>230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>200.0</td>\n",
       "      <td>230000</td>\n",
       "      <td>240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>200.0</td>\n",
       "      <td>240000</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>200.0</td>\n",
       "      <td>250000</td>\n",
       "      <td>260000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>255660 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  path  \\\n",
       "0   /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5   \n",
       "0   /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5   \n",
       "0   /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5   \n",
       "0   /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5   \n",
       "0   /home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5   \n",
       "..                                                                                 ...   \n",
       "2   /home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5   \n",
       "2   /home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5   \n",
       "2   /home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5   \n",
       "2   /home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5   \n",
       "2   /home/pheenix/Development/seqdata/test_data/battery/valid/Sim_RealisticCycle3.hdf5   \n",
       "\n",
       "    seq_len  src_fs   l_slc   r_slc  \n",
       "0    265607   200.0       0   10000  \n",
       "0    265607   200.0       1   10001  \n",
       "0    265607   200.0       2   10002  \n",
       "0    265607   200.0       3   10003  \n",
       "0    265607   200.0       4   10004  \n",
       "..      ...     ...     ...     ...  \n",
       "2    265593   200.0  210000  220000  \n",
       "2    265593   200.0  220000  230000  \n",
       "2    265593   200.0  230000  240000  \n",
       "2    265593   200.0  240000  250000  \n",
       "2    265593   200.0  250000  260000  \n",
       "\n",
       "[255660 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_win_split = DfApplyFuncSplit(\n",
    "    IndexSplitter([1,2]),\n",
    "    DfHDFCreateWindows(win_sz=10000,stp_sz=1,clm='current'),\n",
    "    DfHDFCreateWindows(win_sz=10000,stp_sz=10000,clm='current')\n",
    ")\n",
    "create_win_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def DfFilterQuery(query):\n",
    "    def _inner(df):\n",
    "        return df.query(query)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(DfFilterQuery(query_expr)(win_df),filt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.4 ms, sys: 15.8 ms, total: 58.3 ms\n",
      "Wall time: 57.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'path': '/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 0,\n",
       "  'r_slc': 101},\n",
       " {'path': '/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 10,\n",
       "  'r_slc': 111},\n",
       " {'path': '/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 20,\n",
       "  'r_slc': 121},\n",
       " {'path': '/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 30,\n",
       "  'r_slc': 131},\n",
       " {'path': '/home/pheenix/Development/seqdata/test_data/battery/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 40,\n",
       "  'r_slc': 141}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tfm_src = CreateDict([ValidClmContains(['valid']),DfHDFCreateWindows(win_sz=100+1,stp_sz=10,clm='current')])\n",
    "src_dicts = tfm_src(hdf_files)\n",
    "src_dicts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def DfDropClmExcept(clms = ['path','l_slc','r_slc','p_sample','resampling_factor']):\n",
    "    '''drop unused dataframe columns as a last optional step to accelerate dictionary conversion'''\n",
    "    def _inner(df):\n",
    "        return df[[c for c in clms if c in df]]\n",
    "    return _inner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert Paths to Sequence Objects\n",
    "Der Pfad wird unter Angabe der Spaltennamen in Sequenzen und Skalare Werte umgewandelt, um so am Ende ein 3-Tupel zu erhalten aus:\n",
    "- (Sequence, Scalar, Sequence) <-> (input,input,output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extract sequential data from hdf5-files\n",
    "Two different functions, based on pandas df and on lists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Shift time Series\n",
    "Sometimes we need to shift columns of a sequence by a specific value. Then we cant simply slice the array but have to handle each column individually. First a performance test has to be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_shift_offsets(clm_shift):\n",
    "    clm_shift = array(clm_shift)\n",
    "    l_offs = -min(clm_shift.min(),0)\n",
    "    r_offs = -max(clm_shift.max(),0)\n",
    "    l_shift = clm_shift+l_offs\n",
    "    r_shift = clm_shift+r_offs\n",
    "    dim_red = l_offs-r_offs\n",
    "    return l_shift,r_shift,dim_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 0, 2]), array([-1, -1, -2,  0]), 2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shft = [0,0,-1,1]\n",
    "calc_shift_offsets(shft)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both shifting methods have their own performance character. vstack needs double the time on short sequences, while the creation of a seperate array with copy becomes worse starting at around 5000 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ta = array([[1,2,3]*2]*10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# y = np.vstack([ta[i:-ta.shape[1]+i,i] for i in range(ta.shape[1])]).T   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# x = np.zeros((ta.shape[0]-ta.shape[1],ta.shape[1]))\n",
    "# for i in range(ta.shape[1]):\n",
    "#     x[:,i] = ta[i:-ta.shape[1]+i,i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 HDF2Sequence\n",
    "HDF5 performance is massively affected by the dtype of the signals. f4 (32 bit floating point) Numbers are faster to load and lead to smaller files then f8 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0,axis=0),axis=0) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def downsample_mean(x,N):\n",
    "    shp = x.shape\n",
    "    trunc = -(x.shape[0] % N)\n",
    "    trunc = trunc if trunc != 0 else None\n",
    "    return x[:trunc,:].reshape((-1,N,x.shape[-1])).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from scipy.signal import butter, lfilter, lfilter_zi\n",
    "from scipy import signal\n",
    "def resample_interp(x,resampling_factor,sequence_first=True, lowpass_cut=1.0, upsample_cubic_cut = None):\n",
    "    '''signal resampling using linear or cubic interpolation\n",
    "    \n",
    "    x: signal to resample with shape: features x resampling_dimension or resampling_dimension x  features if sequence_first=True\n",
    "    resampling_factor: Factor > 0 that scales the signal\n",
    "    lowpass_cut: Upper boundary for resampling_factor that activates the lowpassfilter, low values exchange accuracy for performance, default is 0.7\n",
    "    upsample_cubic_cut: Lower boundary for resampling_factor that activates cubic interpolation at high upsampling values. \n",
    "                        Improves signal dynamics in exchange of performance. None deactivates cubic interpolation\n",
    "    '''\n",
    "    \n",
    "    if sequence_first:\n",
    "        x = x.T\n",
    "    \n",
    "    fs_n = resampling_factor\n",
    "    #if downsampling rate is too high, lowpass filter before interpolation\n",
    "    if fs_n < lowpass_cut:\n",
    "        b,a = butter(2, fs_n)\n",
    "        zi = lfilter_zi(b,a)*x[:,:1] #initialize filter with steady state at first time step value\n",
    "        x,_ = lfilter(b,a,x,axis=-1,zi=zi)\n",
    "\n",
    "#         sos = butter(2, fs_n*1.2,output='sos')\n",
    "# #         sos = signal.cheby2(2,20, fs_n,output='sos')\n",
    "# #         import pdb;pdb.set_trace()\n",
    "#         zi = np.swapaxes(signal.sosfilt_zi(sos)[...,None]*x[:,0],1,2)\n",
    "#         x,_ = signal.sosfilt(sos, x,axis=-1,zi=zi)\n",
    "        \n",
    "    x_int = tensor(x)[None,...]\n",
    "    targ_size = int(x.shape[-1]*fs_n)\n",
    "    \n",
    "#     if upsampling rate is too high, switch from linear to cubic interpolation\n",
    "    if upsample_cubic_cut is None or fs_n <= upsample_cubic_cut:\n",
    "        x = array(nn.functional.interpolate(x_int, size=targ_size, mode='linear',align_corners=False)[0])\n",
    "    else:\n",
    "        x = array(nn.functional.interpolate(x_int[...,None], size=[targ_size,1], mode='bicubic',align_corners=False)[0,...,0])\n",
    "#     x = array(x_int)[0]\n",
    "    \n",
    "    if sequence_first:\n",
    "        x = x.T\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(100000,9))\n",
    "test_eq(resample_interp(x,0.3).shape[0],30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plotFFT(buffer,sample_rate,label,fig=plt):\n",
    "#     xf,yf = calcFFT(buffer,sample_rate)\n",
    "#     fig.plot(xf, yf,label=label)\n",
    "    \n",
    "# def calcFFT(buffer,sample_rate):\n",
    "#     N = buffer.shape[0]\n",
    "#     T = 1.0 / sample_rate\n",
    "#     x = np.linspace(0.0, N*T, N)\n",
    "#     y = buffer\n",
    "#     yf = scipy.fft.fft(y)\n",
    "#     xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "#     return xf,2.0/N * np.abs(yf[:N//2])\n",
    "\n",
    "# src_fs= 200\n",
    "# n_fs = 0.9\n",
    "# plt.figure()\n",
    "# plotFFT(x[:,0],src_fs,'raw')\n",
    "# plotFFT(resample_interp(x,n_fs)[:,0],n_fs*src_fs,'filtered')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from scipy.signal import resample\n",
    "def hdf_extract_sequence(hdf_path,clms,dataset = None, l_slc = None, r_slc= None, resampling_factor=None, fs_idx =None,dt_idx =False,fast_resample=True):\n",
    "    '''\n",
    "    extracts a sequence with the shape [seq_len x num_features]\n",
    "    \n",
    "    hdf_path: file path of hdf file, may be a string or path type\n",
    "    clms: list of dataset names of sequences in hdf file\n",
    "    dataset: dataset root for clms. Useful for multiples sequences stored in one file.\n",
    "    l_slc: left boundary for extraction of a window of the whole sequence\n",
    "    r_slc: right boundary for extraction of a window of the whole sequence\n",
    "    resampling_factor: scaling factor for the sequence length, uses 'resample_interp' for resampling\n",
    "    fs_idx: clms list idx of fs entry in sequence. Will be scaled by resampling_factor after resampling\n",
    "    dt_idx: clms list idx of dt entry in sequence. Will be scaled by resampling_factor after resampling\n",
    "    fast_resample: if True, uses linear interpolation with anti-aliasing filter for faster resampling. Is less accurate than fft based resampling\n",
    "    '''\n",
    "\n",
    "    if resampling_factor is not None:\n",
    "        seq_len = r_slc-l_slc if l_slc is not None and r_slc is not None else None #calculate seq_len for later slicing, necesary because of rounding errors in resampling\n",
    "        if l_slc is not None: l_slc= math.floor(l_slc/resampling_factor)\n",
    "        if r_slc is not None: r_slc= math.ceil(r_slc/resampling_factor)\n",
    "\n",
    "    with h5py.File(hdf_path,'r') as f:\n",
    "        ds = f if dataset is None else f[dataset]\n",
    "        l_array = [(ds[n][l_slc:r_slc]) for n in clms]\n",
    "        seq = np.stack(l_array,axis=-1)\n",
    "\n",
    "    if resampling_factor is not None:\n",
    "        if fast_resample:\n",
    "            res_seq = resample_interp(seq,resampling_factor)\n",
    "        else:\n",
    "            res_seq = resample(seq,int(seq.shape[0]*resampling_factor),window=('kaiser', 14.0))\n",
    "        if fs_idx is not None: res_seq[:,fs_idx] = seq[0,fs_idx] * resampling_factor\n",
    "        if dt_idx is not None: res_seq[:,dt_idx] = seq[0,dt_idx] / resampling_factor\n",
    "        seq = res_seq\n",
    "        \n",
    "        if seq_len is not None: seq = seq[:seq_len] #cut the part of the sequence that is too long because of resampling rounding errors\n",
    "        \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Memoize:\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn\n",
    "        self.memo = {}\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        if args not in self.memo:\n",
    "            self.memo[args] = self.fn(*args)\n",
    "        return self.memo[args]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from multiprocessing import shared_memory, Manager, Lock\n",
    "import weakref\n",
    "\n",
    "class MemoizeMP:\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn\n",
    "        self.manager = Manager()\n",
    "        self.results_dict = self.manager.dict()  # Stores metadata about computed results\n",
    "        self.lock = Lock()  # Ensure atomic updates to the results_dict\n",
    "        self.local_memo = {}  # Local cache for each process\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        # First, attempt to return the result from the local cache to avoid unnecessary locking\n",
    "        if args in self.local_memo:\n",
    "            result, _ = self.local_memo[args]\n",
    "            return result\n",
    "\n",
    "        with self.lock:\n",
    "            if args in self.results_dict:\n",
    "                result_info = self.results_dict[args]\n",
    "                existing_shm = shared_memory.SharedMemory(name=result_info['name'])\n",
    "                result = np.ndarray(result_info['shape'], dtype=result_info['dtype'], buffer=existing_shm.buf)\n",
    "                # Register the shared memory for cleanup without removing it from the dictionary\n",
    "                self.local_memo[args] = (result, existing_shm)\n",
    "                # Using a weakref finalizer to ensure cleanup when the numpy array is no longer in use\n",
    "                weakref.finalize(result, self.cleanup_shared_memory, result_info['name'])\n",
    "                return result\n",
    "\n",
    "        result = self.fn(*args)\n",
    "\n",
    "        result_shm = shared_memory.SharedMemory(create=True, size=result.nbytes)\n",
    "        shm_array = np.ndarray(result.shape, dtype=result.dtype, buffer=result_shm.buf)\n",
    "        shm_array[:] = result[:]\n",
    "\n",
    "        with self.lock:\n",
    "            if args not in self.results_dict:\n",
    "                self.results_dict[args] = {\n",
    "                    'name': result_shm.name,\n",
    "                    'shape': result.shape,\n",
    "                    'dtype': result.dtype.str\n",
    "                }\n",
    "                self.local_memo[args] = (result, result_shm)\n",
    "                # Register cleanup for the newly created shared memory block\n",
    "                weakref.finalize(result, self.cleanup_shared_memory, result_shm.name)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def cleanup_shared_memory(self, shm_name):\n",
    "        \"\"\"Cleanup a specific shared memory block.\"\"\"\n",
    "        try:\n",
    "            shm = shared_memory.SharedMemory(name=shm_name)\n",
    "            shm.close()\n",
    "            shm.unlink()\n",
    "        except FileNotFoundError:\n",
    "            # Handle the case where the shared memory block is already cleaned up\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HDF2Sequence(Transform):\n",
    "    \n",
    "    def __init__(self, clm_names,clm_shift=None,truncate_sz=None,to_cls=noop,cached=True, fs_idx =None,dt_idx =None,fast_resample=True):\n",
    "        if not clm_shift is None:\n",
    "            assert len(clm_shift)==len(clm_names) and all(isinstance(n, int) for n in clm_shift)\n",
    "            self.l_shift,self.r_shift,_ = calc_shift_offsets(clm_shift)\n",
    "        \n",
    "        if not cached:\n",
    "            self._exseq = self._hdf_extract_sequence \n",
    "        elif cached == 'local':\n",
    "            self._exseq = Memoize(self._hdf_extract_sequence) \n",
    "        else :\n",
    "            self._exseq = MemoizeMP(self._hdf_extract_sequence)\n",
    "\n",
    "        self.cached = cached is not None\n",
    "        store_attr('clm_names,clm_shift,truncate_sz,to_cls,fs_idx,dt_idx,fast_resample')\n",
    "        \n",
    "    def _hdf_extract_sequence(self,hdf_path,dataset = None, l_slc = None, r_slc= None, resampling_factor=None, fs_idx =None,dt_idx =None,fast_resample=True):\n",
    "        '''\n",
    "        extracts a sequence with the shape [seq_len x num_features]\n",
    "\n",
    "        hdf_path: file path of hdf file, may be a string or path type\n",
    "        clms: list of dataset names of sequences in hdf file\n",
    "        dataset: dataset root for clms. Useful for multiples sequences stored in one file.\n",
    "        l_slc: left boundary for extraction of a window of the whole sequence\n",
    "        r_slc: right boundary for extraction of a window of the whole sequence\n",
    "        resampling_factor: scaling factor for the sequence length, uses 'resample_interp' for resampling\n",
    "        fs_idx: clms list idx of fs entry in sequence. Will be scaled by resampling_factor after resampling\n",
    "        dt_idx: clms list idx of dt entry in sequence. Will be scaled by resampling_factor after resampling\n",
    "        fast_resample: if True, uses linear interpolation with anti-aliasing filter for faster resampling. Is less accurate than fft based resampling\n",
    "        '''\n",
    "\n",
    "        if resampling_factor is not None:\n",
    "            seq_len = r_slc-l_slc if l_slc is not None and r_slc is not None else None #calculate seq_len for later slicing, necesary because of rounding errors in resampling\n",
    "            if l_slc is not None: l_slc= math.floor(l_slc/resampling_factor)\n",
    "            if r_slc is not None: r_slc= math.ceil(r_slc/resampling_factor)\n",
    "\n",
    "        with h5py.File(hdf_path,'r') as f:\n",
    "            ds = f if dataset is None else f[dataset]\n",
    "            l_array = [(ds[n][l_slc:r_slc]) for n in self.clm_names]\n",
    "            seq = np.stack(l_array,axis=-1)\n",
    "\n",
    "        if resampling_factor is not None:\n",
    "            if fast_resample:\n",
    "                res_seq = resample_interp(seq,resampling_factor)\n",
    "            else:\n",
    "                res_seq = resample(seq,int(seq.shape[0]*resampling_factor),window=('kaiser', 14.0))\n",
    "            \n",
    "            if fs_idx is not None: res_seq[:,fs_idx] = seq[0,fs_idx] * resampling_factor\n",
    "            if dt_idx is not None: res_seq[:,dt_idx] = seq[0,dt_idx] / resampling_factor\n",
    "            seq = res_seq\n",
    "\n",
    "            if seq_len is not None: seq = seq[:seq_len] #cut the part of the sequence that is too long because of resampling rounding errors\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def _extract_dict_sequence(self,item):\n",
    "        if hasattr(item,'keys'):\n",
    "            path = item['path']\n",
    "            dataset = item['dataset'] if 'dataset' in item else None\n",
    "            l_slc = item['l_slc'] if 'l_slc' in item else None\n",
    "            r_slc = item['r_slc'] if 'r_slc' in item else None\n",
    "            resampling_factor = item['resampling_factor'] if 'resampling_factor' in item else None\n",
    "\n",
    "            if self.cached:\n",
    "                seq = self._exseq(path,dataset,None,None,resampling_factor,self.fs_idx,self.dt_idx,self.fast_resample)[l_slc:r_slc]\n",
    "            else:\n",
    "                seq = self._exseq(path,dataset,l_slc,r_slc,resampling_factor,self.fs_idx,self.dt_idx,self.fast_resample)\n",
    "        else:\n",
    "            seq = self._exseq(str(item),None,None,None,None,None)\n",
    "\n",
    "        #shift clms of result by given value \n",
    "        if not self.clm_shift is None:\n",
    "            l_seq = seq.shape[0]\n",
    "            seq = np.stack([seq[self.l_shift[i]:l_seq+self.r_shift[i],i] for i in range(seq.shape[1])],axis=-1)\n",
    "            \n",
    "        if not self.truncate_sz is None:\n",
    "            seq = seq[truncate_sz:]\n",
    "        \n",
    "        #it is important to slice first and then do the class conversion\n",
    "#         return self.to_cls(seq.astype('f8'))#workaround for random bug, that mitigates convergence if the numpy array is an f4 array. Seems to make no sense because the result does not change. \n",
    "        return self.to_cls(seq)\n",
    "\n",
    "    def encodes(self, item)->None: \n",
    "        return self._extract_dict_sequence(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  4.1873503],\n",
       "       [-0.1      ,  4.18935  ],\n",
       "       [-0.1      ,  4.1896954],\n",
       "       ...,\n",
       "       [ 8.8388   ,  3.3932123],\n",
       "       [ 8.846    ,  3.3928714],\n",
       "       [ 8.8531   ,  3.3925302]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "hdf2seq = HDF2Sequence(['current','voltage'],cached=False)\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1      ,  4.18935  ],\n",
       "       [-0.1      ,  4.1896954],\n",
       "       [-0.1      ,  4.190037 ],\n",
       "       ...,\n",
       "       [ 8.8388   ,  3.3932123],\n",
       "       [ 8.846    ,  3.3928714],\n",
       "       [ 8.8531   ,  3.3925302]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf2seq = HDF2Sequence(['current','voltage'],clm_shift=[1,1])\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  4.1873503],\n",
       "       [-0.1      ,  4.18935  ],\n",
       "       [-0.1      ,  4.1896954],\n",
       "       ...,\n",
       "       [ 8.8388   ,  3.3932123],\n",
       "       [ 8.846    ,  3.3928714],\n",
       "       [ 8.8531   ,  3.3925302]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf2seq = HDF2Sequence(['current','voltage'],cached='shared')\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hdf2seq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %%timeit\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mhdf2seq\u001b[49m(hdf_files[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hdf2seq' is not defined"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf2seq = HDF2Sequence(['current','voltage'],cached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion lässt sich mittels Pipeline auf eine Liste von Quellobjekten (hier Pfade) anwenden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf2seq = HDF2Sequence(['current'])\n",
    "hdf2seq(hdf_files[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(HDF2Sequence(['current','voltage']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_pipe = pipe(hdf_files)\n",
    "# len(res_pipe), res_pipe[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Test\n",
    "Caching stores the arrays for future use at every function call. Very usefull, especially for windows. Should allways be turned. Only explicitly turn it off when there is not enough memory for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms=[  [HDF2Sequence(['current','voltage'],cached=None)],\n",
    "        [HDF2Sequence(['voltage'],cached=None)]]\n",
    "dsrc = Datasets(src_dicts[:1000],tfms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dsrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for x in dsrc:\n",
    "#     x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms=[  [HDF2Sequence(['current','voltage'],cached=True,clm_shift=[1,2])],\n",
    "        [HDF2Sequence(['voltage'],cached=True)]]\n",
    "dsrc = Datasets(src_dicts[:1000],tfms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "for x in dsrc:\n",
    "    x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching is way faster because every file gets loaded multiple times"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Scalar data from hdf5-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def hdf2scalars(hdf_path,c_names,dataset = None):\n",
    "    with h5py.File(hdf_path,'r') as f:\n",
    "        ds = f if dataset is None else f[dataset]\n",
    "        l_array = [ds.attrs[n] for n in c_names]\n",
    "        scalars = np.stack(l_array,axis=-1)\n",
    "#         import pdb; pdb.set_trace()\n",
    "#         l_array = [f[n][:][:,None] for n in c_names]\n",
    "#         seq = np.concatenate(l_array,axis=1)\n",
    "        return scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdf2scalars('/mnt/data/sicwell/hdf5/Cycles/ch3/cycle00568.hdf5',['soc','temperature1'],dataset='measurement_00000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HDF2Scalars(Transform):\n",
    "    \n",
    "    def __init__(self, clm_names,to_cls=noop):\n",
    "        store_attr('clm_names,to_cls')\n",
    "    \n",
    "    def _extract_dict_scalars(self,item):\n",
    "        if isinstance(item,dict):\n",
    "            path = item['path']\n",
    "            dataset = item['dataset'] if 'dataset' in item else None\n",
    "\n",
    "            seq = hdf2scalars(path,self.clm_names,dataset)\n",
    "        else:\n",
    "            seq = hdf2scalars(str(item),self.clm_names)\n",
    "        return self.to_cls(seq)\n",
    "\n",
    "    def encodes(self, item)->None: \n",
    "        return self._extract_dict_scalars(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDF2Scalars(['soc','temperature1'])({'path':'/mnt/data/sicwell/hdf5/Cycles/ch3/cycle00568.hdf5','dataset':'measurement_00000'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Scalar from sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ScalarSequenceElement(Transform):\n",
    "    \n",
    "    def __init__(self, idx,to_cls=noop):\n",
    "        store_attr('idx,to_cls')\n",
    "\n",
    "    def encodes(self, item)->None: \n",
    "        return self.to_cls(item[self.idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScalarSequenceElement(-1)(hdf2seq(hdf_files[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Datatypes for Sequences and Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TensorSequences(TensorBase):#TensorBase\n",
    "#     def __init__(self,x,c_names=None, **kwargs):\n",
    "#         super().__init__()\n",
    "#         self.c_names = c_names\n",
    "    \n",
    "    def show(self, ctx=None, **kwargs):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        ax = ctx\n",
    "        if ax is None: _,ax = plt.subplots()\n",
    "        ax.plot(self)\n",
    "#         if title is not None: ax.set_title(title)\n",
    "        return ax\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(HDF2Sequence, keep=True)\n",
    "    def from_hdf(cls,clm_names,**kwargs):\n",
    "        return HDF2Sequence(clm_names,**kwargs)\n",
    "    \n",
    "class TensorSequencesInput(TensorSequences): pass\n",
    "class TensorSequencesOutput(TensorSequences): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = TensorSequencesInput.from_hdf(['current'])\n",
    "type(f(hdf_files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorSequences(np.ones((30,2))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@Transform\n",
    "def toTensorSequencesInput(o): return TensorSequencesInput(o)\n",
    "@Transform\n",
    "def toTensorSequencesOutput(o): return TensorSequencesOutput(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TensorScalars(TensorBase):\n",
    "    @classmethod\n",
    "    @delegates(HDF2Scalars, keep=True)\n",
    "    def from_hdf(cls,clm_names,**kwargs):\n",
    "        return HDF2Scalars(clm_names,**kwargs)\n",
    "    \n",
    "    \n",
    "class TensorScalarsInput(TensorScalars): pass\n",
    "class TensorScalarsOutput(TensorScalars): pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor subclassing mechanism since pytorch 1.7 keeps the tensor type in tensor operations. Operations with different branches of subclasses of tensors require a implementation of '__torch_function__'.\n",
    "Fastai implements 'TensorBase.register_func' to mark methods that behave for the given types like the default torch operation.\n",
    "\n",
    "https://pytorch.org/docs/stable/notes/extending.html#extending-torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "for f in torch.nn.functional.mse_loss,torch.nn.functional.huber_loss, Tensor.__getitem__, Tensor.__ne__,Tensor.__eq__,Tensor.add,Tensor.sub,Tensor.mul,Tensor.div,Tensor.__rsub__,Tensor.__radd__,Tensor.matmul,Tensor.bmm:\n",
    "    TensorBase.register_func(f,TensorSequences)\n",
    "    TensorBase.register_func(f,TensorScalars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = TensorSequencesInput(torch.rand((10,10)))\n",
    "x2 = TensorSequencesOutput(torch.rand((10,10)))\n",
    "torch.nn.functional.mse_loss(x1,x2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformations\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Sequence Slicing Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class SeqSlice(Transform):\n",
    "    '''Take a slice from an array-like object. Useful for e.g. shifting input and output'''\n",
    "    def __init__(self, l_slc=None,r_slc=None):\n",
    "        self.l_slc,self.r_slc = l_slc,r_slc\n",
    "        \n",
    "    def encodes(self, o): return o[self.l_slc:self.r_slc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_shift = SeqSlice(r_slc=-1)\n",
    "arr = np.ones((5))\n",
    "test_eq(l_shift(arr),arr[:-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sequence Noise Injection Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SeqNoiseInjection(RandTransform):\n",
    "    split_idx=0 #apply only to training data, if None it will be applied to all data\n",
    "    '''Adds normal distributed noise to the tensor sequence with seperate mean and std for every signal'''\n",
    "    def __init__(self, std=1e-1,mean=0.,p=1.0):\n",
    "        super().__init__(p=p)\n",
    "        self.std = tensor(std).type(torch.float)\n",
    "        self.mean = tensor(mean).type(torch.float)\n",
    "        \n",
    "    def encodes(self, o:TensorSequencesInput): \n",
    "        if o.device != self.mean.device:\n",
    "            self.std = self.std.to(o.device)\n",
    "            self.mean = self.mean.to(o.device)\n",
    "        #expand creates a view on a tensor and is therefore very fast compared to copy\n",
    "        return o+torch.normal(mean=self.mean.expand_as(o), \n",
    "                              std=self.std.expand_as(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TensorSequencesInput(tensor([[1,1,1],[-1,-1,-1.0]]))\n",
    "ns_mean = tensor([0.,10.1,3.1])\n",
    "ns_std = tensor([1.,1.1,0.1])\n",
    "x,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_noise = SeqNoiseInjection(std=ns_std,mean=ns_mean)\n",
    "seq_noise(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_noise = SeqNoiseInjection(std=ns_std*10)\n",
    "seq_noise(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SeqNoiseInjection_Varying(RandTransform):\n",
    "    split_idx=0\n",
    "    '''Adds normal distributed noise to the tensor sequence with a normal distributed standard deviation for every application'''\n",
    "    def __init__(self, std_std=0.1,p=1.0):\n",
    "        super().__init__(p=p)\n",
    "        self.std_std = tensor(std_std).type(torch.float)\n",
    "        \n",
    "    def encodes(self, o:TensorSequencesInput): \n",
    "        if o.device != self.std_std.device:\n",
    "            self.std_std = self.std_std.to(o.device)\n",
    "            \n",
    "        #expand creates a view on a tensor and is therefore very fast compared to copy\n",
    "        std = torch.normal(mean=0,std=self.std_std).abs()\n",
    "        return o+torch.normal(mean=0,std=std.expand_as(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TensorSequencesInput(tensor([[0,0,0],[0,0,0]]))\n",
    "ns_std = tensor([1.,1.1,0.1])\n",
    "x,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_noise = SeqNoiseInjection_Varying(std_std=ns_std)\n",
    "seq_noise(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SeqNoiseInjection_Grouped(RandTransform):\n",
    "    split_idx=0\n",
    "    '''Adds normal distributed noise to the tensor sequence with a normal distributed standard deviation for every application, every group gert'''\n",
    "    def __init__(self, std_std,std_idx,p=1.0):\n",
    "        super().__init__(p=p)\n",
    "        self.std_std = tensor(std_std).type(torch.float)\n",
    "        self.std_idx = tensor(std_idx).type(torch.long)\n",
    "        \n",
    "    def encodes(self, o:TensorSequencesInput): \n",
    "        if o.device != self.std_std.device:\n",
    "            self.std_std = self.std_std.to(o.device)\n",
    "            \n",
    "        #expand creates a view on a tensor and is therefore very fast compared to copy\n",
    "        std = torch.normal(mean=0,std=self.std_std).abs()[self.std_idx]\n",
    "        return o+torch.normal(mean=0,std=std.expand_as(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TensorSequencesInput(tensor([[0,0,0],[0,0,0]]))\n",
    "ns_std = tensor([1.,1.1,0.1])\n",
    "x,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_noise = SeqNoiseInjection_Grouped(std_std=[3.,0],std_idx=[0,0,1])\n",
    "seq_noise(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sequence Bias Injection Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SeqBiasInjection(RandTransform):\n",
    "    split_idx=0\n",
    "    '''Adds a normal distributed offset to the tensor sequence with seperate mean and std for every signal'''\n",
    "    def __init__(self, std=1e-1,mean=0.,p=1.0):\n",
    "        super().__init__(p=p)\n",
    "        self.std = tensor(std).type(torch.float)\n",
    "        self.mean = tensor(mean).type(torch.float)\n",
    "        \n",
    "    def encodes(self, o:TensorSequencesInput): \n",
    "        if o.device != self.mean.device:\n",
    "            self.std = self.std.to(o.device)\n",
    "            self.mean = self.mean.to(o.device)\n",
    "        \n",
    "        #expand creates a view on a tensor and is therefore very fast compared to copy\n",
    "        mean=self.mean.repeat((o.shape[0],1,1)).expand((o.shape[0],1,o.shape[2]))\n",
    "        std= self.std.repeat((o.shape[0],1,1)).expand((o.shape[0],1,o.shape[2]))\n",
    "        n = torch.normal(mean=mean, std=std).expand_as(o)\n",
    "        return o+n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TensorSequencesInput(tensor([[[1,1,1],[-1,-1,-1.0]]]))\n",
    "ns_mean = tensor([0.,10.1,3.1])\n",
    "ns_std = tensor([1.,1.1,0.1])\n",
    "seq_bias = SeqBiasInjection(std=ns_std,mean=ns_std)\n",
    "seq_bias(x)[...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_bias.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_bias = SeqBiasInjection(std=ns_std*10)\n",
    "seq_bias(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Normalization\n",
    "`Normalize` is programmed for `TensorImage` as an input tensor. It gets. At init the variable axes need to be chosen correspondingly to the shape of your tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@Normalize\n",
    "def encodes(self, x:TensorSequencesInput): \n",
    "    if x.device != self.mean.device:\n",
    "        self.mean = self.mean.to(x.device)\n",
    "        self.std = self.std.to(x.device)\n",
    "    return (x-self.mean) / self.std\n",
    "\n",
    "@Normalize\n",
    "def decodes(self, x:TensorSequencesInput):\n",
    "    if x.device != self.mean.device:\n",
    "        self.mean = self.mean.to(x.device)\n",
    "        self.std = self.std.to(x.device)\n",
    "    return (x*self.std + self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalize.from_stats(mean=ns_mean,std=ns_std,dim=1,ndim=2,cuda=False)\n",
    "x,norm(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split in Training, Validation\n",
    "Splitting kann anhand von vorher bekannten Indizes, dem Dateipfad oder anderen allgemeinen Funktion durchgeführt werden.\n",
    "\n",
    "Splitting innerhalb einer Sequenzen sollte in der Praxis nur dann geschehen wenn eine einzige Sequenz vorhanden ist. Diese kann dann vorher manuell geteilt werden.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Splitting mit vorgegebenem Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = IndexSplitter([1,2])\n",
    "test_eq(splitter(hdf_files),[[0],[1,2]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Splitting mit allgemeiner Funktion\n",
    "Items, bei denen die definierte Funktion `True` zurück gibt, werden den Validierungsdatensatz zugeordnet, der Rest dem Training. In diesem Fall wird nach dem Übergeordneten Ordnernamen gesucht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = FuncSplitter(lambda o: Path(o).parent.name == 'valid')\n",
    "test_eq(splitter(hdf_files),[[0,1],[2]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Splitting anhand des Parent-Folders\n",
    "Splitter, der Explizit Training und Validierungsordner den Datensätzen zuordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _parent_idxs(items, name): return mask2idxs(Path(o).parent.name == name for o in items)\n",
    "\n",
    "def ParentSplitter(train_name='train', valid_name='valid'):\n",
    "    \"Split `items` from the parent folder names (`train_name` and `valid_name`).\"\n",
    "    def _inner(o, **kwargs):\n",
    "        return _parent_idxs(o, train_name),_parent_idxs(o, valid_name)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ParentSplitter()\n",
    "test_eq(splitter(hdf_files),[[0,1],[2]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Percentage Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def PercentageSplitter(pct=0.8):\n",
    "    \"Split `items` in order in relative quantity.\"\n",
    "    def _inner(o, **kwargs):\n",
    "        split_idx=int(len(o)*pct)\n",
    "        return L(range(split_idx)),L(range(split_idx,len(o)))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = PercentageSplitter(0.7)\n",
    "test_eq(splitter(hdf_files),[[0,1],[2]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Apply To Dictionary\n",
    "In Case of the Datablock API your items are a list of dictionaries. If you want to apply a Splitter to the path stored within you need a wrapper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ApplyToDict(fn,key='path'):\n",
    "    return lambda x:fn([i[key] for i in x])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Valid Column\n",
    "Using the 'valid' column of the Dataframe that has been created by a transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "valid_clm_splitter =  FuncSplitter(lambda o:o['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_clm_splitter(src_dicts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataloaders Creation\n",
    "A Datasets combines all implemented components on item level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def pad_sequence(batch,sorting = False):\n",
    "    '''collate_fn for padding of sequences of different lengths, use in before_batch of databunch, still quite slow'''\n",
    "    #takes list of tuples as input, returns list of tuples\n",
    "    sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True) if sorting else batch\n",
    "\n",
    "    pad_func = partial(torch.nn.utils.rnn.pad_sequence,batch_first=True)\n",
    "    padded_tensors = [pad_func([x[tup] for x in sorted_batch]) for tup in range(len(batch[0]))]\n",
    "    padded_list = [retain_types(tuple([tup[entry] for tup in padded_tensors]),batch[0]) for entry in range(len(batch))]\n",
    "    #retain types is important for decoding later back to source items\n",
    "#     import pdb; pdb.set_trace()\n",
    "    \n",
    "    return padded_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Low-Level with Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms=[  [HDF2Sequence(['current','voltage']),SeqSlice(l_slc=1),toTensorSequencesInput],\n",
    "        [HDF2Sequence(['voltage']),SeqSlice(r_slc=-1),toTensorSequencesOutput]]\n",
    "splits = splitter([x['path'] for x in src_dicts])\n",
    "dsrc = Datasets(src_dicts,tfms=tfms,splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# dsrc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dsrc.dataloaders(bs=128,after_batch=[SeqNoiseInjection(std=[1.1,0.01]),Normalize(axes=[0,1])],before_batch=pad_sequence)\n",
    "db.one_batch()[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Mid-Level with Datablock API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SequenceBlock(TransformBlock):\n",
    "    def __init__(self, seq_extract,padding=False):\n",
    "        return super().__init__(type_tfms=[seq_extract],\n",
    "                                batch_tfms=[Normalize(axes=[0,1])],\n",
    "                                dls_kwargs={} if not padding else {'before_batch': pad_sequence})\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(HDF2Sequence, keep=True)\n",
    "    def from_hdf(cls, clm_names, seq_cls=TensorSequencesInput,padding=False, **kwargs):\n",
    "        return cls(HDF2Sequence(clm_names,to_cls=seq_cls,**kwargs), padding)\n",
    "\n",
    "    @classmethod\n",
    "    def from_numpy(cls, seq_cls=TensorSequencesInput,padding=False, **kwargs):\n",
    "        return cls(ToTensor(enc=seq_cls), padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = DataBlock(blocks=(SequenceBlock.from_hdf(['current','voltage'],TensorSequencesInput,padding=True,cached=None),\n",
    "                        SequenceBlock.from_hdf(['voltage'],TensorSequencesOutput,cached=None)),\n",
    "                get_items=tfm_src,\n",
    "                splitter=ApplyToDict(ParentSplitter()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = seq.dataloaders(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ScalarNormalize(DisplayedTransform):\n",
    "    def __init__(self, mean=None, std=None, axes=(0,)): store_attr()\n",
    "        \n",
    "    @classmethod\n",
    "    def from_stats(cls, mean, std, dim=1, ndim=4, cuda=True): return cls(*broadcast_vec(dim, ndim, mean, std, cuda=cuda))\n",
    "    \n",
    "    def setups(self, dl:DataLoader):\n",
    "        if self.mean is None or self.std is None:\n",
    "            b = dl.one_batch()\n",
    "            for x in b:\n",
    "                if isinstance(x,TensorScalarsInput):\n",
    "                    self.mean,self.std = x.mean(self.axes, keepdim=True),x.std(self.axes, keepdim=True)+1e-7\n",
    "                    return\n",
    "\n",
    "    def encodes(self, x:TensorScalarsInput): \n",
    "        if x.device != self.mean.device:\n",
    "            self.mean = self.mean.to(x.device)\n",
    "            self.std = self.std.to(x.device)\n",
    "        return (x-self.mean) / self.std\n",
    "    \n",
    "    def decodes(self, x:TensorScalarsInput):\n",
    "        if x.device != self.mean.device:\n",
    "            self.mean = self.mean.to(x.device)\n",
    "            self.std = self.std.to(x.device)\n",
    "        return (x*self.std + self.mean)\n",
    "\n",
    "class ScalarBlock(TransformBlock):\n",
    "    def __init__(self, scl_extract):\n",
    "        return super().__init__(type_tfms=[scl_extract],\n",
    "                                batch_tfms=[ScalarNormalize()])\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(HDF2Scalars, keep=True)\n",
    "    def from_hdf(cls, clm_names, scl_cls=TensorScalarsInput, **kwargs):\n",
    "        return cls(HDF2Scalars(clm_names,to_cls=scl_cls,**kwargs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Spectrogram-Datablock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TensorSpectrogram(TensorBase):\n",
    "    def show(self, ctx=None, ax=None, title=\"\", **kwargs):\n",
    "        ax = ifnone(ax, ctx)\n",
    "        if ax is None:\n",
    "            _, ax = plt.subplots()\n",
    "        ax.axis(False)\n",
    "        n_channels = self.shape[0]\n",
    "        for i, channel in enumerate(self):\n",
    "            ia = ax.inset_axes((i / n_channels, 0.2, 1 / n_channels, 0.7))\n",
    "#             ia = ax.inset_axes((i / n_channels, 0, 1 / n_channels, 1))\n",
    "    \n",
    "            ia.imshow(channel.cpu().numpy(),aspect ='auto',origin ='lower')\n",
    "            if i>0: ia.set_yticks([])\n",
    "            ia.set_title(f\"Channel {i}\")\n",
    "        ax.set_title(title)\n",
    "        return ax\n",
    "\n",
    "class TensorSpectrogramInput(TensorSpectrogram): pass\n",
    "class TensorSpectrogramOutput(TensorSpectrogram): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_spec = TensorSpectrogramInput(HDF2Sequence(['current'],to_cls=TensorSpectrogramInput)._hdf_extract_sequence(hdf_files[0],r_slc=2000))\n",
    "seq_spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def complex_norm(\n",
    "        complex_tensor: Tensor,\n",
    "        power: float = 1.0\n",
    ") -> Tensor:\n",
    "    r\"\"\"Compute the norm of complex tensor input.\n",
    "\n",
    "    Args:\n",
    "        complex_tensor (Tensor): Tensor shape of `(..., complex=2)`\n",
    "        power (float): Power of the norm. (Default: `1.0`).\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Power of the normed input tensor. Shape of `(..., )`\n",
    "    \"\"\"\n",
    "    if power == 1.0:\n",
    "        return torch.norm(complex_tensor, 2, -1)\n",
    "    return torch.norm(complex_tensor, 2, -1).pow(power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def spectrogram(\n",
    "        waveform: Tensor,\n",
    "        pad: int,\n",
    "        window: Tensor,\n",
    "        n_fft: int,\n",
    "        hop_length: int,\n",
    "        win_length: int,\n",
    "        power: Optional[float],\n",
    "        normalized: bool\n",
    ") -> Tensor:\n",
    "    r\"\"\"Create a spectrogram or a batch of spectrograms from a raw audio signal.\n",
    "    The spectrogram can be either magnitude-only or complex.\n",
    "\n",
    "    Args:\n",
    "        waveform (Tensor): Tensor of audio of dimension (..., time)\n",
    "        pad (int): Two sided padding of signal\n",
    "        window (Tensor): Window tensor that is applied/multiplied to each frame/window\n",
    "        n_fft (int): Size of FFT\n",
    "        hop_length (int): Length of hop between STFT windows\n",
    "        win_length (int): Window size\n",
    "        power (float or None): Exponent for the magnitude spectrogram,\n",
    "            (must be > 0) e.g., 1 for energy, 2 for power, etc.\n",
    "            If None, then the complex spectrum is returned instead.\n",
    "        normalized (bool): Whether to normalize by magnitude after stft\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Dimension (..., freq, time), freq is\n",
    "        ``n_fft // 2 + 1`` and ``n_fft`` is the number of\n",
    "        Fourier bins, and time is the number of window hops (n_frame).\n",
    "    \"\"\"\n",
    "\n",
    "    if pad > 0:\n",
    "        # TODO add \"with torch.no_grad():\" back when JIT supports it\n",
    "        waveform = torch.nn.functional.pad(waveform, (pad, pad), \"constant\")\n",
    "\n",
    "    # pack batch\n",
    "    shape = waveform.size()\n",
    "    waveform = waveform.view(-1, shape[-1])\n",
    "\n",
    "    # default values are consistent with librosa.core.spectrum._spectrogram\n",
    "    spec_f = torch.view_as_real(torch.stft(\n",
    "        waveform, n_fft, hop_length, win_length, window, True, \"reflect\", False, True,return_complex=True\n",
    "    ))\n",
    "\n",
    "    # unpack batch\n",
    "    spec_f = spec_f.view(shape[:-1] + spec_f.shape[-3:])\n",
    "\n",
    "    if normalized:\n",
    "        spec_f /= window.pow(2.).sum().sqrt()\n",
    "    if power is not None:\n",
    "        spec_f = complex_norm(spec_f, power=power)\n",
    "\n",
    "    return spec_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "from typing import Callable\n",
    "\n",
    "class Spectrogram(torch.nn.Module):\n",
    "    r\"\"\"Create a spectrogram from a audio signal.\n",
    "\n",
    "    Args:\n",
    "        n_fft (int, optional): Size of FFT, creates ``n_fft // 2 + 1`` bins. (Default: ``400``)\n",
    "        win_length (int or None, optional): Window size. (Default: ``n_fft``)\n",
    "        hop_length (int or None, optional): Length of hop between STFT windows. (Default: ``win_length // 2``)\n",
    "        pad (int, optional): Two sided padding of signal. (Default: ``0``)\n",
    "        window_fn (Callable[..., Tensor], optional): A function to create a window tensor\n",
    "            that is applied/multiplied to each frame/window. (Default: ``torch.hann_window``)\n",
    "        power (float or None, optional): Exponent for the magnitude spectrogram,\n",
    "            (must be > 0) e.g., 1 for energy, 2 for power, etc.\n",
    "            If None, then the complex spectrum is returned instead. (Default: ``2``)\n",
    "        normalized (bool, optional): Whether to normalize by magnitude after stft. (Default: ``False``)\n",
    "        wkwargs (dict or None, optional): Arguments for window function. (Default: ``None``)\n",
    "    \"\"\"\n",
    "    __constants__ = ['n_fft', 'win_length', 'hop_length', 'pad', 'power', 'normalized']\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_fft: int = 400,\n",
    "                 win_length: Optional[int] = None,\n",
    "                 hop_length: Optional[int] = None,\n",
    "                 pad: int = 0,\n",
    "                 window_fn: Callable[..., Tensor] = torch.hann_window,\n",
    "                 power: Optional[float] = 2.,\n",
    "                 normalized: bool = False,\n",
    "                 wkwargs: Optional[dict] = None) -> None:\n",
    "        super(Spectrogram, self).__init__()\n",
    "        self.n_fft = n_fft\n",
    "        # number of FFT bins. the returned STFT result will have n_fft // 2 + 1\n",
    "        # number of frequecies due to onesided=True in torch.stft\n",
    "        self.win_length = win_length if win_length is not None else n_fft\n",
    "        self.hop_length = hop_length if hop_length is not None else self.win_length // 2\n",
    "        window = window_fn(self.win_length) if wkwargs is None else window_fn(self.win_length, **wkwargs)\n",
    "        self.register_buffer('window', window)\n",
    "        self.pad = pad\n",
    "        self.power = power\n",
    "        self.normalized = normalized\n",
    "\n",
    "    def forward(self, waveform: Tensor) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            waveform (Tensor): Tensor of audio of dimension (..., time).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Dimension (..., freq, time), where freq is\n",
    "            ``n_fft // 2 + 1`` where ``n_fft`` is the number of\n",
    "            Fourier bins, and time is the number of window hops (n_frame).\n",
    "        \"\"\"\n",
    "        return spectrogram(waveform, self.pad, self.window, self.n_fft, self.hop_length,\n",
    "                             self.win_length, self.power, self.normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@delegates(Spectrogram, keep=True)\n",
    "class Sequence2Spectrogram(Transform):\n",
    "    '''calculates the FFT of a sequence'''\n",
    "    \n",
    "    def __init__(self,scaling='log',**kwargs): \n",
    "        self.scaling=scaling\n",
    "        self.tfm = Spectrogram(**kwargs)\n",
    "        \n",
    "    def encodes(self, o:TensorSpectrogram): \n",
    "        if o.device != self.tfm.window.device: self.tfm.window = self.tfm.window.to(o.device)\n",
    "#         import pdb;pdb.set_trace()\n",
    "        spec = self.tfm(o.transpose(-1,-2).contiguous())\n",
    "        if self.scaling == 'log': spec = torch.log10(spec + 1e-10)\n",
    "        return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SpectrogramBlock(TransformBlock):\n",
    "    def __init__(self, seq_extract,padding=False,n_fft=100,hop_length=None,normalized=False):\n",
    "        return super().__init__(type_tfms=[seq_extract],\n",
    "                                batch_tfms=[Sequence2Spectrogram(n_fft=n_fft,hop_length=hop_length,normalized=normalized)],\n",
    "                                dls_kwargs={} if not padding else {'before_batch': pad_sequence})\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(HDF2Sequence, keep=True)\n",
    "    def from_hdf(cls, clm_names, seq_cls=TensorSpectrogramInput,padding=False,n_fft=100,hop_length=None,normalized=False, **kwargs):\n",
    "        return cls(HDF2Sequence(clm_names,to_cls=seq_cls,**kwargs), padding,n_fft=n_fft,hop_length=hop_length,normalized=normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_spec = DataBlock(blocks=(SpectrogramBlock.from_hdf(['current','voltage'],n_fft=100,hop_length=10,normalized=True),\n",
    "                        SequenceBlock.from_hdf(['voltage'],TensorSequencesOutput)),\n",
    "                get_items= CreateDict([DfHDFCreateWindows(win_sz=2000+1,stp_sz=10,clm='current')]),\n",
    "                splitter=ApplyToDict(ParentSplitter())).dataloaders(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_spec.one_batch()[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Show Batches and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_sequence(axs,in_sig,targ_sig,out_sig=None,**kwargs):\n",
    "    n_targ = targ_sig.shape[1]\n",
    "    for j,ax in  enumerate(axs[:-1]):\n",
    "        ax.plot(targ_sig[:,j])\n",
    "        if out_sig is not None: \n",
    "            ax.plot(out_sig[:,j])\n",
    "            ax.legend(['y','ŷ'])\n",
    "            if 'ref' in kwargs:\n",
    "                ax.plot(kwargs['ref'][:,j]) \n",
    "        ax.label_outer()\n",
    "    axs[-1].plot(in_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_seqs_single_figure(n_samples,n_targ,samples,plot_func,outs=None,**kwargs):\n",
    "    rows=max(1,((n_samples-1) // 3)+1)\n",
    "    cols=min(3,n_samples)\n",
    "    fig = plt.figure(figsize=(9,2*cols))\n",
    "    outer_grid = fig.add_gridspec(rows, cols)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    for i in range(n_samples):\n",
    "        in_sig = samples[i][0]\n",
    "        targ_sig = samples[i][1]\n",
    "        if outs is not None: out_sig = outs[i][0]\n",
    "        inner_grid = outer_grid[i].subgridspec(n_targ+1, 1)\n",
    "        axs = [fig.add_subplot(inner_grid[j]) for j in range(n_targ+1)]\n",
    "        plot_func(axs,in_sig,targ_sig,out_sig=out_sig if outs is not None else None,**kwargs)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_seqs_multi_figures(n_samples,n_targ,samples,plot_func,outs=None,**kwargs):\n",
    "    for i in range(n_samples):\n",
    "        fig = plt.figure(figsize=(9,3))\n",
    "        axs = fig.subplots(nrows=n_targ+1,sharex=True)\n",
    "        in_sig = samples[i][0]\n",
    "        targ_sig = samples[i][1]\n",
    "        if outs is not None:  out_sig = outs[i][0]\n",
    "            \n",
    "        plot_func(axs,in_sig,targ_sig,out_sig=out_sig if outs is not None else None,**kwargs)\n",
    "        \n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@typedispatch\n",
    "def show_batch(x:TensorSequences, y:TensorSequences, samples, ctxs=None, max_n=6, **kwargs):\n",
    "    n_samples = min(len(samples), max_n)\n",
    "    n_targ = samples[0][1].shape[1]\n",
    "    if n_samples > 3:\n",
    "        #if there are more then 3 samples to plot then put them in a single figure\n",
    "        plot_seqs_single_figure(n_samples,n_targ,samples,plot_sequence, **kwargs)\n",
    "    else:\n",
    "        #if there are less then 3 samples to plot then put each in its own figure\n",
    "        plot_seqs_multi_figures(n_samples,n_targ,samples,plot_sequence, **kwargs)\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@typedispatch\n",
    "def show_results(x:TensorSequences, y:TensorSequences, samples, outs, ctxs=None, max_n=2, **kwargs):\n",
    "    n_samples = min(len(samples), max_n)\n",
    "    n_targ = samples[0][1].shape[1]\n",
    "    if n_samples > 3:\n",
    "        #if there are more then 3 samples to plot then put them in a single figure\n",
    "        plot_seqs_single_figure(n_samples,n_targ,samples,plot_sequence,outs, **kwargs)\n",
    "    else:\n",
    "        #if there are less then 3 samples to plot then put each in its own figure\n",
    "        plot_seqs_multi_figures(n_samples,n_targ,samples,plot_sequence,outs, **kwargs)\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
