{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext line_profiler\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corefunctions\n",
    "> Corefunctionality for data preparation of sequential data for pytorch, fastai models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Structure\n",
    "The data will be extracted and prepared via transforms. Those are grouped in:\n",
    "- Type Transforms: Those extraxt the needed components from the source items, like input sequences or target scalar values. The work on single tensors.\n",
    "- Item Transforms: Those Transforms may work on tuple level and therefore may process relationships between input and output.\n",
    "- Batch Transform: Those transforms work on batch level. They receive batched tensors and may apply lazy transforms like normalization very effeciently.\n",
    "\n",
    "An application example may look like the following:\n",
    "- sourceitems: \n",
    "    - path extraction with hdf5 file endings\n",
    "    - create pandas dataframe with information for type transforms, like slices\n",
    "    - filter items in pandas dataframe\n",
    "- type transforms: \n",
    "    - extract hdf5 input and output sequence\n",
    "    - create windows\n",
    "- item transforms: \n",
    "    - filter sequence by value\n",
    "    - shift output sequence by 1 element\n",
    "- batch transforms: \n",
    "    - noise injection\n",
    "    - normalization\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.augment import RandTransform\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def obj_in_lst(lst,cls):\n",
    "    '''retrieve first object of type cls from a list'''\n",
    "    return next(o for o in lst if type(o) is cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def count_parameters(model):\n",
    "    '''retrieve number of trainable parameters of a model'''\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Source Items\n",
    "The file paths may be extracted with `get_files` of fastai2. `get_hdf_files` removes the need of writing the hdf5 file extension.\n",
    "\n",
    "Then a pandas dataframe may be created in case further information for the source items need to be stored like slices for the windowing function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Extract File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, Path('test_data/train/Sim_RealisticCycle1.hdf5'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_path = 'test_data/'\n",
    "hdf_files = get_files(f_path,extensions='.hdf5',recurse=True)\n",
    "len(hdf_files),hdf_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "hdf_extensions = ['.hdf5']\n",
    "def get_hdf_files(path,recurse=True, folders=None):\n",
    "    \"Get hdf5 files in `path` recursively, only in `folders`, if specified.\"\n",
    "    return get_files(path, extensions=hdf_extensions, recurse=recurse, folders=folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, Path('test_data/train/Sim_RealisticCycle1.hdf5'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf_files = get_hdf_files(f_path)\n",
    "len(hdf_files),hdf_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create Source Dictionaries\n",
    "In order to extract mulitple realizations of one file with different modifications, we create a list of properties. Pandas Dataframes are to slow for iteration but very fast and convenient for creations. So after creation of the pandas Dataframe we convert it to a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def apply_df_tfms(src,pd_tfms = None):\n",
    "    '''Create Pandas Dataframe out of a list of items, with a list of df transforms applied'''\n",
    "    if type(src) is pd.DataFrame:\n",
    "        df = src\n",
    "    else:\n",
    "        df = pd.DataFrame(data=src.items,columns=['path'],dtype=str)\n",
    "    if pd_tfms is not None:\n",
    "        for t in pd_tfms:\n",
    "            df = t(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path\n",
       "0  test_data/train/Sim_RealisticCycle1.hdf5\n",
       "1  test_data/train/Sim_RealisticCycle2.hdf5\n",
       "2  test_data/valid/Sim_RealisticCycle3.hdf5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = apply_df_tfms(hdf_files)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(apply_df_tfms(hdf_files),apply_df_tfms(apply_df_tfms(hdf_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def CreateDict(pd_tfms = None):\n",
    "    '''Create List of Dictionarys out of a list of items, with a list of df transforms applied'''\n",
    "    def _inner(src):\n",
    "        df = apply_df_tfms(src,pd_tfms)\n",
    "#         df_dict_list = df.to_dict(orient='records') native to_dict is slower than self written approach\n",
    "        df_values = df.values\n",
    "        df_dict = {name:list(df_values[:,i]) for (i,name) in enumerate(df.columns)}\n",
    "        df_dict_list = [{name: df_dict[name][i] for name in df_dict} for i in range(len(df))]\n",
    "        return df_dict_list\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': 'test_data/train/Sim_RealisticCycle1.hdf5'},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle2.hdf5'},\n",
       " {'path': 'test_data/valid/Sim_RealisticCycle3.hdf5'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_dict =CreateDict()(hdf_files)\n",
    "l_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ValidClmContains(lst_valid):\n",
    "    '''add validation column using a list of strings that are part of the validation frames'''\n",
    "    def _inner(df):\n",
    "        re_valid = '|'.join([re.escape(f) for f in lst_valid])\n",
    "        df['valid'] = df.path.str.contains(re_valid)\n",
    "        return df\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.87 ms, sys: 0 ns, total: 2.87 ms\n",
      "Wall time: 2.85 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'path': 'test_data/train/Sim_RealisticCycle1.hdf5', 'valid': False},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle2.hdf5', 'valid': False},\n",
       " {'path': 'test_data/valid/Sim_RealisticCycle3.hdf5', 'valid': True}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lst_valid = ['valid']\n",
    "CreateDict([ValidClmContains(lst_valid)])(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ValidClmIs(lst_valid):\n",
    "    '''adds validation column using a list of validation filenames'''\n",
    "    def _inner(df):\n",
    "        df['valid'] = df.path.isin([str(f) for f in lst_valid])\n",
    "        return df\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.43 ms, sys: 0 ns, total: 2.43 ms\n",
      "Wall time: 2.42 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'path': 'test_data/train/Sim_RealisticCycle1.hdf5', 'valid': False},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle2.hdf5', 'valid': True},\n",
       " {'path': 'test_data/valid/Sim_RealisticCycle3.hdf5', 'valid': True}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lst_valid = ['test_data/train/Sim_RealisticCycle2.hdf5','test_data/valid/Sim_RealisticCycle3.hdf5']\n",
    "CreateDict([ValidClmIs(lst_valid)])(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def FilterClm(clm_name,func = lambda x:x):\n",
    "    '''adds validation column using a list of validation filenames'''\n",
    "    def _inner(df):\n",
    "        return df[func(df[clm_name])]\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': 'test_data/train/Sim_RealisticCycle2.hdf5', 'valid': True},\n",
       " {'path': 'test_data/valid/Sim_RealisticCycle3.hdf5', 'valid': True}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CreateDict([ValidClmIs(lst_valid),FilterClm('valid')])(hdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_hdf_seq_len(f_path,clm):\n",
    "    '''extract the sequence length of the dataset with the 'clm' name and 'f_path' path  '''\n",
    "    with h5py.File(f_path,'r') as f:\n",
    "        f_len = max(f[clm].shape)\n",
    "    return f_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def df_get_hdf_seq_len(df,clm):\n",
    "    '''extracts the sequence length of every file in advance to prepare repeated window extractions with 'DfHDFCreateWindows' '''\n",
    "#     df['seq_len'] = ([get_hdf_seq_len(row.path,clm) for (idx, row) in df.iterrows()])\n",
    "    df['seq_len'] = df.path.apply(lambda x: get_hdf_seq_len(x,clm))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def DfHDFGetSeqLen(clm):\n",
    "    def _inner(df):\n",
    "        return df_get_hdf_seq_len(df,clm)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>265598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path  seq_len\n",
       "0  test_data/train/Sim_RealisticCycle1.hdf5   265607\n",
       "1  test_data/train/Sim_RealisticCycle2.hdf5   265598\n",
       "2  test_data/valid/Sim_RealisticCycle3.hdf5   265593"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_get_hdf_seq_len(df,'current')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>265598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path  seq_len\n",
       "0  test_data/train/Sim_RealisticCycle1.hdf5   265607\n",
       "1  test_data/train/Sim_RealisticCycle2.hdf5   265598\n",
       "2  test_data/valid/Sim_RealisticCycle3.hdf5   265593"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DfHDFGetSeqLen('current')(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numbers\n",
    "def DfResamplingFactor(src_fs,lst_targ_fs):\n",
    "    if not isinstance(src_fs, numbers.Number) and not type(src_fs) is str: \n",
    "        raise ValueError('src_fs has to be a column name or a fixed number')\n",
    "    \n",
    "    def _inner(df):\n",
    "        np_targ_fs = array(lst_targ_fs)\n",
    "        pd.options.mode.chained_assignment = None #every row is a reference so we need to suppress the warning messages while copying\n",
    "\n",
    "        #repeat entries for every target fs\n",
    "        res_df = df.iloc[np.repeat(np.arange(len(df)),len(np_targ_fs))] \n",
    "        targ_fs = np.tile(np_targ_fs,len(df))\n",
    "        res_df['targ_fs'] = targ_fs\n",
    "        \n",
    "        if isinstance(src_fs, numbers.Number):\n",
    "            #src_fs is a fixed number\n",
    "            res_df['resampling_factor'] = targ_fs/src_fs\n",
    "        else:\n",
    "            #src_fs is a column name of the df\n",
    "            res_df['resampling_factor'] = targ_fs/res_df[src_fs]\n",
    "\n",
    "        pd.options.mode.chained_assignment = 'warn'\n",
    "        \n",
    "        return res_df\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_fs = [50,100,300]\n",
    "test_eq(len(DfResamplingFactor(100,targ_fs)(df)),9)   \n",
    "df['src_fs'] = 200.\n",
    "test_eq(len(DfResamplingFactor('src_fs',targ_fs)(df)),9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def DfHDFCreateWindows(win_sz,stp_sz, clm, fixed_start = False, fixed_end = False):\n",
    "    '''create windows of sequences, splits sequence into multiple items'''\n",
    "    def _inner(df):\n",
    "        if fixed_start and fixed_end: raise Exception\n",
    "        \n",
    "        if 'seq_len' in df:\n",
    "            np_f_len = df.seq_len.values\n",
    "        else:\n",
    "            np_f_len = np.array([get_hdf_seq_len(row.path,clm) for (idx, row) in df.iterrows()])\n",
    "            \n",
    "        if 'resampling_factor' in df: np_f_len =(np_f_len*df.resampling_factor.values).astype(int)\n",
    "            \n",
    "        n_win = ((np_f_len-win_sz)//stp_sz)+1\n",
    "        n_win = np.clip(n_win,a_min=0,a_max=None) #remove negative values at instances where the winsize is smaller than the seq_len\n",
    "        lst_idx = np.arange(len(np_f_len))\n",
    "        \n",
    "        pd.options.mode.chained_assignment = None #every row is a reference so we need to suppress the warning messages while copying\n",
    "        \n",
    "        res_df = df.iloc[np.repeat(lst_idx,n_win)]\n",
    "#         res_df = df.loc[np.repeat(lst_idx,n_win)] #the loc variant as a little bit slower because it creates copies and returns wrong values with redundant indexes, but is more robust\n",
    "\n",
    "        step_idx = np.concatenate([np.arange(x) for x in n_win])\n",
    "    \n",
    "        \n",
    "        res_df['l_slc'] = step_idx*stp_sz if not fixed_start else None\n",
    "        res_df['r_slc'] = step_idx*stp_sz + win_sz if not fixed_end else None\n",
    "            \n",
    "        pd.options.mode.chained_assignment = 'warn'\n",
    "            \n",
    "        return res_df\n",
    "    \n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.02 ms, sys: 23 µs, total: 4.05 ms\n",
      "Wall time: 3.67 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>src_fs</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>400</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>200.0</td>\n",
       "      <td>265000</td>\n",
       "      <td>265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>200.0</td>\n",
       "      <td>265100</td>\n",
       "      <td>265200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>200.0</td>\n",
       "      <td>265200</td>\n",
       "      <td>265300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>200.0</td>\n",
       "      <td>265300</td>\n",
       "      <td>265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>200.0</td>\n",
       "      <td>265400</td>\n",
       "      <td>265500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7966 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        path  seq_len  src_fs   l_slc   r_slc\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5   265607   200.0       0     100\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5   265607   200.0     100     200\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5   265607   200.0     200     300\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5   265607   200.0     300     400\n",
       "0   test_data/train/Sim_RealisticCycle1.hdf5   265607   200.0     400     500\n",
       "..                                       ...      ...     ...     ...     ...\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5   265593   200.0  265000  265100\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5   265593   200.0  265100  265200\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5   265593   200.0  265200  265300\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5   265593   200.0  265300  265400\n",
       "2   test_data/valid/Sim_RealisticCycle3.hdf5   265593   200.0  265400  265500\n",
       "\n",
       "[7966 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "create_win = DfHDFCreateWindows(win_sz=100,stp_sz=100,clm='current')\n",
    "win_df = create_win(df)\n",
    "win_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>src_fs</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>265594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>265598</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>265594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path  seq_len  src_fs  l_slc   r_slc\n",
       "0  test_data/train/Sim_RealisticCycle1.hdf5   265607   200.0      0  265594\n",
       "1  test_data/train/Sim_RealisticCycle2.hdf5   265598   200.0      0  265594"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "win_df = DfHDFCreateWindows(win_sz=265594,stp_sz=100,clm='current')(df)\n",
    "test_eq(len(win_df),2)\n",
    "win_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(create_win(df_get_hdf_seq_len(df,'current')) , create_win(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>src_fs</th>\n",
       "      <th>targ_fs</th>\n",
       "      <th>resampling_factor</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>265598</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "      <td>265593</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path  seq_len  src_fs  targ_fs  \\\n",
       "0  test_data/train/Sim_RealisticCycle1.hdf5   265607   200.0      0.1   \n",
       "1  test_data/train/Sim_RealisticCycle2.hdf5   265598   200.0      0.1   \n",
       "2  test_data/valid/Sim_RealisticCycle3.hdf5   265593   200.0      0.1   \n",
       "\n",
       "   resampling_factor  l_slc  r_slc  \n",
       "0             0.0005      0    100  \n",
       "1             0.0005      0    100  \n",
       "2             0.0005      0    100  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_win_df = create_win(DfResamplingFactor(200,[0.1])(df))\n",
    "res_win_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(res_win_df),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>src_fs</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "      <td>265607</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>265594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>265598</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>265594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path  seq_len  src_fs  l_slc   r_slc\n",
       "0  test_data/train/Sim_RealisticCycle1.hdf5   265607   200.0      0  265594\n",
       "1  test_data/train/Sim_RealisticCycle2.hdf5   265598   200.0      0  265594"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_expr = 'l_slc <= 200'\n",
    "filt_df = win_df.query(query_expr)\n",
    "filt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def DfFilterQuery(query):\n",
    "    def _inner(df):\n",
    "        return df.query(query)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(DfFilterQuery(query_expr)(win_df),filt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 75.8 ms, sys: 15.1 ms, total: 90.9 ms\n",
      "Wall time: 89.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'path': 'test_data/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 0,\n",
       "  'r_slc': 101},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 10,\n",
       "  'r_slc': 111},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 20,\n",
       "  'r_slc': 121},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 30,\n",
       "  'r_slc': 131},\n",
       " {'path': 'test_data/train/Sim_RealisticCycle1.hdf5',\n",
       "  'valid': False,\n",
       "  'l_slc': 40,\n",
       "  'r_slc': 141}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tfm_src = CreateDict([ValidClmContains(['valid']),DfHDFCreateWindows(win_sz=100+1,stp_sz=10,clm='current')])\n",
    "src_dicts = tfm_src(hdf_files)\n",
    "src_dicts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def DfDropClmExcept(clms = ['path','l_slc','r_slc','p_sample','resampling_factor']):\n",
    "    '''drop unused dataframe columns as a last optional step to accelerate dictionary conversion'''\n",
    "    def _inner(df):\n",
    "        return df[[c for c in clms if c in df]]\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert Paths to Sequence Objects\n",
    "Der Pfad wird unter Angabe der Spaltennamen in Sequenzen und Skalare Werte umgewandelt, um so am Ende ein 3-Tupel zu erhalten aus:\n",
    "- (Sequence, Scalar, Sequence) <-> (input,input,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extract sequential data from hdf5-files\n",
    "Two different functions, based on pandas df and on lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Shift time Series\n",
    "Sometimes we need to shift columns of a sequence by a specific value. Then we cant simply slice the array but have to handle each column individually. First a performance test has to be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calc_shift_offsets(clm_shift):\n",
    "    clm_shift = array(clm_shift)\n",
    "    l_offs = -min(clm_shift.min(),0)\n",
    "    r_offs = -max(clm_shift.max(),0)\n",
    "    l_shift = clm_shift+l_offs\n",
    "    r_shift = clm_shift+r_offs\n",
    "    dim_red = l_offs-r_offs\n",
    "    return l_shift,r_shift,dim_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 0, 2]), array([-1, -1, -2,  0]), 2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shft = [0,0,-1,1]\n",
    "calc_shift_offsets(shft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both shifting methods have their own performance character. vstack needs double the time on short sequences, while the creation of a seperate array with copy becomes worse starting at around 5000 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ta = array([[1,2,3]*2]*10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# y = np.vstack([ta[i:-ta.shape[1]+i,i] for i in range(ta.shape[1])]).T   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# x = np.zeros((ta.shape[0]-ta.shape[1],ta.shape[1]))\n",
    "# for i in range(ta.shape[1]):\n",
    "#     x[:,i] = ta[i:-ta.shape[1]+i,i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 HDF2Sequence\n",
    "HDF5 performance is massively affected by the dtype of the signals. f4 (32 bit floating point) Numbers are faster to load and lead to smaller files then f8 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0,axis=0),axis=0) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def downsample_mean(x,N):\n",
    "    shp = x.shape\n",
    "    trunc = -(x.shape[0] % N)\n",
    "    trunc = trunc if trunc != 0 else None\n",
    "    return x[:trunc,:].reshape((-1,N,x.shape[-1])).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from scipy.signal import butter, lfilter, lfilter_zi\n",
    "from scipy import signal\n",
    "def resample_interp(x,resampling_factor,sequence_first=True, lowpass_cut=0.7, upsample_cubic_cut = None):\n",
    "    '''signal resampling using linear or cubic interpolation\n",
    "    \n",
    "    x: signal to resample with shape: features x resampling_dimension or resampling_dimension x  features if sequence_first=True\n",
    "    resampling_factor: Factor > 0 that scales the signal\n",
    "    lowpass_cut: Upper boundary for resampling_factor that activates the lowpassfilter, low values exchange accuracy for performance, default is 0.7\n",
    "    upsample_cubic_cut: Lower boundary for resampling_factor that activates cubic interpolation at high upsampling values. \n",
    "                        Improves signal dynamics in exchange of performance. None deactivates cubic interpolation\n",
    "    '''\n",
    "    \n",
    "    if sequence_first:\n",
    "        x = x.T\n",
    "    \n",
    "    fs_n = resampling_factor\n",
    "    #if downsampling rate is too high, lowpass filter before interpolation\n",
    "    if fs_n < lowpass_cut:\n",
    "        b,a = butter(2, fs_n)\n",
    "        zi = lfilter_zi(b,a)*x[:,:1] #initialize filter with steady state at first time step value\n",
    "        x,_ = lfilter(b,a,x,axis=-1,zi=zi)\n",
    "\n",
    "#         sos = butter(2, fs_n*1.2,output='sos')\n",
    "# #         sos = signal.cheby2(2,20, fs_n,output='sos')\n",
    "# #         import pdb;pdb.set_trace()\n",
    "#         zi = np.swapaxes(signal.sosfilt_zi(sos)[...,None]*x[:,0],1,2)\n",
    "#         x,_ = signal.sosfilt(sos, x,axis=-1,zi=zi)\n",
    "        \n",
    "    x_int = tensor(x)[None,...]\n",
    "    targ_size = int(x.shape[-1]*fs_n)\n",
    "    \n",
    "#     if upsampling rate is too high, switch from linear to cubic interpolation\n",
    "    if upsample_cubic_cut is None or fs_n <= upsample_cubic_cut:\n",
    "        x = array(nn.functional.interpolate(x_int, size=targ_size, mode='linear',align_corners=False)[0])\n",
    "    else:\n",
    "        x = array(nn.functional.interpolate(x_int[...,None], size=[targ_size,1], mode='bicubic',align_corners=False)[0,...,0])\n",
    "#     x = array(x_int)[0]\n",
    "    \n",
    "    if sequence_first:\n",
    "        x = x.T\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(size=(100000,9))\n",
    "test_eq(resample_interp(x,0.3).shape[0],30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plotFFT(buffer,sample_rate,label,fig=plt):\n",
    "#     xf,yf = calcFFT(buffer,sample_rate)\n",
    "#     fig.plot(xf, yf,label=label)\n",
    "    \n",
    "# def calcFFT(buffer,sample_rate):\n",
    "#     N = buffer.shape[0]\n",
    "#     T = 1.0 / sample_rate\n",
    "#     x = np.linspace(0.0, N*T, N)\n",
    "#     y = buffer\n",
    "#     yf = scipy.fft.fft(y)\n",
    "#     xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "#     return xf,2.0/N * np.abs(yf[:N//2])\n",
    "\n",
    "# src_fs= 200\n",
    "# n_fs = 0.9\n",
    "# plt.figure()\n",
    "# plotFFT(x[:,0],src_fs,'raw')\n",
    "# plotFFT(resample_interp(x,n_fs)[:,0],n_fs*src_fs,'filtered')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hdf_extract_sequence(hdf_path,clms,dataset = None, l_slc = None, r_slc= None, resampling_factor=None, fs_idx =None,dt_idx =False):\n",
    "    '''\n",
    "    extracts a sequence with the shape [seq_len x num_features]\n",
    "    \n",
    "    hdf_path: file path of hdf file, may be a string or path type\n",
    "    clms: list of dataset names of sequences in hdf file\n",
    "    dataset: dataset root for clms. Useful for multiples sequences stored in one file.\n",
    "    l_slc: left boundary for extraction of a window of the whole sequence\n",
    "    r_slc: right boundary for extraction of a window of the whole sequence\n",
    "    resampling_factor: scaling factor for the sequence length, uses 'resample_interp' for resampling\n",
    "    fs_idx: clms list idx of fs entry in sequence. Will be scaled by resampling_factor after resampling\n",
    "    dt_idx: clms list idx of dt entry in sequence. Will be scaled by resampling_factor after resampling\n",
    "    '''\n",
    "\n",
    "    if resampling_factor is not None:\n",
    "        seq_len = r_slc-l_slc if l_slc is not None and r_slc is not None else None #calculate seq_len for later slicing, necesary because of rounding errors in resampling\n",
    "        if l_slc is not None: l_slc= math.floor(l_slc/resampling_factor)\n",
    "        if r_slc is not None: r_slc= math.ceil(r_slc/resampling_factor)\n",
    "\n",
    "    with h5py.File(hdf_path,'r') as f:\n",
    "        ds = f if dataset is None else f[dataset]\n",
    "        l_array = [(ds[n][l_slc:r_slc]) for n in clms]\n",
    "        seq = np.stack(l_array,axis=-1)\n",
    "\n",
    "    if resampling_factor is not None:\n",
    "        res_seq = resample_interp(seq,resampling_factor)\n",
    "        if fs_idx is not None: res_seq[:,fs_idx] = seq[0,fs_idx] * resampling_factor\n",
    "        if dt_idx is not None: res_seq[:,dt_idx] = seq[0,dt_idx] / resampling_factor\n",
    "        seq = res_seq\n",
    "        \n",
    "        if seq_len is not None: seq = seq[:seq_len] #cut the part of the sequence that is too long because of resampling rounding errors\n",
    "        \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Memoize:\n",
    "    def __init__(self, fn):\n",
    "        self.fn = fn\n",
    "        self.memo = {}\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        if args not in self.memo:\n",
    "            self.memo[args] = self.fn(*args)\n",
    "        return self.memo[args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HDF2Sequence(Transform):\n",
    "    \n",
    "    def __init__(self, clm_names,clm_shift=None,truncate_sz=None,to_cls=noop,cached=False, fs_idx =None,dt_idx =False):\n",
    "        if not clm_shift is None:\n",
    "            assert len(clm_shift)==len(clm_names) and all(isinstance(n, int) for n in clm_shift)\n",
    "            self.l_shift,self.r_shift,_ = calc_shift_offsets(clm_shift)\n",
    "        \n",
    "        self._exseq = Memoize(self._hdf_extract_sequence) if cached else self._hdf_extract_sequence\n",
    "        \n",
    "        store_attr('clm_names,clm_shift,truncate_sz,to_cls,cached,fs_idx,dt_idx')\n",
    "        \n",
    "    def _hdf_extract_sequence(self,hdf_path,dataset = None, l_slc = None, r_slc= None, resampling_factor=None, fs_idx =None,dt_idx =False):\n",
    "        '''\n",
    "        extracts a sequence with the shape [seq_len x num_features]\n",
    "\n",
    "        hdf_path: file path of hdf file, may be a string or path type\n",
    "        clms: list of dataset names of sequences in hdf file\n",
    "        dataset: dataset root for clms. Useful for multiples sequences stored in one file.\n",
    "        l_slc: left boundary for extraction of a window of the whole sequence\n",
    "        r_slc: right boundary for extraction of a window of the whole sequence\n",
    "        resampling_factor: scaling factor for the sequence length, uses 'resample_interp' for resampling\n",
    "        fs_idx: clms list idx of fs entry in sequence. Will be scaled by resampling_factor after resampling\n",
    "        dt_idx: clms list idx of dt entry in sequence. Will be scaled by resampling_factor after resampling\n",
    "        '''\n",
    "\n",
    "        if resampling_factor is not None:\n",
    "            seq_len = r_slc-l_slc if l_slc is not None and r_slc is not None else None #calculate seq_len for later slicing, necesary because of rounding errors in resampling\n",
    "            if l_slc is not None: l_slc= math.floor(l_slc/resampling_factor)\n",
    "            if r_slc is not None: r_slc= math.ceil(r_slc/resampling_factor)\n",
    "\n",
    "        with h5py.File(hdf_path,'r') as f:\n",
    "            ds = f if dataset is None else f[dataset]\n",
    "            l_array = [(ds[n][l_slc:r_slc]) for n in self.clm_names]\n",
    "            seq = np.stack(l_array,axis=-1)\n",
    "\n",
    "        if resampling_factor is not None:\n",
    "            res_seq = resample_interp(seq,resampling_factor)\n",
    "            if fs_idx is not None: res_seq[:,fs_idx] = seq[0,fs_idx] * resampling_factor\n",
    "            if dt_idx is not None: res_seq[:,dt_idx] = seq[0,dt_idx] / resampling_factor\n",
    "            seq = res_seq\n",
    "\n",
    "            if seq_len is not None: seq = seq[:seq_len] #cut the part of the sequence that is too long because of resampling rounding errors\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def _extract_dict_sequence(self,item):\n",
    "        if isinstance(item,dict):\n",
    "            path = item['path']\n",
    "            dataset = item['dataset'] if 'dataset' in item else None\n",
    "            l_slc = item['l_slc'] if 'l_slc' in item else None\n",
    "            r_slc = item['r_slc'] if 'r_slc' in item else None\n",
    "            resampling_factor = item['resampling_factor'] if 'resampling_factor' in item else None\n",
    "\n",
    "            if self.cached:\n",
    "                seq = self._exseq(path,dataset,None,None,resampling_factor,self.fs_idx,self.dt_idx)[l_slc:r_slc]\n",
    "            else:\n",
    "                seq = self._exseq(path,dataset,l_slc,r_slc,resampling_factor,self.fs_idx,self.dt_idx)\n",
    "        else:\n",
    "            seq = self._exseq(str(item),None,None,None,None)\n",
    "\n",
    "        #shift clms of result by given value \n",
    "        if not self.clm_shift is None:\n",
    "            l_seq = seq.shape[0]\n",
    "            seq = np.stack([seq[self.l_shift[i]:l_seq+self.r_shift[i],i] for i in range(seq.shape[1])],axis=-1)\n",
    "            \n",
    "        if not self.truncate_sz is None:\n",
    "            seq = seq[truncate_sz:]\n",
    "        \n",
    "        #it is important to slice first and then do the class conversion\n",
    "#         return self.to_cls(seq.astype('f8'))#workaround for random bug, that mitigates convergence if the numpy array is an f4 array. Seems to make no sense because the result does not change. \n",
    "        return self.to_cls(seq)\n",
    "\n",
    "    def encodes(self, item)->None: \n",
    "        return self._extract_dict_sequence(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  4.1873503],\n",
       "       [-0.1      ,  4.18935  ],\n",
       "       [-0.1      ,  4.1896954],\n",
       "       ...,\n",
       "       [ 8.8388   ,  3.3932123],\n",
       "       [ 8.846    ,  3.3928714],\n",
       "       [ 8.8531   ,  3.3925302]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "hdf2seq = HDF2Sequence(['current','voltage'],cached=False)\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1      ,  4.18935  ],\n",
       "       [-0.1      ,  4.1896954],\n",
       "       [-0.1      ,  4.190037 ],\n",
       "       ...,\n",
       "       [ 8.8388   ,  3.3932123],\n",
       "       [ 8.846    ,  3.3928714],\n",
       "       [ 8.8531   ,  3.3925302]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf2seq = HDF2Sequence(['current','voltage'],clm_shift=[1,1])\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  4.1873503],\n",
       "       [-0.1      ,  4.18935  ],\n",
       "       [-0.1      ,  4.1896954],\n",
       "       ...,\n",
       "       [ 8.8388   ,  3.3932123],\n",
       "       [ 8.846    ,  3.3928714],\n",
       "       [ 8.8531   ,  3.3925302]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf2seq = HDF2Sequence(['current','voltage'],cached=False)\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.4 ms ± 234 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf2seq = HDF2Sequence(['current','voltage'],cached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ,  4.1873503],\n",
       "       [-0.1      ,  4.18935  ],\n",
       "       [-0.1      ,  4.1896954],\n",
       "       ...,\n",
       "       [ 8.8388   ,  3.3932123],\n",
       "       [ 8.846    ,  3.3928714],\n",
       "       [ 8.8531   ,  3.3925302]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion lässt sich mittels Pipeline auf eine Liste von Quellobjekten (hier Pfade) anwenden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265607, 1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf2seq = HDF2Sequence(['current'])\n",
    "hdf2seq(hdf_files[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(HDF2Sequence(['current','voltage']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_pipe = pipe(hdf_files)\n",
    "# len(res_pipe), res_pipe[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hdf2scalars(hdf_path,c_names):\n",
    "    with h5py.File(hdf_path,'r') as f:\n",
    "#         import pdb; pdb.set_trace()\n",
    "#         l_array = [f[n][:][:,None] for n in c_names]\n",
    "#         seq = np.concatenate(l_array,axis=1)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Test\n",
    "Caching stores the arrays for future use at every function call. Very usefull, especially for windows. Should allways be turned. Only explicitly turn it off when there is not enough memory for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms=[  [HDF2Sequence(['current','voltage'],cached=False)],\n",
    "        [HDF2Sequence(['voltage'],cached=False)]]\n",
    "dsrc = Datasets(src_dicts[:1000],tfms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dsrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for x in dsrc:\n",
    "#     x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms=[  [HDF2Sequence(['current','voltage'],cached=True,clm_shift=[1,2])],\n",
    "        [HDF2Sequence(['voltage'],cached=True)]]\n",
    "dsrc = Datasets(src_dicts[:1000],tfms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "for x in dsrc:\n",
    "    x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching is way faster because every file gets loaded multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Datatypes for Sequences and Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TensorSequences(TensorBase):#TensorBase\n",
    "#     def __init__(self,x,c_names=None, **kwargs):\n",
    "#         super().__init__()\n",
    "#         self.c_names = c_names\n",
    "    \n",
    "    def show(self, ctx=None, **kwargs):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        ax = ctx\n",
    "        if ax is None: _,ax = plt.subplots()\n",
    "        ax.plot(self)\n",
    "#         if title is not None: ax.set_title(title)\n",
    "        return ax\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(HDF2Sequence, keep=True)\n",
    "    def from_hdf(cls,clm_names,**kwargs):\n",
    "        return HDF2Sequence(clm_names,**kwargs)\n",
    "    \n",
    "class TensorSequencesInput(TensorSequences): pass\n",
    "class TensorSequencesOutput(TensorSequences): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = TensorSequencesInput.from_hdf(['current'])\n",
    "type(f(hdf_files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorSequences(np.ones((30,2))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@Transform\n",
    "def toTensorSequencesInput(o): return TensorSequencesInput(o)\n",
    "@Transform\n",
    "def toTensorSequencesOutput(o): return TensorSequencesOutput(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TensorScalars(Tensor): pass\n",
    "class TensorScalarsInput(TensorScalars): pass\n",
    "class TensorScalarsOutput(TensorScalars): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Sequence Slicing Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class SeqSlice(Transform):\n",
    "    '''Take a slice from an array-like object. Useful for e.g. shifting input and output'''\n",
    "    def __init__(self, l_slc=None,r_slc=None):\n",
    "        self.l_slc,self.r_slc = l_slc,r_slc\n",
    "        \n",
    "    def encodes(self, o): return o[self.l_slc:self.r_slc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_shift = SeqSlice(r_slc=-1)\n",
    "arr = np.ones((5))\n",
    "test_eq(l_shift(arr),arr[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sequence Noise Injection Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqNoiseInjection(RandTransform):\n",
    "    split_idx=0\n",
    "    '''Adds normal distributed noise to the tensor sequence with seperate mean and std for every signal'''\n",
    "    def __init__(self, std=1e-1,mean=0.,p=1.0):\n",
    "        super().__init__(p=p)\n",
    "        self.std = tensor(std).type(torch.float)\n",
    "        self.mean = tensor(mean).type(torch.float)\n",
    "        \n",
    "    def encodes(self, o:TensorSequencesInput): \n",
    "        if o.device != self.mean.device:\n",
    "            self.std = self.std.to(o.device)\n",
    "            self.mean = self.mean.to(o.device)\n",
    "        #expand creates a view on a tensor and is therefore very fast compared to copy\n",
    "        return o+torch.normal(mean=self.mean.expand_as(o), \n",
    "                              std=self.std.expand_as(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSequencesInput([[ 1.,  1.,  1.],\n",
       "         [-1., -1., -1.]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = TensorSequencesInput(tensor([[1,1,1],[-1,-1,-1.0]]))\n",
    "ns_mean = tensor([0.,10.1,3.1])\n",
    "ns_std = tensor([1.,1.1,0.1])\n",
    "x,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSequencesInput([[ 1.,  1.,  1.],\n",
       "        [-1., -1., -1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_noise = SeqNoiseInjection(std=ns_std,mean=ns_mean)\n",
    "seq_noise(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSequencesInput([[ 1.,  1.,  1.],\n",
       "        [-1., -1., -1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_noise = SeqNoiseInjection(std=ns_std*10)\n",
    "seq_noise(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqNoiseInjection_Varying(RandTransform):\n",
    "    split_idx=0\n",
    "    '''Adds normal distributed noise to the tensor sequence with a normal distributed standard deviation for every application'''\n",
    "    def __init__(self, std_std=0.1,p=1.0):\n",
    "        super().__init__(p=p)\n",
    "        self.std_std = tensor(std_std).type(torch.float)\n",
    "        \n",
    "    def encodes(self, o:TensorSequencesInput): \n",
    "        if o.device != self.std_std.device:\n",
    "            self.std_std = self.std_std.to(o.device)\n",
    "            \n",
    "        #expand creates a view on a tensor and is therefore very fast compared to copy\n",
    "        std = torch.normal(mean=0,std=self.std_std).abs()\n",
    "        return o+torch.normal(mean=0,std=std.expand_as(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSequencesInput([[0, 0, 0],\n",
       "         [0, 0, 0]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = TensorSequencesInput(tensor([[0,0,0],[0,0,0]]))\n",
    "ns_std = tensor([1.,1.1,0.1])\n",
    "x,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSequencesInput([[0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_noise = SeqNoiseInjection_Varying(std_std=ns_std)\n",
    "seq_noise(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqNoiseInjection_Grouped(RandTransform):\n",
    "    split_idx=0\n",
    "    '''Adds normal distributed noise to the tensor sequence with a normal distributed standard deviation for every application, every group gert'''\n",
    "    def __init__(self, std_std,std_idx,p=1.0):\n",
    "        super().__init__(p=p)\n",
    "        self.std_std = tensor(std_std).type(torch.float)\n",
    "        self.std_idx = tensor(std_idx).type(torch.long)\n",
    "        \n",
    "    def encodes(self, o:TensorSequencesInput): \n",
    "        if o.device != self.std_std.device:\n",
    "            self.std_std = self.std_std.to(o.device)\n",
    "            \n",
    "        #expand creates a view on a tensor and is therefore very fast compared to copy\n",
    "        std = torch.normal(mean=0,std=self.std_std).abs()[self.std_idx]\n",
    "        return o+torch.normal(mean=0,std=std.expand_as(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSequencesInput([[0, 0, 0],\n",
       "         [0, 0, 0]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = TensorSequencesInput(tensor([[0,0,0],[0,0,0]]))\n",
    "ns_std = tensor([1.,1.1,0.1])\n",
    "x,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSequencesInput([[0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_noise = SeqNoiseInjection_Grouped(std_std=[3.,0],std_idx=[0,0,1])\n",
    "seq_noise(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sequence Bias Injection Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqBiasInjection(RandTransform):\n",
    "    split_idx=0\n",
    "    '''Adds a normal distributed offset to the tensor sequence with seperate mean and std for every signal'''\n",
    "    def __init__(self, std=1e-1,mean=0.,p=1.0):\n",
    "        super().__init__(p=p)\n",
    "        self.std = tensor(std).type(torch.float)\n",
    "        self.mean = tensor(mean).type(torch.float)\n",
    "        \n",
    "    def encodes(self, o:TensorSequencesInput): \n",
    "        if o.device != self.mean.device:\n",
    "            self.std = self.std.to(o.device)\n",
    "            self.mean = self.mean.to(o.device)\n",
    "        \n",
    "        #expand creates a view on a tensor and is therefore very fast compared to copy\n",
    "        mean=self.mean.repeat((o.shape[0],1,1)).expand((o.shape[0],1,o.shape[2]))\n",
    "        std= self.std.repeat((o.shape[0],1,1)).expand((o.shape[0],1,o.shape[2]))\n",
    "        n = torch.normal(mean=mean, std=std).expand_as(o)\n",
    "        return o+n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSequencesInput([[ 1., -1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = TensorSequencesInput(tensor([[[1,1,1],[-1,-1,-1.0]]]))\n",
    "ns_mean = tensor([0.,10.1,3.1])\n",
    "ns_std = tensor([1.,1.1,0.1])\n",
    "seq_bias = SeqBiasInjection(std=ns_std,mean=ns_std)\n",
    "seq_bias(x)[...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.1000, 0.1000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_bias.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSequencesInput([[[ 1.,  1.,  1.],\n",
       "         [-1., -1., -1.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_bias = SeqBiasInjection(std=ns_std*10)\n",
    "seq_bias(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Normalization\n",
    "`Normalize` is programmed for `TensorImage` as an input tensor. It gets. At init the variable axes need to be chosen correspondingly to the shape of your tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@Normalize\n",
    "def encodes(self, x:TensorSequencesInput): \n",
    "    if x.device != self.mean.device:\n",
    "        self.mean = self.mean.to(x.device)\n",
    "        self.std = self.std.to(x.device)\n",
    "    return (x-self.mean) / self.std\n",
    "\n",
    "@Normalize\n",
    "def decodes(self, x:TensorSequencesInput):\n",
    "    if x.device != self.mean.device:\n",
    "        self.mean = self.mean.to(x.device)\n",
    "        self.std = self.std.to(x.device)\n",
    "    return (x*self.std + self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSequencesInput([[[ 1.,  1.,  1.],\n",
       "          [-1., -1., -1.]]]),\n",
       " TensorSequencesInput([[[  1.0000,  -8.2727, -21.0000],\n",
       "          [ -1.0000, -10.0909, -41.0000]]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = Normalize.from_stats(mean=ns_mean,std=ns_std,dim=1,ndim=2,cuda=False)\n",
    "x,norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split in Training, Validation\n",
    "Splitting kann anhand von vorher bekannten Indizes, dem Dateipfad oder anderen allgemeinen Funktion durchgeführt werden.\n",
    "\n",
    "Splitting innerhalb einer Sequenzen sollte in der Praxis nur dann geschehen wenn eine einzige Sequenz vorhanden ist. Diese kann dann vorher manuell geteilt werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Splitting mit vorgegebenem Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = IndexSplitter([1,2])\n",
    "test_eq(splitter(hdf_files),[[0],[1,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Splitting mit allgemeiner Funktion\n",
    "Items, bei denen die definierte Funktion `True` zurück gibt, werden den Validierungsdatensatz zugeordnet, der Rest dem Training. In diesem Fall wird nach dem Übergeordneten Ordnernamen gesucht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = FuncSplitter(lambda o: Path(o).parent.name == 'valid')\n",
    "test_eq(splitter(hdf_files),[[0,1],[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Splitting anhand des Parent-Folders\n",
    "Splitter, der Explizit Training und Validierungsordner den Datensätzen zuordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _parent_idxs(items, name): return mask2idxs(Path(o).parent.name == name for o in items)\n",
    "\n",
    "def ParentSplitter(train_name='train', valid_name='valid'):\n",
    "    \"Split `items` from the parent folder names (`train_name` and `valid_name`).\"\n",
    "    def _inner(o, **kwargs):\n",
    "        return _parent_idxs(o, train_name),_parent_idxs(o, valid_name)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ParentSplitter()\n",
    "test_eq(splitter(hdf_files),[[0,1],[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Percentage Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def PercentageSplitter(pct=0.8):\n",
    "    \"Split `items` in order in relative quantity.\"\n",
    "    def _inner(o, **kwargs):\n",
    "        split_idx=int(len(o)*pct)\n",
    "        return L(range(split_idx)),L(range(split_idx,len(o)))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = PercentageSplitter(0.7)\n",
    "test_eq(splitter(hdf_files),[[0,1],[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Apply To Dictionary\n",
    "In Case of the Datablock API your items are a list of dictionaries. If you want to apply a Splitter to the path stored within you need a wrapper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ApplyToDict(fn,key='path'):\n",
    "    return lambda x:fn([i[key] for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Valid Column\n",
    "Using the 'valid' column of the Dataframe that has been created by a transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "valid_clm_splitter =  FuncSplitter(lambda o:o['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#53101) [0,1,2,3,4,5,6,7,8,9...],\n",
       " (#26550) [53101,53102,53103,53104,53105,53106,53107,53108,53109,53110...])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_clm_splitter(src_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataloaders Creation\n",
    "A Datasets combines all implemented components on item level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pad_sequence(batch,sorting = False):\n",
    "    '''collate_fn for padding of sequences of different lengths, use in before_batch of databunch, still quite slow'''\n",
    "    #takes list of tuples as input, returns list of tuples\n",
    "    sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True) if sorting else batch\n",
    "\n",
    "    pad_func = partial(torch.nn.utils.rnn.pad_sequence,batch_first=True)\n",
    "    padded_tensors = [pad_func([x[tup] for x in sorted_batch]) for tup in range(len(batch[0]))]\n",
    "    padded_list = [retain_types(tuple([tup[entry] for tup in padded_tensors]),batch[0]) for entry in range(len(batch))]\n",
    "    #retain types is important for decoding later back to source items\n",
    "#     import pdb; pdb.set_trace()\n",
    "    \n",
    "    return padded_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Low-Level with Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms=[  [HDF2Sequence(['current','voltage']),SeqSlice(l_slc=1),toTensorSequencesInput],\n",
    "        [HDF2Sequence(['voltage']),SeqSlice(r_slc=-1),toTensorSequencesOutput]]\n",
    "splits = splitter([x['path'] for x in src_dicts])\n",
    "dsrc = Datasets(src_dicts,tfms=tfms,splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# dsrc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 100, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = dsrc.dataloaders(bs=128,after_batch=[SeqNoiseInjection(std=[1.1,0.01]),Normalize(axes=[0,1])],before_batch=pad_sequence)\n",
    "db.one_batch()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Mid-Level with Datablock API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequenceBlock(TransformBlock):\n",
    "    def __init__(self, seq_extract,padding=False):\n",
    "        return super().__init__(type_tfms=[seq_extract],\n",
    "                                batch_tfms=[Normalize(axes=[0,1])],\n",
    "                                dls_kwargs={} if not padding else {'before_batch': pad_sequence})\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(HDF2Sequence, keep=True)\n",
    "    def from_hdf(cls, clm_names, seq_cls=TensorSequencesInput,padding=False, **kwargs):\n",
    "        return cls(HDF2Sequence(clm_names,to_cls=seq_cls,**kwargs), padding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = DataBlock(blocks=(SequenceBlock.from_hdf(['current','voltage'],TensorSequencesInput,padding=True,cached=False),\n",
    "                        SequenceBlock.from_hdf(['voltage'],TensorSequencesOutput,cached=False)),\n",
    "                get_items=tfm_src,\n",
    "                splitter=ApplyToDict(ParentSplitter()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = seq.dataloaders(hdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Show Batches and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_sequence(axs,in_sig,targ_sig,out_sig=None,**kwargs):\n",
    "    n_targ = targ_sig.shape[1]\n",
    "    for j,ax in  enumerate(axs[:-1]):\n",
    "        ax.plot(targ_sig[:,j])\n",
    "        if out_sig is not None: \n",
    "            ax.plot(out_sig[:,j])\n",
    "            ax.legend(['y','ŷ'])\n",
    "            if 'ref' in kwargs:\n",
    "                ax.plot(kwargs['ref'][:,j]) \n",
    "        ax.label_outer()\n",
    "    axs[-1].plot(in_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_seqs_single_figure(n_samples,n_targ,samples,plot_func,outs=None,**kwargs):\n",
    "    rows=max(1,((n_samples-1) // 3)+1)\n",
    "    cols=min(3,n_samples)\n",
    "    fig = plt.figure(figsize=(9,2*cols))\n",
    "    outer_grid = fig.add_gridspec(rows, cols)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    for i in range(n_samples):\n",
    "        in_sig = samples[i][0]\n",
    "        targ_sig = samples[i][1]\n",
    "        if outs is not None: out_sig = outs[i][0]\n",
    "        inner_grid = outer_grid[i].subgridspec(n_targ+1, 1)\n",
    "        axs = [fig.add_subplot(inner_grid[j]) for j in range(n_targ+1)]\n",
    "        plot_func(axs,in_sig,targ_sig,out_sig=out_sig if outs is not None else None,**kwargs)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def plot_seqs_multi_figures(n_samples,n_targ,samples,plot_func,outs=None,**kwargs):\n",
    "    for i in range(n_samples):\n",
    "        fig = plt.figure(figsize=(9,3))\n",
    "        axs = fig.subplots(nrows=n_targ+1,sharex=True)\n",
    "        in_sig = samples[i][0]\n",
    "        targ_sig = samples[i][1]\n",
    "        if outs is not None:  out_sig = outs[i][0]\n",
    "            \n",
    "        plot_func(axs,in_sig,targ_sig,out_sig=out_sig if outs is not None else None,**kwargs)\n",
    "        \n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x:TensorSequences, y:TensorSequences, samples, ctxs=None, max_n=6, **kwargs):\n",
    "    n_samples = min(len(samples), max_n)\n",
    "    n_targ = samples[0][1].shape[1]\n",
    "    if n_samples > 3:\n",
    "        #if there are more then 3 samples to plot then put them in a single figure\n",
    "        plot_seqs_single_figure(n_samples,n_targ,samples,plot_sequence, **kwargs)\n",
    "    else:\n",
    "        #if there are less then 3 samples to plot then put each in its own figure\n",
    "        plot_seqs_multi_figures(n_samples,n_targ,samples,plot_sequence, **kwargs)\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_results(x:TensorSequences, y:TensorSequences, samples, outs, ctxs=None, max_n=2, **kwargs):\n",
    "    n_samples = min(len(samples), max_n)\n",
    "    n_targ = samples[0][1].shape[1]\n",
    "    if n_samples > 3:\n",
    "        #if there are more then 3 samples to plot then put them in a single figure\n",
    "        plot_seqs_single_figure(n_samples,n_targ,samples,plot_sequence,outs, **kwargs)\n",
    "    else:\n",
    "        #if there are less then 3 samples to plot then put each in its own figure\n",
    "        plot_seqs_multi_figures(n_samples,n_targ,samples,plot_sequence,outs, **kwargs)\n",
    "    return ctxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40338f43a52641d09f5a201949d2e28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_models.ipynb.\n",
      "Converted 01a_IndRNN.ipynb.\n",
      "Converted 02_learner.ipynb.\n",
      "Converted 03_dataloaders.ipynb.\n",
      "Converted 11_dualrnn.ipynb.\n",
      "Converted 12_TensorQuaternions.ipynb.\n",
      "Converted 13_HPOpt.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
