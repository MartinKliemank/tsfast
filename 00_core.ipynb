{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corefunctions\n",
    "> Corefunctionality for data preparation of sequential data for pytorch, fastai models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Structure\n",
    "The data will be extracted and prepared via transforms. Those are grouped in:\n",
    "- Type Transforms: Those extraxt the needed components from the source items, like input sequences or target scalar values. The work on single tensors.\n",
    "- Item Transforms: Those Transforms may work on tuple level and therefore may process relationships between input and output.\n",
    "- Batch Transform: Those transforms work on batch level. They receive batched tensors and may apply lazy transforms like normalization very effeciently.\n",
    "\n",
    "An application example may look like the following:\n",
    "- sourceitems: \n",
    "    - path extraction with hdf5 file endings\n",
    "    - create pandas dataframe with information for type transforms, like slices\n",
    "    - filter items in pandas dataframe\n",
    "- type transforms: \n",
    "    - extract hdf5 input and output sequence\n",
    "    - create windows\n",
    "- item transforms: \n",
    "    - filter sequence by value\n",
    "    - shift output sequence by 1 element\n",
    "- batch transforms: \n",
    "    - noise injection\n",
    "    - normalization\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.data.all import *\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Source Items\n",
    "The file paths may be extracted with `get_files` of fastai2. `get_hdf_files` removes the need of writing the hdf5 file extension.\n",
    "\n",
    "Then a pandas dataframe may be created in case further information for the source items need to be stored like slices for the windowing function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Extract File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, PosixPath('test_data/train/Sim_RealisticCycle2.hdf5'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_path = 'test_data/'\n",
    "hdf_files = get_files(f_path,extensions='.hdf5',recurse=True)\n",
    "len(hdf_files),hdf_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "hdf_extensions = ['.hdf5']\n",
    "def get_hdf_files(path,recurse=True, folders=None):\n",
    "    \"Get hdf5 files in `path` recursively, only in `folders`, if specified.\"\n",
    "    return get_files(path, extensions=hdf_extensions, recurse=recurse, folders=folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, PosixPath('test_data/train/Sim_RealisticCycle2.hdf5'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf_files = get_hdf_files(f_path)\n",
    "len(hdf_files),hdf_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create Pandas Source Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def df_source_items(f_list,pd_tfms = None):\n",
    "    '''Create Pandas Dataframe out of a list of items, with a list of df transforms applied'''\n",
    "    df = pd.DataFrame(data=f_list.items,columns=['path'])\n",
    "    if pd_tfms is not None:\n",
    "        for t in pd_tfms:\n",
    "            df = t(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle1.hdf5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/valid/Sim_RealisticCycle3.hdf5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path\n",
       "0  test_data/train/Sim_RealisticCycle2.hdf5\n",
       "1  test_data/train/Sim_RealisticCycle1.hdf5\n",
       "2  test_data/valid/Sim_RealisticCycle3.hdf5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_source_items(hdf_files)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def DfHDFCreateWindows(win_sz,stp_sz, clm, fixed_start = False, fixed_end = False):\n",
    "    '''create windows of sequences, splits sequence into multiple items'''\n",
    "    def _inner(df):\n",
    "        if fixed_start and fixed_end: raise Exception\n",
    "\n",
    "        lst_df = [] #new dataframe for every row\n",
    "        for idx, row in df.iterrows():\n",
    "            with h5py.File(row.path,'r') as f:\n",
    "                #TODO make clm optional\n",
    "#                 if clm == '': \n",
    "#                     clm = list(f.keys())[0]\n",
    "                f_len = f[clm].shape[0]\n",
    "\n",
    "                n_win = ((f_len-win_sz)//stp_sz)+1\n",
    "                tmp_df = df.iloc[[idx]*n_win]; #duplicate the row of the df multiple times by reference\n",
    "                lst_idx = np.arange(n_win)\n",
    "\n",
    "                #every row is a reference so we need to suppress the warning messages while copying\n",
    "                pd.options.mode.chained_assignment = None\n",
    "                tmp_df['l_slc'] = lst_idx*stp_sz\n",
    "                tmp_df['r_slc'] = lst_idx*stp_sz + win_sz\n",
    "                pd.options.mode.chained_assignment = 'warn'\n",
    "\n",
    "                lst_df.append(tmp_df)\n",
    "\n",
    "        res_df = pd.concat(lst_df)\n",
    "        return res_df\n",
    "    \n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.69 ms, sys: 1.63 ms, total: 10.3 ms\n",
      "Wall time: 9.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "create_win = DfHDFCreateWindows(win_sz=100,stp_sz=100,clm='current')\n",
    "create_win(df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>l_slc</th>\n",
       "      <th>r_slc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>10</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>20</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>30</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/train/Sim_RealisticCycle2.hdf5</td>\n",
       "      <td>40</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       path  l_slc  r_slc\n",
       "0  test_data/train/Sim_RealisticCycle2.hdf5      0    101\n",
       "0  test_data/train/Sim_RealisticCycle2.hdf5     10    111\n",
       "0  test_data/train/Sim_RealisticCycle2.hdf5     20    121\n",
       "0  test_data/train/Sim_RealisticCycle2.hdf5     30    131\n",
       "0  test_data/train/Sim_RealisticCycle2.hdf5     40    141"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "src_df = df_source_items(hdf_files,[DfHDFCreateWindows(win_sz=100+1,stp_sz=10,clm='current')])\n",
    "src_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert Paths to Sequence Objects\n",
    "Der Pfad wird unter Angabe der Spaltennamen in Sequenzen und Skalare Werte umgewandelt, um so am Ende ein 3-Tupel zu erhalten aus:\n",
    "- (Sequence, Scalar, Sequence) <-> (input,input,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Datatypes for Sequences and Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TensorSequences(TensorBase): pass\n",
    "class TensorSequencesInput(TensorSequences): pass\n",
    "class TensorSequencesOutput(TensorSequences): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@Transform\n",
    "def toTensorSequencesInput(o): return TensorSequencesInput(o)\n",
    "@Transform\n",
    "def toTensorSequencesOutput(o): return TensorSequencesOutput(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TensorScalars(TensorBase): pass\n",
    "class TensorScalarsInput(TensorScalars): pass\n",
    "class TensorScalarsOutput(TensorScalars): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extract sequential data from hdf5-files\n",
    "Two different functions, based on pandas df and on lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from functools import lru_cache\n",
    "\n",
    "def HDF2Sequence(c_names,cached=True):\n",
    "    def _extract_sequence(hdf_path,dataset = None, l_slc = None, r_slc= None):\n",
    "        with h5py.File(hdf_path,'r') as f:\n",
    "            ds = f if dataset is None else f[dataset]\n",
    "            l_array = [ds[n][l_slc:r_slc][:,None] for n in c_names]\n",
    "            seq = np.concatenate(l_array,axis=1)\n",
    "            return tensor(seq)\n",
    "        \n",
    "    _exseq = lru_cache(maxsize=None)(_extract_sequence) if cached else _extract_sequence\n",
    "    \n",
    "    def _extract_df_sequence(item):\n",
    "        if not isinstance(item,pd.Series):\n",
    "            return _exseq(str(item))\n",
    "        \n",
    "        path = item.path\n",
    "        dataset = item.dataset if hasattr(item,'dataset') else None\n",
    "        l_slc = item.l_slc if hasattr(item,'l_slc') else None\n",
    "        r_slc = item.r_slc if hasattr(item,'r_slc') else None\n",
    "        \n",
    "        if cached:\n",
    "            return _exseq(path,dataset)[l_slc:r_slc]\n",
    "        else:\n",
    "            return _exseq(path,dataset,l_slc,r_slc)\n",
    "\n",
    "    return _extract_df_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  4.1874],\n",
       "        [-0.0052,  4.1875],\n",
       "        [-0.0090,  4.1875],\n",
       "        ...,\n",
       "        [ 1.0783,  3.7160],\n",
       "        [ 1.0739,  3.7161],\n",
       "        [ 1.0706,  3.7162]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "hdf2seq = HDF2Sequence(['current','voltage'],cached=False)\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf2seq = HDF2Sequence(['current','voltage'],cached=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  4.1874],\n",
       "        [-0.0052,  4.1875],\n",
       "        [-0.0090,  4.1875],\n",
       "        ...,\n",
       "        [ 1.0783,  3.7160],\n",
       "        [ 1.0739,  3.7161],\n",
       "        [ 1.0706,  3.7162]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "hdf2seq(hdf_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion lässt sich mittels Pipeline auf eine Liste von Quellobjekten (hier Pfade) anwenden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(HDF2Sequence(['current','voltage']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, tensor([0.0000, 4.1874]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_pipe = pipe(hdf_files)\n",
    "len(res_pipe), res_pipe[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hdf2scalars(hdf_path,c_names):\n",
    "    with h5py.File(hdf_path,'r') as f:\n",
    "#         import pdb; pdb.set_trace()\n",
    "#         l_array = [f[n][:][:,None] for n in c_names]\n",
    "#         seq = np.concatenate(l_array,axis=1)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Test\n",
    "Caching stores the arrays for future use at every function call. Very usefull, especially for windows. Should allways be turned. Only explicitly turn it off when there is not enough memory for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms=[  [HDF2Sequence(['current','voltage'],cached=False)],\n",
    "        [HDF2Sequence(['voltage'],cached=False)]]\n",
    "dsrc = DataSource(src_df.iloc[:1000],tfms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dsrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for x in dsrc:\n",
    "#     x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms=[  [HDF2Sequence(['current','voltage'],cached=True)],\n",
    "        [HDF2Sequence(['voltage'],cached=True)]]\n",
    "dsrc = DataSource(src_df.iloc[:1000],tfms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 701 ms, sys: 7.01 ms, total: 708 ms\n",
      "Wall time: 708 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for x in dsrc:\n",
    "    x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching is way faster because every file gets loaded multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SequenceItem\n",
    "Damit die Sequenz visualisiert werden kann und auch dritte Informationen gespeichert werden können, wird eine Klasse erstellt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#TODO: Fallunterscheidung der Sequenzen\n",
    "class SequenceItem(Tuple):\n",
    "    def show(self, ctx=None, **kwargs): \n",
    "        plt.figure()\n",
    "        plt.plot(self[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SequenceItem ist nur für die Darstellung eines Tupels von Sequenzen zuständig. Es muss zwischen Skalaren und Vektoriellen Zielgrößen unterschieden werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqTfm(Transform):\n",
    "    def decodes(self, x): return SequenceItem(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SequenceTfm erstellt ein SequenceItem beim decoding für die spätere Darstellung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Item Transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Sequence Slicing Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class SeqSlice(Transform):\n",
    "    '''Take a slice from an array-like object. Useful for e.g. shifting input and output'''\n",
    "    def __init__(self, l_slc=None,r_slc=None):\n",
    "        self.l_slc,self.r_slc = l_slc,r_slc\n",
    "        \n",
    "    def encodes(self, o): return o[self.l_slc:self.r_slc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_shift = SeqSlice(r_slc=-1)\n",
    "arr = np.ones((5))\n",
    "test_eq(l_shift(arr),arr[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sequence Noise Injection Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SeqNoiseInjection(Transform):\n",
    "    '''Adds normal distributed noise to the tensor sequence with seperate mean and std for every signal'''\n",
    "    def __init__(self, std=1e-1,mean=0.):\n",
    "        self.std,self.mean = tensor(std),tensor(mean)\n",
    "        \n",
    "    def setups(self, dl:DataLoader):\n",
    "        #check the tensor type of your input\n",
    "        #TODO: include scalar type case\n",
    "        x,*_ = dl.one_batch()\n",
    "        self.std = to_device(self.std,x.device)\n",
    "        self.mean = to_device(self.mean,x.device)\n",
    "        \n",
    "    def encodes(self, o:TensorSequencesInput): \n",
    "        #expand creates a view on a tensor and is therefore very fast compared to copy\n",
    "        return o+torch.normal(mean=self.mean.expand_as(o), \n",
    "                              std=self.std.expand_as(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  1.,  1.],\n",
       "         [-1., -1., -1.]]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = TensorSequencesInput(tensor([[1,1,1],[-1,-1,-1.0]]))\n",
    "ns_mean = tensor([0.,10.1,3.1])\n",
    "ns_std = tensor([1.,1.1,0.1])\n",
    "x,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.9732,  3.8923,  1.1886],\n",
       "        [-2.5649, -1.1318, -1.0120]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_noise = SeqNoiseInjection(std=ns_std,mean=ns_std)\n",
    "seq_noise(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-16.8420,  -2.0927,   0.6381],\n",
       "        [  5.0517,   7.7966,  -1.0036]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_noise = SeqNoiseInjection(std=ns_std*10)\n",
    "seq_noise(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Normalization\n",
    "`Normalize` is programmed for `TensorImage` as an input tensor. It gets. At init the variable axes need to be chosen correspondingly to the shape of your tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Normalize\n",
    "def encodes(self, x:TensorSequencesInput): \n",
    "    return (x-self.mean) / self.std\n",
    "\n",
    "@Normalize\n",
    "def decodes(self, x:TensorSequencesInput):\n",
    "    f = to_cpu if x.device.type=='cpu' else noop\n",
    "    return (x*f(self.std) + f(self.mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  1.,  1.],\n",
       "         [-1., -1., -1.]]), tensor([[  1.0000,  -8.2727, -21.0000],\n",
       "         [ -1.0000, -10.0909, -41.0000]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = Normalize.from_stats(mean=ns_mean,std=ns_std,dim=1,ndim=2,cuda=False)\n",
    "x,norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split in Training, Validation\n",
    "Splitting kann anhand von vorher bekannten Indizes, dem Dateipfad oder anderen allgemeinen Funktion durchgeführt werden.\n",
    "\n",
    "Splitting innerhalb einer Sequenzen sollte in der Praxis nur dann geschehen wenn eine einzige Sequenz vorhanden ist. Diese kann dann vorher manuell geteilt werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Splitting mit vorgegebenem Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = IndexSplitter([1,2])\n",
    "test_eq(splitter(hdf_files),[[0],[1,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Splitting mit allgemeiner Funktion\n",
    "Items, bei denen die definierte Funktion `True` zurück gibt, werden den Validierungsdatensatz zugeordnet, der Rest dem Training. In diesem Fall wird nach dem Übergeordneten Ordnernamen gesucht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = FuncSplitter(lambda o: Path(o).parent.name == 'valid')\n",
    "test_eq(splitter(hdf_files),[[0,1],[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Splitting anhand des Parent-Folders\n",
    "Splitter, der Explizit Training und Validierungsordner den Datensätzen zuordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _parent_idxs(items, name): return mask2idxs(Path(o).parent.name == name for o in items)\n",
    "\n",
    "def ParentSplitter(train_name='train', valid_name='valid'):\n",
    "    \"Split `items` from the parent folder names (`train_name` and `valid_name`).\"\n",
    "    def _inner(o, **kwargs):\n",
    "        return _parent_idxs(o, train_name),_parent_idxs(o, valid_name)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = ParentSplitter()\n",
    "test_eq(splitter(hdf_files),[[0,1],[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Datasource\n",
    "A Datasource combines all implemented components on item level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms=[  [HDF2Sequence(['current','voltage']),SeqSlice(l_slc=1),toTensorSequencesInput],\n",
    "        [HDF2Sequence(['voltage']),SeqSlice(r_slc=-1),toTensorSequencesOutput]]\n",
    "dsrc = DataSource(src_df,tfms=tfms,splits=splitter(src_df.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 100, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = dsrc.databunch(bs=128,after_batch=[Cuda(),SeqNoiseInjection(std=[1.1,0.01]),Normalize(axes=[0,1])])\n",
    "db.one_batch()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_model.ipynb.\n",
      "Converted 10_performance_test.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
