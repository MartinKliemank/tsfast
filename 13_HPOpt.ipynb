{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Pytorch Models for Sequential Data\n",
    "output-file: hpopt.html\n",
    "title: Hyperparameter Optimization Module\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp hpopt\n",
    "#| default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library \"haste_pytorch\" not found\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from seqdata.core import *\n",
    "from seqdata.models.core import *\n",
    "from seqdata.learner import *\n",
    "from fastai.basics import *\n",
    "from fastai.callback.schedule import *\n",
    "from fastai.callback.rnn import *\n",
    "from fastai.callback.tracker import *\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers import *\n",
    "from ray.tune.experiment.trial import ExportFormat\n",
    "from ray import train\n",
    "from ray.train import Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_paths = Path.cwd() / 'test_data/WienerHammerstein/'\n",
    "hdf_files = L([f for f in get_hdf_files(f_paths) if '_test.hdf5' not in str(f)])\n",
    "tfm_src = CreateDict([DfHDFCreateWindows(win_sz=400,stp_sz=100,clm='u')])\n",
    "dls = DataBlock(blocks=(SequenceBlock.from_hdf(['u'],TensorSequencesInput),\n",
    "                        SequenceBlock.from_hdf(['y'],TensorSequencesOutput)),\n",
    "                get_items=tfm_src,\n",
    "                splitter=ApplyToDict(FuncSplitter(lambda o: 'valid' in str(o)))).dataloaders(hdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a log uniform distibution for variables with vast value ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def log_uniform(min_bound, max_bound, base=10):\n",
    "    '''uniform sampling in an exponential range'''\n",
    "    logmin = np.log(min_bound) / np.log(base)\n",
    "    logmax = np.log(max_bound) / np.log(base)\n",
    "    def _sample():\n",
    "        return base**(np.random.uniform(logmin, logmax))\n",
    "    return _sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.707423528633258e-06,\n",
       " 1.1678226754983236e-07,\n",
       " 1.8065566484337061e-06,\n",
       " 7.65402711625238e-05,\n",
       " 9.501829452482763e-07]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[log_uniform(1e-8, 1e-2)() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LearnerTrainable(tune.Trainable):\n",
    "\n",
    "    def setup(self, config):\n",
    "        self.create_lrn = ray.get(config['create_lrn'])\n",
    "        self.dls = ray.get(config['dls'])\n",
    "\n",
    "        self.lrn = self.create_lrn(self.dls,config)\n",
    "\n",
    "    def step(self):\n",
    "        with self.lrn.no_bar(): self.lrn.fit(1)\n",
    "        train_loss,valid_loss,rmse = self.lrn.recorder.values[-1]\n",
    "        result = {'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss,\n",
    "                'mean_loss': rmse}\n",
    "        return result\n",
    "\n",
    "    def save_checkpoint(self, tmp_checkpoint_dir):\n",
    "        checkpoint_path = os.path.join(tmp_checkpoint_dir, \"model.pth\")\n",
    "        torch.save(self.lrn.model.state_dict(), checkpoint_path)\n",
    "        return tmp_checkpoint_dir\n",
    "\n",
    "    def load_checkpoint(self, tmp_checkpoint_dir):\n",
    "        checkpoint_path = os.path.join(tmp_checkpoint_dir, \"model.pth\")\n",
    "        self.lrn.model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "    def _export_model(self, export_formats, export_dir):\n",
    "        if export_formats == [ExportFormat.MODEL]:\n",
    "            path = os.path.join(export_dir, \"exported_model\")\n",
    "            torch.save(self.lrn.model.state_dict(), path)\n",
    "            return {ExportFormat.MODEL: path}\n",
    "        else:\n",
    "            raise ValueError(\"unexpected formats: \" + str(export_formats))\n",
    "\n",
    "    # the learner class will be recreated with every perturbation, saving the model\n",
    "    # that way the new hyperparameter will be applied\n",
    "    def reset_config(self, new_config):\n",
    "        self.lrn = self.create_lrn(self.dls,new_config)\n",
    "        self.config = new_config\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.callback.tracker import SaveModelCallback \n",
    "class CBRaySaveModel(SaveModelCallback):\n",
    "    \"A `TrackerCallback` that saves the model's best during training in a tune checkpoint directory\"\n",
    "    \n",
    "    def _save(self, name):\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            file = os.path.join(temp_checkpoint_dir,name+'.pth')\n",
    "            save_model(file, self.learn.model,opt=None)\n",
    "            self.last_saved_path = file\n",
    "            \n",
    "    #final checkpoint\n",
    "    def after_fit(self, **kwargs):\n",
    "        self._save(f'{self.fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def learner_optimize(config):\n",
    "        create_lrn = ray.get(config['create_lrn'])\n",
    "        dls = ray.get(config['dls'])\n",
    "        \n",
    "        #Scheduling Parameters for training the Model\n",
    "        lrn_kwargs = {'n_epoch':100,'pct_start':0.5}\n",
    "        for attr in ['n_epoch','pct_start']:\n",
    "            if attr in config: lrn_kwargs[attr] = config[attr]\n",
    "\n",
    "        lrn = create_lrn(dls,config)\n",
    "        \n",
    "        # load checkpoint data if provided\n",
    "        checkpoint: train.Checkpoint = train.get_checkpoint()\n",
    "        if checkpoint:\n",
    "            with checkpoint.as_directory() as checkpoint_dir:\n",
    "                lrn.model.load_state_dict(torch.load(checkpoint_dir + 'model.pth'))\n",
    "        \n",
    "        lrn.lr = config['lr'] if 'lr' in config else 3e-3\n",
    "        lrn.add_cb(CBRayReporter() if 'reporter' not in config else ray.get(config['reporter'])())\n",
    "        # lrn.add_cb(CBRaySaveModel()) #the model saving now has to be done by the reporter callback\n",
    "        with lrn.no_bar(): \n",
    "            ray.get(config['fit_method'])(lrn,**lrn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mutation config dictionary consists of functions that sample from a distribution. In order to retrieve a dictionary with one realisation we need the function sample_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample_config(config):\n",
    "    ret_conf = config.copy()\n",
    "    for k in ret_conf:\n",
    "        ret_conf[k]=ret_conf[k]()\n",
    "    return ret_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CBRayReporter(Callback):\n",
    "    \"`Callback` reports progress after every epoch to the ray tune logger\"\n",
    "    \n",
    "    order=70 #order has to be >50, to be executed after the recorder callback\n",
    "\n",
    "    def after_epoch(self):\n",
    "        # train_loss,valid_loss,rmse = self.learn.recorder.values[-1]\n",
    "        # metrics = {\n",
    "        #     'train_loss': train_loss,\n",
    "        #     'valid_loss': valid_loss,\n",
    "        #     'mean_loss': rmse,\n",
    "        # }\n",
    "        scores = self.learn.recorder.values[-1]\n",
    "        metrics = {\n",
    "            'train_loss': scores[0],\n",
    "            'valid_loss': scores[1]\n",
    "        }\n",
    "        for metric,value in zip(self.learn.metrics,scores[2:]):\n",
    "            m_name = metric.name if hasattr(metric,'name') else str(metric)\n",
    "            metrics[m_name] = value\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            file = os.path.join(temp_checkpoint_dir,'model.pth')\n",
    "            #the model has to be saved to the checkpoint directory on creation\n",
    "            #that is why a seperate callback for model saving is not trivial\n",
    "            save_model(file, self.learn.model,opt=None) \n",
    "            ray.train.report(metrics, checkpoint=Checkpoint.from_directory(temp_checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HPOptimizer():\n",
    "    def __init__(self,create_lrn,dls):\n",
    "        self.create_lrn = create_lrn\n",
    "        self.dls = dls\n",
    "        self.analysis = None\n",
    "    \n",
    "    @delegates(ray.init)\n",
    "    def start_ray(self,**kwargs):\n",
    "        ray.shutdown()\n",
    "        ray.init(**kwargs)\n",
    "        \n",
    "    def stop_ray(self):\n",
    "        ray.shutdown()\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    @delegates(tune.run, keep=True)\n",
    "    def optimize(self,config,optimize_func=learner_optimize,resources_per_trial={\"gpu\": 1.0},verbose=1,**kwargs):\n",
    "        config['create_lrn'] = ray.put(self.create_lrn)\n",
    "        #dls are large objects, letting ray handle the copying process makes it much faster\n",
    "        config['dls'] = ray.put(self.dls) \n",
    "        if 'fit_method' not in config: config['fit_method'] = ray.put(Learner.fit_flat_cos)\n",
    "\n",
    "        kwargs.setdefault('keep_checkpoints_num', 1)#keep only the last checkpoint\n",
    "\n",
    "        self.analysis = tune.run(\n",
    "            optimize_func,\n",
    "            config=config,\n",
    "            resources_per_trial=resources_per_trial,\n",
    "            verbose=verbose,\n",
    "            **kwargs)\n",
    "        return self.analysis\n",
    "        \n",
    "    @delegates(tune.run, keep=True)\n",
    "    def optimize_pbt(self,opt_name,num_samples,config,mut_conf,perturbation_interval=2,\n",
    "                 stop={\"training_iteration\": 40 },\n",
    "                 resources_per_trial={\"gpu\": 1 },\n",
    "                 resample_probability=0.25,\n",
    "                 quantile_fraction=0.25,\n",
    "                 **kwargs):\n",
    "        self.mut_conf = mut_conf\n",
    "        \n",
    "        config['create_lrn'] = ray.put(self.create_lrn)\n",
    "        #dls are large objects, letting ray handle the copying process makes it much faster\n",
    "        config['dls'] = ray.put(self.dls) \n",
    "        \n",
    "        kwargs.setdefault('keep_checkpoints_num', 2)\n",
    "        \n",
    "\n",
    "        \n",
    "        scheduler = PopulationBasedTraining(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"mean_loss\",\n",
    "        mode=\"min\",\n",
    "        perturbation_interval=perturbation_interval,\n",
    "        resample_probability=resample_probability,\n",
    "        quantile_fraction=quantile_fraction,\n",
    "        hyperparam_mutations=mut_conf)\n",
    "        \n",
    "        self.analysis = tune.run(\n",
    "            LearnerTrainable,\n",
    "            name=opt_name,\n",
    "            scheduler=scheduler,\n",
    "            reuse_actors=True,\n",
    "            verbose=1,\n",
    "            stop=stop,\n",
    "            checkpoint_score_attr=\"mean_loss\",\n",
    "            num_samples=num_samples,\n",
    "            resources_per_trial=resources_per_trial,\n",
    "            config=config,\n",
    "            **kwargs)\n",
    "        return self.analysis\n",
    "    \n",
    "    def best_model(self):\n",
    "        if self.analysis is None: raise Exception\n",
    "        model = self.create_lrn(self.dls,sample_config(self.mut_conf)).model\n",
    "        f_path = ray.get(self.analysis.get_best_trial('mean_loss',mode='min').checkpoint.value)\n",
    "        model.load_state_dict(torch.load(f_path))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Population Based Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lrn(dls,config):\n",
    "    lr = config['lr']\n",
    "    alpha = config['alpha']\n",
    "    beta = config['beta']\n",
    "    weight_p = config['weight_p']\n",
    "    \n",
    "    lrn = RNNLearner(dls)\n",
    "    lrn.lr = lr\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-02-27 09:08:51</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:09.91        </td></tr>\n",
       "<tr><td>Memory:      </td><td>51.0/251.7 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      PopulationBasedTraining: 4 checkpoints, 1 perturbs<br>Logical resource usage: 0/112 CPUs, 3.0/4 GPUs (0.0/1.0 accelerator_type:RTX)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc                   </th><th style=\"text-align: right;\">      alpha</th><th style=\"text-align: right;\">       beta</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  weight_p</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  valid_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LearnerTrainable_6ba23_00000</td><td>TERMINATED</td><td>141.23.125.123:2274922</td><td style=\"text-align: right;\">0.000754759</td><td style=\"text-align: right;\">0.147041   </td><td style=\"text-align: right;\">0.00206852</td><td style=\"text-align: right;\">  0.427199</td><td style=\"text-align: right;\">0.243669</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         4.09341</td><td style=\"text-align: right;\">   0.054592 </td><td style=\"text-align: right;\">   0.0594217</td></tr>\n",
       "<tr><td>LearnerTrainable_6ba23_00001</td><td>TERMINATED</td><td>141.23.125.123:2274980</td><td style=\"text-align: right;\">6.6786e-05 </td><td style=\"text-align: right;\">1.14297e-05</td><td style=\"text-align: right;\">0.00229839</td><td style=\"text-align: right;\">  0.28322 </td><td style=\"text-align: right;\">0.24408 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         3.57251</td><td style=\"text-align: right;\">   0.0548091</td><td style=\"text-align: right;\">   0.0596214</td></tr>\n",
       "<tr><td>LearnerTrainable_6ba23_00002</td><td>TERMINATED</td><td>141.23.125.123:2274951</td><td style=\"text-align: right;\">0.000943449</td><td style=\"text-align: right;\">0.122534   </td><td style=\"text-align: right;\">0.00258565</td><td style=\"text-align: right;\">  0.389235</td><td style=\"text-align: right;\">0.242528</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         4.3396 </td><td style=\"text-align: right;\">   0.0545417</td><td style=\"text-align: right;\">   0.0588672</td></tr>\n",
       "<tr><td>LearnerTrainable_6ba23_00003</td><td>TERMINATED</td><td>141.23.125.123:2274980</td><td style=\"text-align: right;\">8.31592e-05</td><td style=\"text-align: right;\">0.000128321</td><td style=\"text-align: right;\">0.00130677</td><td style=\"text-align: right;\">  0.15309 </td><td style=\"text-align: right;\">0.245304</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         3.69751</td><td style=\"text-align: right;\">   0.0551621</td><td style=\"text-align: right;\">   0.0602236</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(LearnerTrainable pid=2274951)\u001b[0m Library \"haste_pytorch\" not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 09:08:47,999\tINFO pbt.py:716 -- [pbt]: no checkpoint for trial LearnerTrainable_6ba23_00001. Skip exploit for Trial LearnerTrainable_6ba23_00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(LearnerTrainable pid=2274922)\u001b[0m [0, 0.056544214487075806, 0.0614364854991436, 0.24775400757789612, '00:01']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(LearnerTrainable pid=2274980)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/pheenix/ray_results/pbt_test/LearnerTrainable_6ba23_00003_3_alpha=0.0001,beta=0.0001,lr=0.0013,weight_p=0.1531_2024-02-27_09-08-42/checkpoint_000000)\n",
      "2024-02-27 09:08:49,183\tINFO pbt.py:878 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 6ba23_00002 (score = -0.245733) into trial 6ba23_00000 (score = -0.260895)\n",
      "\n",
      "2024-02-27 09:08:49,184\tINFO pbt.py:905 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial6ba23_00000:\n",
      "lr : 0.002585650659524256 --- (* 0.8) --> 0.002068520527619405\n",
      "alpha : 0.0009434489459286575 --- (* 0.8) --> 0.0007547591567429261\n",
      "beta : 0.12253412616726472 --- (* 1.2) --> 0.14704095140071766\n",
      "weight_p : 0.38923479562285357 --- (resample) --> 0.42719880741350896\n",
      "\n",
      "\u001b[36m(LearnerTrainable pid=2274922)\u001b[0m Restored on 141.23.125.123 from checkpoint: Checkpoint(filesystem=local, path=/home/pheenix/ray_results/pbt_test/LearnerTrainable_6ba23_00002_2_alpha=0.0009,beta=0.1225,lr=0.0026,weight_p=0.3892_2024-02-27_09-08-42/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(LearnerTrainable pid=2274921)\u001b[0m Library \"haste_pytorch\" not found\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(LearnerTrainable pid=2274980)\u001b[0m [0, 0.05516214296221733, 0.06022358313202858, 0.24530412256717682, '00:00']\u001b[32m [repeated 12x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 09:08:54,335\tINFO tune.py:1042 -- Total run time: 12.32 seconds (9.90 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\n",
    "            \"lr\": tune.loguniform(1e-2, 1e-4),\n",
    "            \"alpha\": tune.loguniform(1e-5, 10),\n",
    "            \"beta\": tune.loguniform(1e-5, 10),\n",
    "            \"weight_p\": tune.uniform(0, 0.5)}\n",
    "mut_conf = {# distribution for resampling\n",
    "            \"lr\": log_uniform(1e-8, 1e-2),\n",
    "            \"alpha\": log_uniform(1e-5, 10),\n",
    "            \"beta\": log_uniform(1e-5, 10),\n",
    "            \"weight_p\": lambda: np.random.uniform(0, 0.5)}\n",
    "\n",
    "hp_opt = HPOptimizer(create_lrn,dls)\n",
    "hp_opt.start_ray()\n",
    "hp_opt.optimize_pbt('pbt_test',4,config,mut_conf,perturbation_interval=1,\n",
    "                 stop={\"training_iteration\": 3 },\n",
    "                 resources_per_trial={\"gpu\": 0.5},\n",
    "                 storage_path=str(Path.home() / 'ray_results'))#no cpu count is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hp_opt.best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.data.core.DataLoaders>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dls.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lrn(dls,config):\n",
    "    lrn = RNNLearner(dls,hidden_size=config['hidden_size'],metrics=[fun_rmse,mse])\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomReporter(Callback):\n",
    "#     \"`Callback` reports progress after every epoch to the ray tune logger\"\n",
    "    \n",
    "#     order=70 #order has to be >50, to be executed after the recorder callback\n",
    "\n",
    "#     def after_epoch(self):\n",
    "#         train_loss,valid_loss,rmse,mse = self.learn.recorder.values[-1]\n",
    "#         print(self.learn.recorder.values[-1])\n",
    "#         metrics = {\n",
    "#             'train_loss': train_loss,\n",
    "#             'valid_loss': valid_loss,\n",
    "#             'mean_loss': rmse,\n",
    "#             'mse': mse,\n",
    "#         }\n",
    "#         with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "#             ray.train.report(metrics, checkpoint=Checkpoint.from_directory(temp_checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_opt = HPOptimizer(create_lrn,dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 09:08:57,880\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "hp_opt.start_ray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"hidden_size\": tune.grid_search([10,20,50,100]),\n",
    "    'n_epoch':10,\n",
    "    # 'reporter':ray.put(CustomReporter)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-02-27 09:09:21</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:19.05        </td></tr>\n",
       "<tr><td>Memory:      </td><td>49.6/251.7 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/112 CPUs, 0.5/4 GPUs (0.0/1.0 accelerator_type:RTX)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc                   </th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  valid_loss</th><th style=\"text-align: right;\">  fun_rmse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>learner_optimize_77a19_00000</td><td>TERMINATED</td><td>141.23.125.123:2313691</td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         15.3151</td><td style=\"text-align: right;\">   0.0959706</td><td style=\"text-align: right;\">  0.0715517 </td><td style=\"text-align: right;\"> 0.267347 </td></tr>\n",
       "<tr><td>learner_optimize_77a19_00001</td><td>TERMINATED</td><td>141.23.125.123:2313693</td><td style=\"text-align: right;\">           20</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         15.1866</td><td style=\"text-align: right;\">   0.0574443</td><td style=\"text-align: right;\">  0.0602697 </td><td style=\"text-align: right;\"> 0.245406 </td></tr>\n",
       "<tr><td>learner_optimize_77a19_00002</td><td>TERMINATED</td><td>141.23.125.123:2313694</td><td style=\"text-align: right;\">           50</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         15.1335</td><td style=\"text-align: right;\">   0.0306168</td><td style=\"text-align: right;\">  0.017711  </td><td style=\"text-align: right;\"> 0.13303  </td></tr>\n",
       "<tr><td>learner_optimize_77a19_00003</td><td>TERMINATED</td><td>141.23.125.123:2313695</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         15.388 </td><td style=\"text-align: right;\">   0.0178903</td><td style=\"text-align: right;\">  0.00585899</td><td style=\"text-align: right;\"> 0.0765191</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(learner_optimize pid=2313693)\u001b[0m Library \"haste_pytorch\" not found\n",
      "\u001b[36m(learner_optimize pid=2313693)\u001b[0m [0, 0.08589156717061996, 0.08840160816907883, 0.2971020042896271, 0.08840160816907883, '00:02']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(learner_optimize pid=2313694)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/pheenix/ray_results/learner_optimize_2024-02-27_09-09-02/learner_optimize_77a19_00002_2_hidden_size=50_2024-02-27_09-09-02/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(learner_optimize pid=2313694)\u001b[0m Library \"haste_pytorch\" not found\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(learner_optimize pid=2313693)\u001b[0m [4, 0.0641716793179512, 0.061240144073963165, 0.2473810464143753, 0.061240144073963165, '00:01']\u001b[32m [repeated 16x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(learner_optimize pid=2313693)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/pheenix/ray_results/learner_optimize_2024-02-27_09-09-02/learner_optimize_77a19_00001_1_hidden_size=20_2024-02-27_09-09-02/checkpoint_000004)\u001b[32m [repeated 16x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(learner_optimize pid=2313694)\u001b[0m [8, 0.0348917655646801, 0.017931213602423668, 0.13385559618473053, 0.017931213602423668, '00:01']\u001b[32m [repeated 16x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(learner_optimize pid=2313694)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/pheenix/ray_results/learner_optimize_2024-02-27_09-09-02/learner_optimize_77a19_00002_2_hidden_size=50_2024-02-27_09-09-02/checkpoint_000008)\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "2024-02-27 09:09:21,214\tINFO tune.py:1042 -- Total run time: 19.07 seconds (19.05 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_opt.optimize(resources_per_trial={\"gpu\": 0.5},\n",
    "                config=search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 09:09:21,227\tWARNING experiment_analysis.py:584 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    }
   ],
   "source": [
    "hp_opt.analysis.get_best_config('mean_loss',mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(learner_optimize pid=2313695)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/pheenix/ray_results/learner_optimize_2024-02-27_09-09-02/learner_optimize_77a19_00003_3_hidden_size=100_2024-02-27_09-09-02/checkpoint_000009)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(learner_optimize pid=2313695)\u001b[0m [9, 0.017890259623527527, 0.005858990829437971, 0.07651908695697784, 0.005858990829437971, '00:01']\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
