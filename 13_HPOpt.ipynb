{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Pytorch Models for Sequential Data\n",
    "output-file: hpopt.html\n",
    "title: Hyperparameter Optimization Module\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp hpopt\n",
    "#| default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library \"haste_pytorch\" not found\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from seqdata.core import *\n",
    "from seqdata.models.core import *\n",
    "from seqdata.learner import *\n",
    "from fastai.basics import *\n",
    "from fastai.callback.schedule import *\n",
    "from fastai.callback.rnn import *\n",
    "from fastai.callback.tracker import *\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers import *\n",
    "from ray.tune.experiment.trial import ExportFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_paths = Path.cwd() / 'test_data/WienerHammerstein/'\n",
    "hdf_files = L([f for f in get_hdf_files(f_paths) if '_test.hdf5' not in str(f)])\n",
    "tfm_src = CreateDict([DfHDFCreateWindows(win_sz=400,stp_sz=100,clm='u')])\n",
    "dls = DataBlock(blocks=(SequenceBlock.from_hdf(['u'],TensorSequencesInput),\n",
    "                        SequenceBlock.from_hdf(['y'],TensorSequencesOutput)),\n",
    "                get_items=tfm_src,\n",
    "                splitter=ApplyToDict(FuncSplitter(lambda o: 'valid' in str(o)))).dataloaders(hdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a log uniform distibution for variables with vast value ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def log_uniform(min_bound, max_bound, base=10):\n",
    "    '''uniform sampling in an exponential range'''\n",
    "    logmin = np.log(min_bound) / np.log(base)\n",
    "    logmax = np.log(max_bound) / np.log(base)\n",
    "    def _sample():\n",
    "        return base**(np.random.uniform(logmin, logmax))\n",
    "    return _sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.360565948781439e-08,\n",
       " 0.0007031243952688263,\n",
       " 4.6828570547193034e-05,\n",
       " 4.8789405507773396e-08,\n",
       " 7.552366695077453e-05]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[log_uniform(1e-8, 1e-2)() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LearnerTrainable(tune.Trainable):\n",
    "\n",
    "    def setup(self, config):\n",
    "        self.create_lrn = ray.get(config['create_lrn'])\n",
    "        self.dls = ray.get(config['dls'])\n",
    "\n",
    "        self.lrn = self.create_lrn(self.dls,config)\n",
    "\n",
    "    def step(self):\n",
    "        with self.lrn.no_bar(): self.lrn.fit(1)\n",
    "        train_loss,valid_loss,rmse = self.lrn.recorder.values[-1]\n",
    "        result = {'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss,\n",
    "                'mean_loss': rmse}\n",
    "        return result\n",
    "\n",
    "    def save_checkpoint(self, tmp_checkpoint_dir):\n",
    "        checkpoint_path = os.path.join(tmp_checkpoint_dir, \"model.pth\")\n",
    "        torch.save(self.lrn.model.state_dict(), checkpoint_path)\n",
    "        return tmp_checkpoint_dir\n",
    "\n",
    "    def load_checkpoint(self, tmp_checkpoint_dir):\n",
    "        checkpoint_path = os.path.join(tmp_checkpoint_dir, \"model.pth\")\n",
    "        self.lrn.model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "    def _export_model(self, export_formats, export_dir):\n",
    "        if export_formats == [ExportFormat.MODEL]:\n",
    "            path = os.path.join(export_dir, \"exported_model\")\n",
    "            torch.save(self.lrn.model.state_dict(), path)\n",
    "            return {ExportFormat.MODEL: path}\n",
    "        else:\n",
    "            raise ValueError(\"unexpected formats: \" + str(export_formats))\n",
    "\n",
    "    # the learner class will be recreated with every perturbation, saving the model\n",
    "    # that way the new hyperparameter will be applied\n",
    "    def reset_config(self, new_config):\n",
    "        self.lrn = self.create_lrn(self.dls,new_config)\n",
    "        self.config = new_config\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.callback.tracker import SaveModelCallback \n",
    "class CBRaySaveModel(SaveModelCallback):\n",
    "    \"A `TrackerCallback` that saves the model's best during training in a tune checkpoint directory\"\n",
    "    \n",
    "    def _save(self, name):\n",
    "        if not hasattr(self,'idx_checkpoint'): self.idx_checkpoint = 0\n",
    "        with tune.checkpoint_dir(step=self.idx_checkpoint) as checkpoint_dir:\n",
    "            file = os.path.join(checkpoint_dir,name+'.pth')\n",
    "            print(file)\n",
    "            save_model(file, self.learn.model,opt=None)\n",
    "            self.last_saved_path = file\n",
    "            self.idx_checkpoint += 1\n",
    "            \n",
    "    #final checkpoint\n",
    "    def after_fit(self, **kwargs):\n",
    "        self._save(f'{self.fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def learner_optimize(config, checkpoint_dir=None):\n",
    "        create_lrn = ray.get(config['create_lrn'])\n",
    "        dls = ray.get(config['dls'])\n",
    "        \n",
    "        #Scheduling Parameters for training the Model\n",
    "        lrn_kwargs = {'n_epoch':100,'pct_start':0.5}\n",
    "        for attr in ['n_epoch','pct_start']:\n",
    "            if attr in config: lrn_kwargs[attr] = config[attr]\n",
    "\n",
    "        lrn = create_lrn(dls,config)\n",
    "        \n",
    "        # load checkpoint data if provided\n",
    "        if checkpoint_dir:\n",
    "            lrn.model.load_state_dict(torch.load(tmp_checkpoint_dir))\n",
    "        \n",
    "        lrn.lr = config['lr'] if 'lr' in config else 3e-3\n",
    "        lrn.add_cb(CBRayReporter() if 'reporter' not in config else ray.get(config['reporter'])())\n",
    "        lrn.add_cb(CBRaySaveModel())\n",
    "        with lrn.no_bar(): \n",
    "            ray.get(config['fit_method'])(lrn,**lrn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mutation config dictionary consists of functions that sample from a distribution. In order to retrieve a dictionary with one realisation we need the function sample_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample_config(config):\n",
    "    ret_conf = config.copy()\n",
    "    for k in ret_conf:\n",
    "        ret_conf[k]=ret_conf[k]()\n",
    "    return ret_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CBRayReporter(Callback):\n",
    "    \"`Callback` reports progress after every epoch to the ray tune logger\"\n",
    "    \n",
    "    order=70 #order has to be >50, to be executed after the recorder callback\n",
    "\n",
    "    def after_epoch(self):\n",
    "        train_loss,valid_loss,rmse = self.learn.recorder.values[-1]\n",
    "        tune.report(train_loss=train_loss,\n",
    "                        valid_loss=valid_loss,\n",
    "                        mean_loss=rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HPOptimizer():\n",
    "    def __init__(self,create_lrn,dls):\n",
    "        self.create_lrn = create_lrn\n",
    "        self.dls = dls\n",
    "        self.analysis = None\n",
    "    \n",
    "    @delegates(ray.init)\n",
    "    def start_ray(self,**kwargs):\n",
    "        ray.shutdown()\n",
    "        ray.init(**kwargs)\n",
    "        \n",
    "    def stop_ray(self):\n",
    "        ray.shutdown()\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    @delegates(tune.run, keep=True)\n",
    "    def optimize(self,config,optimize_func=learner_optimize,resources_per_trial={\"gpu\": 1.0},verbose=1,**kwargs):\n",
    "        config['create_lrn'] = ray.put(self.create_lrn)\n",
    "        #dls are large objects, letting ray handle the copying process makes it much faster\n",
    "        config['dls'] = ray.put(self.dls) \n",
    "        if 'fit_method' not in config: config['fit_method'] = ray.put(Learner.fit_flat_cos)\n",
    "\n",
    "        self.analysis = tune.run(\n",
    "            optimize_func,\n",
    "            config=config,\n",
    "            resources_per_trial=resources_per_trial,\n",
    "            verbose=verbose,\n",
    "            **kwargs)\n",
    "        return self.analysis\n",
    "        \n",
    "    @delegates(tune.run, keep=True)\n",
    "    def optimize_pbt(self,opt_name,num_samples,config,mut_conf,perturbation_interval=2,\n",
    "                 stop={\"training_iteration\": 40 },\n",
    "                 resources_per_trial={\"gpu\": 1 },\n",
    "                 resample_probability=0.25,\n",
    "                 quantile_fraction=0.25,\n",
    "                 **kwargs):\n",
    "        self.mut_conf = mut_conf\n",
    "        \n",
    "        config['create_lrn'] = ray.put(self.create_lrn)\n",
    "        #dls are large objects, letting ray handle the copying process makes it much faster\n",
    "        config['dls'] = ray.put(self.dls) \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        scheduler = PopulationBasedTraining(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"mean_loss\",\n",
    "        mode=\"min\",\n",
    "        perturbation_interval=perturbation_interval,\n",
    "        resample_probability=resample_probability,\n",
    "        quantile_fraction=quantile_fraction,\n",
    "        hyperparam_mutations=mut_conf)\n",
    "        \n",
    "        self.analysis = tune.run(\n",
    "            LearnerTrainable,\n",
    "            name=opt_name,\n",
    "            scheduler=scheduler,\n",
    "            reuse_actors=True,\n",
    "            verbose=1,\n",
    "            stop=stop,\n",
    "            checkpoint_score_attr=\"mean_loss\",\n",
    "            num_samples=num_samples,\n",
    "            resources_per_trial=resources_per_trial,\n",
    "            config=config,\n",
    "            **kwargs)\n",
    "        return self.analysis\n",
    "    \n",
    "    def best_model(self):\n",
    "        if self.analysis is None: raise Exception\n",
    "        model = self.create_lrn(self.dls,sample_config(self.mut_conf)).model\n",
    "        f_path = ray.get(self.analysis.get_best_trial('mean_loss',mode='min').checkpoint.value)\n",
    "        model.load_state_dict(torch.load(f_path))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Population Based Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lrn(dls,config):\n",
    "    lr = config['lr']\n",
    "    alpha = config['alpha']\n",
    "    beta = config['beta']\n",
    "    weight_p = config['weight_p']\n",
    "    \n",
    "    lrn = RNNLearner(dls)\n",
    "    lrn.lr = lr\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-06-02 11:42:20</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:17.15        </td></tr>\n",
       "<tr><td>Memory:      </td><td>26.9/251.7 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      PopulationBasedTraining: 2 checkpoints, 1 perturbs<br>Logical resource usage: 0/112 CPUs, 0/4 GPUs (0.0/1.0 accelerator_type:RTX)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc                   </th><th style=\"text-align: right;\">      alpha</th><th style=\"text-align: right;\">       beta</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  weight_p</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  valid_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LearnerTrainable_baea2_00000</td><td>TERMINATED</td><td>141.23.125.123:2016777</td><td style=\"text-align: right;\">6.06626e-05</td><td style=\"text-align: right;\">1.15864    </td><td style=\"text-align: right;\">0.000395092</td><td style=\"text-align: right;\"> 0.356367 </td><td style=\"text-align: right;\">0.245526</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         3.5451 </td><td style=\"text-align: right;\">   0.0552665</td><td style=\"text-align: right;\">   0.0603346</td></tr>\n",
       "<tr><td>LearnerTrainable_baea2_00001</td><td>TERMINATED</td><td>141.23.125.123:2017023</td><td style=\"text-align: right;\">0.000426795</td><td style=\"text-align: right;\">0.000405125</td><td style=\"text-align: right;\">0.000813385</td><td style=\"text-align: right;\"> 0.263439 </td><td style=\"text-align: right;\">0.245425</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         3.55573</td><td style=\"text-align: right;\">   0.0550838</td><td style=\"text-align: right;\">   0.060279 </td></tr>\n",
       "<tr><td>LearnerTrainable_baea2_00002</td><td>TERMINATED</td><td>141.23.125.123:2017021</td><td style=\"text-align: right;\">5.13894e-05</td><td style=\"text-align: right;\">0.00366434 </td><td style=\"text-align: right;\">0.000576683</td><td style=\"text-align: right;\"> 0.0545391</td><td style=\"text-align: right;\">0.246309</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         3.90986</td><td style=\"text-align: right;\">   0.0557063</td><td style=\"text-align: right;\">   0.0607176</td></tr>\n",
       "<tr><td>LearnerTrainable_baea2_00003</td><td>TERMINATED</td><td>141.23.125.123:2054475</td><td style=\"text-align: right;\">2.63035    </td><td style=\"text-align: right;\">0.427742   </td><td style=\"text-align: right;\">0.00035838 </td><td style=\"text-align: right;\"> 0.329298 </td><td style=\"text-align: right;\">0.245838</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         4.44512</td><td style=\"text-align: right;\">   0.0551142</td><td style=\"text-align: right;\">   0.0604877</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LearnerTrainable pid=2016777)\u001b[0m Library \"haste_pytorch\" not found\n",
      "\u001b[2m\u001b[36m(LearnerTrainable pid=2016777)\u001b[0m [0, 0.05623083561658859, 0.06141030788421631, 0.24773479998111725, '00:01']\n",
      "\u001b[2m\u001b[36m(LearnerTrainable pid=2016777)\u001b[0m [0, 0.055782824754714966, 0.06067557632923126, 0.24623264372348785, '00:00']\n",
      "\u001b[2m\u001b[36m(LearnerTrainable pid=2016777)\u001b[0m [0, 0.055266521871089935, 0.06033457815647125, 0.24552637338638306, '00:00']\n",
      "\u001b[2m\u001b[36m(LearnerTrainable pid=2017021)\u001b[0m Library \"haste_pytorch\" not found\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 11:42:14,297\tINFO pbt.py:808 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial baea2_00003 (score = -0.246968) into trial baea2_00001 (score = -0.248813)\n",
      "\n",
      "2023-06-02 11:42:14,298\tINFO pbt.py:835 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trialbaea2_00001:\n",
      "lr : 0.0003583798133123241 --- (resample) --> 0.0008133846763418052\n",
      "alpha : 2.630345446261178 --- (resample) --> 0.0004267946720993688\n",
      "beta : 0.4277416920291437 --- (resample) --> 0.00040512506873475264\n",
      "weight_p : 0.3292983861228925 --- (* 0.8) --> 0.263438708898314\n",
      "\n",
      "2023-06-02 11:42:14,300\tWARNING trial_runner.py:1543 -- You are trying to access pause_trial interface of TrialRunner in TrialScheduler, which is being restricted. If you believe it is reasonable for your scheduler to access this TrialRunner API, please reach out to Ray team on GitHub. A more strict API access pattern would be enforced starting 1.12s.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LearnerTrainable pid=2017023)\u001b[0m [0, 0.05556301400065422, 0.06058104336261749, 0.24602438509464264, '00:00']\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LearnerTrainable pid=2017023)\u001b[0m 2023-06-02 11:42:14,885\tINFO trainable.py:913 -- Restored on 141.23.125.123 from checkpoint: /tmp/checkpoint_tmp_8c56e66bcc4544ca9bcf579ea7d0cd33\n",
      "\u001b[2m\u001b[36m(LearnerTrainable pid=2017023)\u001b[0m 2023-06-02 11:42:14,885\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 1.769798994064331, '_episodes_total': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LearnerTrainable pid=2054475)\u001b[0m [0, 0.05511416494846344, 0.06048772484064102, 0.24583807587623596, '00:01']\n",
      "\u001b[2m\u001b[36m(LearnerTrainable pid=2054475)\u001b[0m [0, 0.05511416494846344, 0.06048772484064102, 0.24583807587623596, '00:01']\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 11:42:20,781\tINFO tune.py:945 -- Total run time: 17.16 seconds (17.14 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\n",
    "            \"lr\": tune.loguniform(1e-2, 1e-4),\n",
    "            \"alpha\": tune.loguniform(1e-5, 10),\n",
    "            \"beta\": tune.loguniform(1e-5, 10),\n",
    "            \"weight_p\": tune.uniform(0, 0.5)}\n",
    "mut_conf = {# distribution for resampling\n",
    "            \"lr\": log_uniform(1e-8, 1e-2),\n",
    "            \"alpha\": log_uniform(1e-5, 10),\n",
    "            \"beta\": log_uniform(1e-5, 10),\n",
    "            \"weight_p\": lambda: np.random.uniform(0, 0.5)}\n",
    "\n",
    "hp_opt = HPOptimizer(create_lrn,dls)\n",
    "hp_opt.start_ray()\n",
    "hp_opt.optimize_pbt('pbt_test',4,config,mut_conf,perturbation_interval=1,\n",
    "                 stop={\"training_iteration\": 3 },\n",
    "                 resources_per_trial={\"gpu\": 0.5},\n",
    "                 local_dir=Path.home() / 'ray_results')#no cpu count is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hp_opt.best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lrn(dls,config):\n",
    "    lrn = RNNLearner(dls,hidden_size=config['hidden_size'],metrics=[fun_rmse,mse])\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReporter(Callback):\n",
    "    \"`Callback` reports progress after every epoch to the ray tune logger\"\n",
    "    \n",
    "    order=70 #order has to be >50, to be executed after the recorder callback\n",
    "\n",
    "    def after_epoch(self):\n",
    "        train_loss,valid_loss,rmse,mse = self.learn.recorder.values[-1]\n",
    "        tune.report(train_loss=train_loss,\n",
    "                        valid_loss=valid_loss,\n",
    "                        mean_loss=rmse,\n",
    "                        mse=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_opt = HPOptimizer(create_lrn,dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LearnerTrainable pid=2054475)\u001b[0m 2023-06-02 11:42:19,031\tINFO trainable.py:913 -- Restored on 141.23.125.123 from checkpoint: /tmp/checkpoint_tmp_e10d019bdb3c4142a570321db81ebfe8\n",
      "\u001b[2m\u001b[36m(LearnerTrainable pid=2054475)\u001b[0m 2023-06-02 11:42:19,031\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 2.717446804046631, '_episodes_total': None}\n",
      "2023-06-02 11:42:30,256\tINFO worker.py:1625 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "hp_opt.start_ray(local_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"hidden_size\": tune.grid_search([10,20,50,100]),\n",
    "    'n_epoch':10,\n",
    "    'reporter':ray.put(CustomReporter)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-06-02 11:42:58</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:19.15        </td></tr>\n",
       "<tr><td>Memory:      </td><td>27.0/251.7 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/112 CPUs, 0/4 GPUs (0.0/1.0 accelerator_type:RTX)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc                   </th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">     loss</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  valid_loss</th><th style=\"text-align: right;\">       mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>learner_optimize_d06c4_00000</td><td>TERMINATED</td><td>141.23.125.123:2080632</td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">0.247013 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         12.565 </td><td style=\"text-align: right;\">   0.0649806</td><td style=\"text-align: right;\">  0.0610653 </td><td style=\"text-align: right;\">0.0610653 </td></tr>\n",
       "<tr><td>learner_optimize_d06c4_00001</td><td>TERMINATED</td><td>141.23.125.123:2080839</td><td style=\"text-align: right;\">           20</td><td style=\"text-align: right;\">0.242244 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         13.2356</td><td style=\"text-align: right;\">   0.0566147</td><td style=\"text-align: right;\">  0.0587278 </td><td style=\"text-align: right;\">0.0587278 </td></tr>\n",
       "<tr><td>learner_optimize_d06c4_00002</td><td>TERMINATED</td><td>141.23.125.123:2080841</td><td style=\"text-align: right;\">           50</td><td style=\"text-align: right;\">0.23657  </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         13.3229</td><td style=\"text-align: right;\">   0.0549262</td><td style=\"text-align: right;\">  0.0560062 </td><td style=\"text-align: right;\">0.0560062 </td></tr>\n",
       "<tr><td>learner_optimize_d06c4_00003</td><td>TERMINATED</td><td>141.23.125.123:2080843</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">0.0732271</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         12.2489</td><td style=\"text-align: right;\">   0.0163189</td><td style=\"text-align: right;\">  0.00536533</td><td style=\"text-align: right;\">0.00536533</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(learner_optimize pid=2080632)\u001b[0m Library \"haste_pytorch\" not found\n",
      "\u001b[2m\u001b[36m(learner_optimize pid=2080632)\u001b[0m [0, 0.1300196796655655, 0.13209111988544464, 0.36324384808540344, 0.13209111988544464, '00:01']\n",
      "\u001b[2m\u001b[36m(learner_optimize pid=2080632)\u001b[0m Better model found at epoch 0 with valid_loss value: 0.13209111988544464.\n",
      "\u001b[2m\u001b[36m(learner_optimize pid=2080632)\u001b[0m /home/pheenix/ray_results/learner_optimize_2023-06-02_11-42-39/learner_optimize_d06c4_00000_0_hidden_size=10_2023-06-02_11-42-39/checkpoint_000000/model.pth\n",
      "\u001b[2m\u001b[36m(learner_optimize pid=2080632)\u001b[0m [1, 0.1227642297744751, 0.1166750118136406, 0.34136971831321716, 0.1166750118136406, '00:00']\n",
      "\u001b[2m\u001b[36m(learner_optimize pid=2080632)\u001b[0m Better model found at epoch 1 with valid_loss value: 0.1166750118136406.\n",
      "\u001b[2m\u001b[36m(learner_optimize pid=2080632)\u001b[0m /home/pheenix/ray_results/learner_optimize_2023-06-02_11-42-39/learner_optimize_d06c4_00000_0_hidden_size=10_2023-06-02_11-42-39/checkpoint_000001/model.pth\n",
      "\u001b[2m\u001b[36m(learner_optimize pid=2080632)\u001b[0m [2, 0.11328967660665512, 0.09937257319688797, 0.31502074003219604, 0.09937257319688797, '00:00']\n",
      "\u001b[2m\u001b[36m(learner_optimize pid=2080632)\u001b[0m Better model found at epoch 2 with valid_loss value: 0.09937257319688797.\n",
      "\u001b[2m\u001b[36m(learner_optimize pid=2080632)\u001b[0m /home/pheenix/ray_results/learner_optimize_2023-06-02_11-42-39/learner_optimize_d06c4_00000_0_hidden_size=10_2023-06-02_11-42-39/checkpoint_000002/model.pth\n",
      "\u001b[2m\u001b[36m(learner_optimize pid=2080839)\u001b[0m Library \"haste_pytorch\" not found\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(learner_optimize pid=2080843)\u001b[0m /home/pheenix/ray_results/learner_optimize_2023-06-02_11-42-39/learner_optimize_d06c4_00003_3_hidden_size=100_2023-06-02_11-42-42/checkpoint_000002/model.pth\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(learner_optimize pid=2080843)\u001b[0m Better model found at epoch 2 with valid_loss value: 0.057177722454071045.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(learner_optimize pid=2080632)\u001b[0m /home/pheenix/ray_results/learner_optimize_2023-06-02_11-42-39/learner_optimize_d06c4_00000_0_hidden_size=10_2023-06-02_11-42-39/checkpoint_000010/model.pth\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(learner_optimize pid=2080632)\u001b[0m Better model found at epoch 9 with valid_loss value: 0.061065252870321274.\u001b[32m [repeated 13x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 11:42:58,873\tINFO tune.py:945 -- Total run time: 19.18 seconds (19.14 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_opt.optimize(resources_per_trial={\"gpu\": 0.5},\n",
    "                config=search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_size': 100,\n",
       " 'n_epoch': 10,\n",
       " 'reporter': ObjectRef(00ffffffffffffffffffffffffffffffffffffff0100000005000000),\n",
       " 'create_lrn': ObjectRef(00ffffffffffffffffffffffffffffffffffffff0100000006000000),\n",
       " 'dls': ObjectRef(00ffffffffffffffffffffffffffffffffffffff0100000007000000),\n",
       " 'fit_method': ObjectRef(00ffffffffffffffffffffffffffffffffffffff0100000008000000)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_opt.analysis.get_best_config('mean_loss',mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
