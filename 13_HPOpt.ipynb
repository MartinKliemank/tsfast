{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Pytorch Models for Sequential Data\n",
    "output-file: hpopt.html\n",
    "title: Hyperparameter Optimization Module\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp hpopt\n",
    "#| default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library \"haste_pytorch\" not found\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from seqdata.core import *\n",
    "from seqdata.models.core import *\n",
    "from seqdata.learner import *\n",
    "from fastai.basics import *\n",
    "from fastai.callback.schedule import *\n",
    "from fastai.callback.rnn import *\n",
    "from fastai.callback.tracker import *\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers import *\n",
    "from ray.tune.experiment.trial import ExportFormat\n",
    "from ray import train\n",
    "from ray.train import Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_paths = Path.cwd() / 'test_data/WienerHammerstein/'\n",
    "hdf_files = L([f for f in get_hdf_files(f_paths) if '_test.hdf5' not in str(f)])\n",
    "tfm_src = CreateDict([DfHDFCreateWindows(win_sz=400,stp_sz=100,clm='u')])\n",
    "dls = DataBlock(blocks=(SequenceBlock.from_hdf(['u'],TensorSequencesInput),\n",
    "                        SequenceBlock.from_hdf(['y'],TensorSequencesOutput)),\n",
    "                get_items=tfm_src,\n",
    "                splitter=ApplyToDict(FuncSplitter(lambda o: 'valid' in str(o)))).dataloaders(hdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a log uniform distibution for variables with vast value ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def log_uniform(min_bound, max_bound, base=10):\n",
    "    '''uniform sampling in an exponential range'''\n",
    "    logmin = np.log(min_bound) / np.log(base)\n",
    "    logmax = np.log(max_bound) / np.log(base)\n",
    "    def _sample():\n",
    "        return base**(np.random.uniform(logmin, logmax))\n",
    "    return _sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.1354527820082406e-07,\n",
       " 2.6678100760137837e-07,\n",
       " 5.19785921421441e-06,\n",
       " 1.4530731602317383e-08,\n",
       " 0.0018066853260662302]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[log_uniform(1e-8, 1e-2)() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LearnerTrainable(tune.Trainable):\n",
    "\n",
    "    def setup(self, config):\n",
    "        self.create_lrn = ray.get(config['create_lrn'])\n",
    "        self.dls = ray.get(config['dls'])\n",
    "\n",
    "        self.lrn = self.create_lrn(self.dls,config)\n",
    "\n",
    "    def step(self):\n",
    "        with self.lrn.no_bar(): self.lrn.fit(1)\n",
    "        train_loss,valid_loss,rmse = self.lrn.recorder.values[-1]\n",
    "        result = {'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss,\n",
    "                'mean_loss': rmse}\n",
    "        return result\n",
    "\n",
    "    def save_checkpoint(self, tmp_checkpoint_dir):\n",
    "        checkpoint_path = os.path.join(tmp_checkpoint_dir, \"model.pth\")\n",
    "        torch.save(self.lrn.model.state_dict(), checkpoint_path)\n",
    "        return tmp_checkpoint_dir\n",
    "\n",
    "    def load_checkpoint(self, tmp_checkpoint_dir):\n",
    "        checkpoint_path = os.path.join(tmp_checkpoint_dir, \"model.pth\")\n",
    "        self.lrn.model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "    def _export_model(self, export_formats, export_dir):\n",
    "        if export_formats == [ExportFormat.MODEL]:\n",
    "            path = os.path.join(export_dir, \"exported_model\")\n",
    "            torch.save(self.lrn.model.state_dict(), path)\n",
    "            return {ExportFormat.MODEL: path}\n",
    "        else:\n",
    "            raise ValueError(\"unexpected formats: \" + str(export_formats))\n",
    "\n",
    "    # the learner class will be recreated with every perturbation, saving the model\n",
    "    # that way the new hyperparameter will be applied\n",
    "    def reset_config(self, new_config):\n",
    "        self.lrn = self.create_lrn(self.dls,new_config)\n",
    "        self.config = new_config\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.callback.tracker import SaveModelCallback \n",
    "class CBRaySaveModel(SaveModelCallback):\n",
    "    \"A `TrackerCallback` that saves the model's best during training in a tune checkpoint directory\"\n",
    "    \n",
    "    def _save(self, name):\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            file = os.path.join(temp_checkpoint_dir,name+'.pth')\n",
    "            save_model(file, self.learn.model,opt=None)\n",
    "            self.last_saved_path = file\n",
    "            \n",
    "    #final checkpoint\n",
    "    def after_fit(self, **kwargs):\n",
    "        self._save(f'{self.fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def learner_optimize(config):\n",
    "        create_lrn = ray.get(config['create_lrn'])\n",
    "        dls = ray.get(config['dls'])\n",
    "        \n",
    "        #Scheduling Parameters for training the Model\n",
    "        lrn_kwargs = {'n_epoch':100,'pct_start':0.5}\n",
    "        for attr in ['n_epoch','pct_start']:\n",
    "            if attr in config: lrn_kwargs[attr] = config[attr]\n",
    "\n",
    "        lrn = create_lrn(dls,config)\n",
    "        \n",
    "        # load checkpoint data if provided\n",
    "        checkpoint: train.Checkpoint = train.get_checkpoint()\n",
    "        if checkpoint:\n",
    "            with checkpoint.as_directory() as checkpoint_dir:\n",
    "                lrn.model.load_state_dict(torch.load(checkpoint_dir + 'model.pth'))\n",
    "        \n",
    "        lrn.lr = config['lr'] if 'lr' in config else 3e-3\n",
    "        lrn.add_cb(CBRayReporter() if 'reporter' not in config else ray.get(config['reporter'])())\n",
    "        lrn.add_cb(CBRaySaveModel())\n",
    "        with lrn.no_bar(): \n",
    "            ray.get(config['fit_method'])(lrn,**lrn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mutation config dictionary consists of functions that sample from a distribution. In order to retrieve a dictionary with one realisation we need the function sample_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample_config(config):\n",
    "    ret_conf = config.copy()\n",
    "    for k in ret_conf:\n",
    "        ret_conf[k]=ret_conf[k]()\n",
    "    return ret_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CBRayReporter(Callback):\n",
    "    \"`Callback` reports progress after every epoch to the ray tune logger\"\n",
    "    \n",
    "    order=70 #order has to be >50, to be executed after the recorder callback\n",
    "\n",
    "    def after_epoch(self):\n",
    "        # train_loss,valid_loss,rmse = self.learn.recorder.values[-1]\n",
    "        # metrics = {\n",
    "        #     'train_loss': train_loss,\n",
    "        #     'valid_loss': valid_loss,\n",
    "        #     'mean_loss': rmse,\n",
    "        # }\n",
    "        scores = self.learn.recorder.values[-1]\n",
    "        metrics = {\n",
    "            'train_loss': scores[0],\n",
    "            'valid_loss': scores[1]\n",
    "        }\n",
    "        for metric,value in zip(self.learn.metrics,scores[2:]):\n",
    "            m_name = metric.name if hasattr(metric,'name') else str(metric)\n",
    "            metrics[m_name] = value\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            ray.train.report(metrics, checkpoint=Checkpoint.from_directory(temp_checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HPOptimizer():\n",
    "    def __init__(self,create_lrn,dls):\n",
    "        self.create_lrn = create_lrn\n",
    "        self.dls = dls\n",
    "        self.analysis = None\n",
    "    \n",
    "    @delegates(ray.init)\n",
    "    def start_ray(self,**kwargs):\n",
    "        ray.shutdown()\n",
    "        ray.init(**kwargs)\n",
    "        \n",
    "    def stop_ray(self):\n",
    "        ray.shutdown()\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    @delegates(tune.run, keep=True)\n",
    "    def optimize(self,config,optimize_func=learner_optimize,resources_per_trial={\"gpu\": 1.0},verbose=1,**kwargs):\n",
    "        config['create_lrn'] = ray.put(self.create_lrn)\n",
    "        #dls are large objects, letting ray handle the copying process makes it much faster\n",
    "        config['dls'] = ray.put(self.dls) \n",
    "        if 'fit_method' not in config: config['fit_method'] = ray.put(Learner.fit_flat_cos)\n",
    "\n",
    "        self.analysis = tune.run(\n",
    "            optimize_func,\n",
    "            config=config,\n",
    "            resources_per_trial=resources_per_trial,\n",
    "            verbose=verbose,\n",
    "            **kwargs)\n",
    "        return self.analysis\n",
    "        \n",
    "    @delegates(tune.run, keep=True)\n",
    "    def optimize_pbt(self,opt_name,num_samples,config,mut_conf,perturbation_interval=2,\n",
    "                 stop={\"training_iteration\": 40 },\n",
    "                 resources_per_trial={\"gpu\": 1 },\n",
    "                 resample_probability=0.25,\n",
    "                 quantile_fraction=0.25,\n",
    "                 **kwargs):\n",
    "        self.mut_conf = mut_conf\n",
    "        \n",
    "        config['create_lrn'] = ray.put(self.create_lrn)\n",
    "        #dls are large objects, letting ray handle the copying process makes it much faster\n",
    "        config['dls'] = ray.put(self.dls) \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        scheduler = PopulationBasedTraining(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"mean_loss\",\n",
    "        mode=\"min\",\n",
    "        perturbation_interval=perturbation_interval,\n",
    "        resample_probability=resample_probability,\n",
    "        quantile_fraction=quantile_fraction,\n",
    "        hyperparam_mutations=mut_conf)\n",
    "        \n",
    "        self.analysis = tune.run(\n",
    "            LearnerTrainable,\n",
    "            name=opt_name,\n",
    "            scheduler=scheduler,\n",
    "            reuse_actors=True,\n",
    "            verbose=1,\n",
    "            stop=stop,\n",
    "            checkpoint_score_attr=\"mean_loss\",\n",
    "            num_samples=num_samples,\n",
    "            resources_per_trial=resources_per_trial,\n",
    "            config=config,\n",
    "            **kwargs)\n",
    "        return self.analysis\n",
    "    \n",
    "    def best_model(self):\n",
    "        if self.analysis is None: raise Exception\n",
    "        model = self.create_lrn(self.dls,sample_config(self.mut_conf)).model\n",
    "        f_path = ray.get(self.analysis.get_best_trial('mean_loss',mode='min').checkpoint.value)\n",
    "        model.load_state_dict(torch.load(f_path))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Population Based Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lrn(dls,config):\n",
    "    lr = config['lr']\n",
    "    alpha = config['alpha']\n",
    "    beta = config['beta']\n",
    "    weight_p = config['weight_p']\n",
    "    \n",
    "    lrn = RNNLearner(dls)\n",
    "    lrn.lr = lr\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-01-16 21:42:47</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:09.07        </td></tr>\n",
       "<tr><td>Memory:      </td><td>13.3/251.7 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      PopulationBasedTraining: 1 checkpoints, 1 perturbs<br>Logical resource usage: 0/112 CPUs, 1.0/4 GPUs (0.0/1.0 accelerator_type:RTX)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc                   </th><th style=\"text-align: right;\">      alpha</th><th style=\"text-align: right;\">    beta</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  weight_p</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  valid_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LearnerTrainable_c96e4_00000</td><td>TERMINATED</td><td>141.23.125.123:1898972</td><td style=\"text-align: right;\">0.00196667 </td><td style=\"text-align: right;\">0.101946</td><td style=\"text-align: right;\">0.000619386</td><td style=\"text-align: right;\">  0.362489</td><td style=\"text-align: right;\">0.24595 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         3.85264</td><td style=\"text-align: right;\">   0.0552986</td><td style=\"text-align: right;\">   0.0605417</td></tr>\n",
       "<tr><td>LearnerTrainable_c96e4_00001</td><td>TERMINATED</td><td>141.23.125.123:1898973</td><td style=\"text-align: right;\">8.31283    </td><td style=\"text-align: right;\">1.23534 </td><td style=\"text-align: right;\">0.00945742 </td><td style=\"text-align: right;\">  0.343248</td><td style=\"text-align: right;\">0.190787</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         3.74056</td><td style=\"text-align: right;\">   0.0421032</td><td style=\"text-align: right;\">   0.0364181</td></tr>\n",
       "<tr><td>LearnerTrainable_c96e4_00002</td><td>TERMINATED</td><td>141.23.125.123:1898974</td><td style=\"text-align: right;\">0.0670224  </td><td style=\"text-align: right;\">0.360587</td><td style=\"text-align: right;\">0.00037046 </td><td style=\"text-align: right;\">  0.468087</td><td style=\"text-align: right;\">0.247418</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         3.85624</td><td style=\"text-align: right;\">   0.0560081</td><td style=\"text-align: right;\">   0.0612589</td></tr>\n",
       "<tr><td>LearnerTrainable_c96e4_00003</td><td>TERMINATED</td><td>141.23.125.123:1898973</td><td style=\"text-align: right;\">0.000107274</td><td style=\"text-align: right;\">1.48241 </td><td style=\"text-align: right;\">0.00756594 </td><td style=\"text-align: right;\">  0.411898</td><td style=\"text-align: right;\">0.232037</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         3.58664</td><td style=\"text-align: right;\">   0.0514592</td><td style=\"text-align: right;\">   0.0538803</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(LearnerTrainable pid=1898972)\u001b[0m Library \"haste_pytorch\" not found\n",
      "\u001b[36m(LearnerTrainable pid=1898973)\u001b[0m [0, 0.05915459245443344, 0.060427144169807434, 0.24573084712028503, '00:01']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 21:42:44,855\tINFO pbt.py:716 -- [pbt]: no checkpoint for trial LearnerTrainable_c96e4_00001. Skip exploit for Trial LearnerTrainable_c96e4_00003\n",
      "\u001b[36m(LearnerTrainable pid=1898973)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/pheenix/ray_results/pbt_test/LearnerTrainable_c96e4_00001_1_alpha=8.3128,beta=1.2353,lr=0.0095,weight_p=0.3432_2024-01-16_21-42-38/checkpoint_000000)\n",
      "2024-01-16 21:42:45,983\tINFO pbt.py:878 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial c96e4_00001 (score = -0.239847) into trial c96e4_00003 (score = -0.276435)\n",
      "\n",
      "2024-01-16 21:42:45,984\tINFO pbt.py:905 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trialc96e4_00003:\n",
      "lr : 0.00945742489328565 --- (* 0.8) --> 0.0075659399146285194\n",
      "alpha : 8.312832074369041 --- (resample) --> 0.00010727409665289279\n",
      "beta : 1.2353394457071754 --- (* 1.2) --> 1.4824073348486104\n",
      "weight_p : 0.3432481277547884 --- (* 1.2) --> 0.4118977533057461\n",
      "\n",
      "\u001b[36m(LearnerTrainable pid=1898972)\u001b[0m Restored on 141.23.125.123 from checkpoint: Checkpoint(filesystem=local, path=/home/pheenix/ray_results/pbt_test/LearnerTrainable_c96e4_00000_0_alpha=0.0020,beta=0.1019,lr=0.0006,weight_p=0.3625_2024-01-16_21-42-38/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(LearnerTrainable pid=1898974)\u001b[0m Library \"haste_pytorch\" not found\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 21:42:49,923\tINFO tune.py:1042 -- Total run time: 11.25 seconds (9.06 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\n",
    "            \"lr\": tune.loguniform(1e-2, 1e-4),\n",
    "            \"alpha\": tune.loguniform(1e-5, 10),\n",
    "            \"beta\": tune.loguniform(1e-5, 10),\n",
    "            \"weight_p\": tune.uniform(0, 0.5)}\n",
    "mut_conf = {# distribution for resampling\n",
    "            \"lr\": log_uniform(1e-8, 1e-2),\n",
    "            \"alpha\": log_uniform(1e-5, 10),\n",
    "            \"beta\": log_uniform(1e-5, 10),\n",
    "            \"weight_p\": lambda: np.random.uniform(0, 0.5)}\n",
    "\n",
    "hp_opt = HPOptimizer(create_lrn,dls)\n",
    "hp_opt.start_ray()\n",
    "hp_opt.optimize_pbt('pbt_test',4,config,mut_conf,perturbation_interval=1,\n",
    "                 stop={\"training_iteration\": 3 },\n",
    "                 resources_per_trial={\"gpu\": 0.5},\n",
    "                 storage_path=str(Path.home() / 'ray_results'))#no cpu count is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hp_opt.best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lrn(dls,config):\n",
    "    lrn = RNNLearner(dls,hidden_size=config['hidden_size'],metrics=[fun_rmse,mse])\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReporter(Callback):\n",
    "    \"`Callback` reports progress after every epoch to the ray tune logger\"\n",
    "    \n",
    "    order=70 #order has to be >50, to be executed after the recorder callback\n",
    "\n",
    "    def after_epoch(self):\n",
    "        train_loss,valid_loss,rmse,mse = self.learn.recorder.values[-1]\n",
    "        print(self.learn.recorder.values[-1])\n",
    "        metrics = {\n",
    "            'train_loss': train_loss,\n",
    "            'valid_loss': valid_loss,\n",
    "            'mean_loss': rmse,\n",
    "            'mse': mse,\n",
    "        }\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            ray.train.report(metrics, checkpoint=Checkpoint.from_directory(temp_checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_opt = HPOptimizer(create_lrn,dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 21:58:00,222\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "hp_opt.start_ray(local_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"hidden_size\": tune.grid_search([10,20,50,100]),\n",
    "    'n_epoch':10,\n",
    "    # 'reporter':ray.put(CustomReporter)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-01-16 21:58:17</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:15.72        </td></tr>\n",
       "<tr><td>Memory:      </td><td>13.9/251.7 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/112 CPUs, 0.5/4 GPUs (0.0/1.0 accelerator_type:RTX)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc                   </th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  valid_loss</th><th style=\"text-align: right;\">  fun_rmse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>learner_optimize_ef5a6_00000</td><td>TERMINATED</td><td>141.23.125.123:2532984</td><td style=\"text-align: right;\">           10</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         12.2926</td><td style=\"text-align: right;\">   0.0554329</td><td style=\"text-align: right;\">   0.0593868</td><td style=\"text-align: right;\"> 0.243596 </td></tr>\n",
       "<tr><td>learner_optimize_ef5a6_00001</td><td>TERMINATED</td><td>141.23.125.123:2532985</td><td style=\"text-align: right;\">           20</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         12.4891</td><td style=\"text-align: right;\">   0.0505375</td><td style=\"text-align: right;\">   0.0515845</td><td style=\"text-align: right;\"> 0.227052 </td></tr>\n",
       "<tr><td>learner_optimize_ef5a6_00002</td><td>TERMINATED</td><td>141.23.125.123:2532986</td><td style=\"text-align: right;\">           50</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         12.6524</td><td style=\"text-align: right;\">   0.0433181</td><td style=\"text-align: right;\">   0.0343779</td><td style=\"text-align: right;\"> 0.185372 </td></tr>\n",
       "<tr><td>learner_optimize_ef5a6_00003</td><td>TERMINATED</td><td>141.23.125.123:2532987</td><td style=\"text-align: right;\">          100</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         12.2271</td><td style=\"text-align: right;\">   0.0177341</td><td style=\"text-align: right;\">   0.0057915</td><td style=\"text-align: right;\"> 0.0760743</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(learner_optimize pid=2532986)\u001b[0m Library \"haste_pytorch\" not found\n",
      "\u001b[36m(learner_optimize pid=2532984)\u001b[0m [0, 0.06718236953020096, 0.07150647044181824, 0.26737454533576965, 0.07150647044181824, '00:01']\n",
      "\u001b[36m(learner_optimize pid=2532984)\u001b[0m Better model found at epoch 0 with valid_loss value: 0.07150647044181824.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(learner_optimize pid=2532984)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/pheenix/ray_results/learner_optimize_2024-01-16_21-58-01/learner_optimize_ef5a6_00000_0_hidden_size=10_2024-01-16_21-58-01/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(learner_optimize pid=2532985)\u001b[0m Library \"haste_pytorch\" not found\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(learner_optimize pid=2532987)\u001b[0m [5, 0.040142856538295746, 0.017964176833629608, 0.13396701216697693, 0.017964176833629608, '00:01']\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[36m(learner_optimize pid=2532987)\u001b[0m Better model found at epoch 5 with valid_loss value: 0.017964176833629608.\u001b[32m [repeated 19x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(learner_optimize pid=2532987)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/pheenix/ray_results/learner_optimize_2024-01-16_21-58-01/learner_optimize_ef5a6_00003_3_hidden_size=100_2024-01-16_21-58-01/checkpoint_000005)\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "2024-01-16 21:58:17,032\tINFO tune.py:1042 -- Total run time: 15.75 seconds (15.71 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(learner_optimize pid=2532985)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/pheenix/ray_results/learner_optimize_2024-01-16_21-58-01/learner_optimize_ef5a6_00001_1_hidden_size=20_2024-01-16_21-58-01/checkpoint_000009)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(learner_optimize pid=2532986)\u001b[0m [9, 0.04331808537244797, 0.034377869218587875, 0.18537168204784393, 0.034377869218587875, '00:01']\n",
      "\u001b[36m(learner_optimize pid=2532986)\u001b[0m Better model found at epoch 9 with valid_loss value: 0.034377869218587875.\n"
     ]
    }
   ],
   "source": [
    "hp_opt.optimize(resources_per_trial={\"gpu\": 0.5},\n",
    "                config=search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 21:59:03,852\tWARNING experiment_analysis.py:584 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    }
   ],
   "source": [
    "hp_opt.analysis.get_best_config('mean_loss',mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
