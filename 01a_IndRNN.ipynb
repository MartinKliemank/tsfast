{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.indrnn\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IndRNN\n",
    "> Pytorch Models for Sequential Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from seqdata.core import *\n",
    "from fastai2.basics import *\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "#import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from cupy.cuda import function\n",
    "from pynvrtc.compiler import Program\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = DataBlock(blocks=(SequenceBlock.from_hdf(['current','voltage'],TensorSequencesInput,clm_shift=[0,-1]),\n",
    "                        SequenceBlock.from_hdf(['voltage'],TensorSequencesOutput,clm_shift=[-1])),\n",
    "                 get_items=CreateDict([DfHDFCreateWindows(win_sz=1000+1,stp_sz=50,clm='current')]),\n",
    "                 splitter=ApplyToDict(ParentSplitter()))\n",
    "db = seq.dataloaders(get_hdf_files('/mnt/data/Development/seqdata/test_data/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndRNN Cuda Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "IndRNN_CODE = \"\"\"\n",
    "extern \"C\" {\n",
    "\n",
    "    __forceinline__ __device__ float reluf(float x)\n",
    "    {\n",
    "        return (x > 0.f) ? x : 0.f;\n",
    "    }\n",
    "\n",
    "    __forceinline__ __device__ float calc_grad_activation(float x)\n",
    "    {\n",
    "        return (x > 0.f) ? 1.f : 0.f;\n",
    "    }\n",
    "\n",
    "    __global__ void indrnn_fwd( const float * __restrict__ x,\n",
    "                            const float * __restrict__ weight_hh, const float * __restrict__ h0,\n",
    "                            const int len, const int batch, const int hidden_size, \n",
    "                            float * __restrict__ h)\n",
    "    {\n",
    "        int ncols = batch*hidden_size;\n",
    "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "        if (col >= ncols) return;       \n",
    "        const float weight_hh_cur = *(weight_hh + (col%hidden_size));\n",
    "        float cur = *(h0 + col);\n",
    "        const float *xp = x+col;\n",
    "        float *hp = h+col;\n",
    "\n",
    "        for (int row = 0; row < len; ++row)\n",
    "        {\n",
    "            cur=reluf(cur*weight_hh_cur+(*xp));\n",
    "            *hp=cur;\n",
    "            xp += ncols;\n",
    "            hp += ncols;            \n",
    "        }\n",
    "    }\n",
    "\n",
    "    __global__ void indrnn_bwd(const float * __restrict__ x,\n",
    "                             const float * __restrict__ weight_hh, const float * __restrict__ h0,\n",
    "                             const float * __restrict__ h,\n",
    "                            const float * __restrict__ grad_h, \n",
    "                            const int len, const int batch, const int hidden_size, \n",
    "                            float * __restrict__ grad_x,\n",
    "                            float * __restrict__ grad_weight_hh, float * __restrict__ grad_h0)\n",
    "    {    \n",
    "        int ncols = batch*hidden_size;\n",
    "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "        if (col >= ncols) return;        \n",
    "        const float weight_hh_cur = *(weight_hh + (col%hidden_size));\n",
    "        float gweight_hh_cur = 0;\n",
    "        float cur = 0;  // *(grad_last + col);        //0; strange gradient behavior. grad_last and grad_h, one of them is zero.     \n",
    "        \n",
    "        const float *xp = x+col + (len-1)*ncols;\n",
    "        const float *hp = h+col + (len-1)*ncols;      \n",
    "        float *gxp = grad_x + col + (len-1)*ncols;\n",
    "        const float *ghp = grad_h + col + (len-1)*ncols;\n",
    "        \n",
    "\n",
    "        for (int row = len-1; row >= 0; --row)\n",
    "        {        \n",
    "            const float prev_h_val = (row>0) ? (*(hp-ncols)) : (*(h0+col));\n",
    "            //float h_val_beforeact = prev_h_val*weight_hh_cur+(*xp);\n",
    "            float gh_beforeact = ((*ghp) + cur)*calc_grad_activation(prev_h_val*weight_hh_cur+(*xp));\n",
    "            cur = gh_beforeact*weight_hh_cur;\n",
    "            gweight_hh_cur += gh_beforeact*prev_h_val;\n",
    "            *gxp = gh_beforeact;\n",
    "\n",
    "            xp -= ncols;\n",
    "            hp -= ncols;\n",
    "            gxp -= ncols;\n",
    "            ghp -= ncols;        \n",
    "        }\n",
    "\n",
    "        atomicAdd(grad_weight_hh + (col%hidden_size), gweight_hh_cur);\n",
    "        *(grad_h0 +col) = cur;\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_indrnn_cuda(device):\n",
    "    if not hasattr(_get_indrnn_cuda, '_DEVICE2FUNC'):_get_indrnn_cuda._DEVICE2FUNC = {}\n",
    "    res = _get_indrnn_cuda._DEVICE2FUNC.get(device, None)\n",
    "    if res:\n",
    "        return res\n",
    "    else:\n",
    "        print ('IndRNN loaded for gpu {}'.format(device))\n",
    "        mod = function.Module()\n",
    "        mod.load(bytes(Program(IndRNN_CODE, 'indrnn_prog.cu').compile().encode()))\n",
    "        fwd_func = mod.get_function('indrnn_fwd')\n",
    "        bwd_func = mod.get_function('indrnn_bwd')\n",
    "\n",
    "        Stream = namedtuple('Stream', ['ptr'])\n",
    "        current_stream = Stream(ptr=torch.cuda.current_stream().cuda_stream)\n",
    "\n",
    "        _get_indrnn_cuda._DEVICE2FUNC[device] = (current_stream, fwd_func, bwd_func)\n",
    "        return current_stream, fwd_func, bwd_func\n",
    "\n",
    "class IndRNN_Compute_GPU(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight_hh, h0):\n",
    "        length = x.size(0) if x.dim() == 3 else 1\n",
    "        batch = x.size(-2)\n",
    "        hidden_size = x.size(-1)  #hidden_size\n",
    "        ncols = batch*hidden_size\n",
    "        thread_per_block = min(512, ncols)\n",
    "        num_block = (ncols-1)//thread_per_block+1\n",
    "        \n",
    "        size = (length, batch, hidden_size) if x.dim() == 3 else (batch, hidden_size)\n",
    "        h = x.new(*size)\n",
    "\n",
    "        stream, fwd_func, _ = _get_indrnn_cuda(x.device)\n",
    "        FUNC = fwd_func\n",
    "        FUNC(args=[\n",
    "            x.contiguous().data_ptr(),\n",
    "            weight_hh.contiguous().data_ptr(),\n",
    "            h0.contiguous().data_ptr(),\n",
    "            length,\n",
    "            batch,\n",
    "            hidden_size,\n",
    "            h.contiguous().data_ptr()],\n",
    "            block = (thread_per_block,1,1), grid = (num_block,1,1),\n",
    "            stream=stream\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward(x, h, weight_hh, h0)#\n",
    "        return h\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_h):\n",
    "        x, h, weight_hh, h0 = ctx.saved_tensors\n",
    "        length = x.size(0) if x.dim() == 3 else 1\n",
    "        batch = x.size(-2)\n",
    "        hidden_size = x.size(-1)#self.hidden_size\n",
    "        ncols = batch*hidden_size\n",
    "        thread_per_block = min(512, ncols)\n",
    "        num_block = (ncols-1)//thread_per_block+1\n",
    "\n",
    "        grad_x = x.new(*x.size())\n",
    "        grad_weight_hh = x.new(hidden_size).zero_()\n",
    "        grad_h0 = x.new(batch, hidden_size)  \n",
    "\n",
    "        stream, _, bwd_func = _get_indrnn_cuda(x.device)\n",
    "        FUNC = bwd_func\n",
    "        FUNC(args=[\n",
    "            x.contiguous().data_ptr(),\n",
    "            weight_hh.contiguous().data_ptr(),\n",
    "            h0.contiguous().data_ptr(),\n",
    "            h.contiguous().data_ptr(),\n",
    "            grad_h.contiguous().data_ptr(),\n",
    "            length,\n",
    "            batch,\n",
    "            hidden_size,\n",
    "            grad_x.contiguous().data_ptr(),\n",
    "            grad_weight_hh.contiguous().data_ptr(),\n",
    "            grad_h0.contiguous().data_ptr()],\n",
    "            block = (thread_per_block,1,1), grid = (num_block,1,1),\n",
    "            stream=stream\n",
    "        )\n",
    "        return grad_x, grad_weight_hh, grad_h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class IndRNN_onlyrecurrent(nn.Module):\n",
    "    def __init__(self, hidden_size, recurrent_init=None):\n",
    "        super(IndRNN_onlyrecurrent, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.recurrent_init = recurrent_init\n",
    "        self.weight_hh = Parameter(torch.Tensor(hidden_size))        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for name, weight in self.named_parameters():\n",
    "            if \"weight_hh\" in name: nn.init.uniform_(weight, a=0, b=1)\n",
    "\n",
    "    def forward(self, input, h0=None):\n",
    "        assert input.dim() == 2 or input.dim() == 3        \n",
    "        if h0 is None:\n",
    "            h0 = input.data.new(input.size(-2),input.size(-1)).zero_()\n",
    "        elif (h0.size(-1)!=input.size(-1)) or (h0.size(-2)!=input.size(-2)):\n",
    "            raise RuntimeError(\n",
    "                'The initial hidden size must be equal to input_size. Expected {}, got {}'.format(\n",
    "                    h0.size(), input.size()))\n",
    "        IndRNN_Compute = IndRNN_Compute_GPU.apply\n",
    "        #h=IndRNN_Compute(input, self.weight_hh, h0)\n",
    "        return IndRNN_Compute(input, self.weight_hh, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Linear_overtime_module(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,bias=True):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x.contiguous().view(-1, self.input_size)\n",
    "        y = self.fc(y)\n",
    "        y = y.view(x.size()[0], x.size()[1], self.hidden_size)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# from seqdata.models.core import SeqLinear\n",
    "class IndRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1,batch_first=True):\n",
    "        super().__init__()        \n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "#         self.list_linear=nn.ModuleList([SeqLinear(input_size if i == 0 else hidden_size, hidden_size,hidden_layer=0) for i in range(num_layers)])\n",
    "        self.list_linear=nn.ModuleList([Linear_overtime_module(input_size if i == 0 else hidden_size, hidden_size) for i in range(num_layers)])\n",
    "        self.list_rnn = nn.ModuleList([IndRNN_onlyrecurrent(hidden_size) for i in range(num_layers)])     \n",
    "            \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight_hh' in name:\n",
    "                param.data.uniform_(0,0.99)            \n",
    "            if ('list_linear' in name) and 'weight' in name:#'denselayer' in name and \n",
    "                nn.init.kaiming_uniform_(param, a=8, mode='fan_in')#\n",
    "            if 'bias' in name:\n",
    "                param.data.fill_(0.0)\n",
    "    \n",
    "    def forward(self, inp, hid=None):\n",
    "#         import pdb;pdb.set_trace()\n",
    "        new_hid = []\n",
    "        if self.batch_first: inp = inp.transpose(0,1)\n",
    "        for i, (lin,rnn) in enumerate(zip(self.list_linear,self.list_rnn)):\n",
    "            inp = lin(inp)\n",
    "            inp = rnn(inp , None if hid is None else hid[i])\n",
    "            new_hid.append(inp[-1])\n",
    "            \n",
    "        if self.batch_first: inp = inp.transpose(0,1)\n",
    "        return inp, torch.stack(new_hid, 0)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class IndRNNRegularizer(Callback):\n",
    "    \"Callback that adds ||max(0,|U|-U_bound)|| to the loss, only useful for IndRNN\"\n",
    "    run_before=TrainEvalCallback\n",
    "    def __init__(self, clip_limit = 0.99):\n",
    "        super().__init__()\n",
    "        self.clip_limit = clip_limit\n",
    "        self.zero = None\n",
    "        \n",
    "    def after_loss(self):\n",
    "#         ||max(0,|U|-U_bound)||\n",
    "        if not self.training: return\n",
    "        if self.zero is None: self.zero = list(self.model.parameters())[0].new_tensor(0.)\n",
    "        \n",
    "        for m in self.model.modules():\n",
    "            if type(m) == IndRNN_onlyrecurrent:\n",
    "#                 import pdb;pdb.set_trace()\n",
    "                self.learn.loss += torch.max(self.zero,m.weight_hh.abs()-self.clip_limit).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleIndRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,output_size,num_layers=1,hidden_size=100,bn=False,seq_len=1000):\n",
    "        super().__init__()\n",
    "        self.rnn = IndRNN(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers)\n",
    "        self.final = SeqLinear(hidden_size,output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out,z = self.rnn(x)\n",
    "#         print(x.shape,z.shape,out.shape)\n",
    "        out = self.final(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SeqLinear' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-773524d575c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleIndRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlrn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#,opt_func=ranger)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# lrn.add_cb(WeightClipping(model.rnn.list_rnn))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlrn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_cb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndRNNRegularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# lrn.add_cb(GradientClipping())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-208c7b32fd64>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, output_size, num_layers, hidden_size, bn, seq_len)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeqLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SeqLinear' is not defined"
     ]
    }
   ],
   "source": [
    "model = SimpleIndRNN(2,1,6,hidden_size=100)\n",
    "lrn = Learner(db,model,loss_func=nn.MSELoss())#,opt_func=ranger)\n",
    "# lrn.add_cb(WeightClipping(model.rnn.list_rnn))\n",
    "lrn.add_cb(IndRNNRegularizer())\n",
    "# lrn.add_cb(GradientClipping())\n",
    "lrn.fit(1,lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleIndRNN(2,1,6,hidden_size=100,bn=True,seq_len=1000)\n",
    "lrn = Learner(db,model,loss_func=nn.MSELoss())#,opt_func=ranger)\n",
    "# lrn.add_cb(WeightClipping(model.rnn.list_rnn))\n",
    "lrn.add_cb(IndRNNRegularizer())\n",
    "# lrn.add_cb(GradientClipping())\n",
    "lrn.fit(1,lr=2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lrn.validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
