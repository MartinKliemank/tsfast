{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Pytorch Models for Sequential Data\n",
    "output-file: indrnn.html\n",
    "title: IndRNN\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.indrnn\n",
    "#| default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from seqdata.core import *\n",
    "from fastai.basics import *\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "#import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = DataBlock(blocks=(SequenceBlock.from_hdf(['current','voltage'],TensorSequencesInput,clm_shift=[0,-1]),\n",
    "                        SequenceBlock.from_hdf(['voltage'],TensorSequencesOutput,clm_shift=[-1])),\n",
    "                 get_items=CreateDict([DfHDFCreateWindows(win_sz=1000+1,stp_sz=50,clm='current')]),\n",
    "                 splitter=ApplyToDict(ParentSplitter()))\n",
    "db = seq.dataloaders(get_hdf_files('/mnt/data/Development/seqdata/test_data/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndRNN Cuda Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "IndRNN_CODE = \"\"\"\n",
    "extern \"C\" {\n",
    "\n",
    "    __forceinline__ __device__ float reluf(float x)\n",
    "    {\n",
    "        return (x > 0.f) ? x : 0.f;\n",
    "    }\n",
    "\n",
    "    __forceinline__ __device__ float calc_grad_activation(float x)\n",
    "    {\n",
    "        return (x > 0.f) ? 1.f : 0.f;\n",
    "    }\n",
    "\n",
    "    __global__ void indrnn_fwd( const float * __restrict__ x,\n",
    "                            const float * __restrict__ weight_hh, const float * __restrict__ h0,\n",
    "                            const int len, const int batch, const int hidden_size, \n",
    "                            float * __restrict__ h)\n",
    "    {\n",
    "        int ncols = batch*hidden_size;\n",
    "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "        if (col >= ncols) return;       \n",
    "        const float weight_hh_cur = *(weight_hh + (col%hidden_size));\n",
    "        float cur = *(h0 + col);\n",
    "        const float *xp = x+col;\n",
    "        float *hp = h+col;\n",
    "\n",
    "        for (int row = 0; row < len; ++row)\n",
    "        {\n",
    "            cur=reluf(cur*weight_hh_cur+(*xp));\n",
    "            *hp=cur;\n",
    "            xp += ncols;\n",
    "            hp += ncols;            \n",
    "        }\n",
    "    }\n",
    "\n",
    "    __global__ void indrnn_bwd(const float * __restrict__ x,\n",
    "                             const float * __restrict__ weight_hh, const float * __restrict__ h0,\n",
    "                             const float * __restrict__ h,\n",
    "                            const float * __restrict__ grad_h, \n",
    "                            const int len, const int batch, const int hidden_size, \n",
    "                            float * __restrict__ grad_x,\n",
    "                            float * __restrict__ grad_weight_hh, float * __restrict__ grad_h0)\n",
    "    {    \n",
    "        int ncols = batch*hidden_size;\n",
    "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "        if (col >= ncols) return;        \n",
    "        const float weight_hh_cur = *(weight_hh + (col%hidden_size));\n",
    "        float gweight_hh_cur = 0;\n",
    "        float cur = 0;  // *(grad_last + col);        //0; strange gradient behavior. grad_last and grad_h, one of them is zero.     \n",
    "        \n",
    "        const float *xp = x+col + (len-1)*ncols;\n",
    "        const float *hp = h+col + (len-1)*ncols;      \n",
    "        float *gxp = grad_x + col + (len-1)*ncols;\n",
    "        const float *ghp = grad_h + col + (len-1)*ncols;\n",
    "        \n",
    "\n",
    "        for (int row = len-1; row >= 0; --row)\n",
    "        {        \n",
    "            const float prev_h_val = (row>0) ? (*(hp-ncols)) : (*(h0+col));\n",
    "            //float h_val_beforeact = prev_h_val*weight_hh_cur+(*xp);\n",
    "            float gh_beforeact = ((*ghp) + cur)*calc_grad_activation(prev_h_val*weight_hh_cur+(*xp));\n",
    "            cur = gh_beforeact*weight_hh_cur;\n",
    "            gweight_hh_cur += gh_beforeact*prev_h_val;\n",
    "            *gxp = gh_beforeact;\n",
    "\n",
    "            xp -= ncols;\n",
    "            hp -= ncols;\n",
    "            gxp -= ncols;\n",
    "            ghp -= ncols;        \n",
    "        }\n",
    "\n",
    "        atomicAdd(grad_weight_hh + (col%hidden_size), gweight_hh_cur);\n",
    "        *(grad_h0 +col) = cur;\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import warnings\n",
    "\n",
    "\n",
    "try:\n",
    "    from cupy.cuda import function\n",
    "    from pynvrtc.compiler import Program\n",
    "except:\n",
    "    warnings.warn('Failed to import pynvrtc and cupy for indrnn model')\n",
    "\n",
    "def _get_indrnn_cuda(device):\n",
    "    if not hasattr(_get_indrnn_cuda, '_DEVICE2FUNC'):_get_indrnn_cuda._DEVICE2FUNC = {}\n",
    "    res = _get_indrnn_cuda._DEVICE2FUNC.get(device, None)\n",
    "    if res:\n",
    "        return res\n",
    "    else:\n",
    "        print ('IndRNN loaded for gpu {}'.format(device))\n",
    "        mod = function.Module()\n",
    "        mod.load(bytes(Program(IndRNN_CODE, 'indrnn_prog.cu').compile().encode()))\n",
    "        fwd_func = mod.get_function('indrnn_fwd')\n",
    "        bwd_func = mod.get_function('indrnn_bwd')\n",
    "\n",
    "        Stream = namedtuple('Stream', ['ptr'])\n",
    "        current_stream = Stream(ptr=torch.cuda.current_stream().cuda_stream)\n",
    "\n",
    "        _get_indrnn_cuda._DEVICE2FUNC[device] = (current_stream, fwd_func, bwd_func)\n",
    "        return current_stream, fwd_func, bwd_func\n",
    "\n",
    "class IndRNN_Compute_GPU(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight_hh, h0):\n",
    "        length = x.size(0) if x.dim() == 3 else 1\n",
    "        batch = x.size(-2)\n",
    "        hidden_size = x.size(-1)  #hidden_size\n",
    "        ncols = batch*hidden_size\n",
    "        thread_per_block = min(512, ncols)\n",
    "        num_block = (ncols-1)//thread_per_block+1\n",
    "        \n",
    "        size = (length, batch, hidden_size) if x.dim() == 3 else (batch, hidden_size)\n",
    "        h = x.new(*size)\n",
    "\n",
    "        stream, fwd_func, _ = _get_indrnn_cuda(x.device)\n",
    "        FUNC = fwd_func\n",
    "        FUNC(args=[\n",
    "            x.contiguous().data_ptr(),\n",
    "            weight_hh.contiguous().data_ptr(),\n",
    "            h0.contiguous().data_ptr(),\n",
    "            length,\n",
    "            batch,\n",
    "            hidden_size,\n",
    "            h.contiguous().data_ptr()],\n",
    "            block = (thread_per_block,1,1), grid = (num_block,1,1),\n",
    "            stream=stream\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward(x, h, weight_hh, h0)#\n",
    "        return h\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_h):\n",
    "        x, h, weight_hh, h0 = ctx.saved_tensors\n",
    "        length = x.size(0) if x.dim() == 3 else 1\n",
    "        batch = x.size(-2)\n",
    "        hidden_size = x.size(-1)#self.hidden_size\n",
    "        ncols = batch*hidden_size\n",
    "        thread_per_block = min(512, ncols)\n",
    "        num_block = (ncols-1)//thread_per_block+1\n",
    "\n",
    "        grad_x = x.new(*x.size())\n",
    "        grad_weight_hh = x.new(hidden_size).zero_()\n",
    "        grad_h0 = x.new(batch, hidden_size)  \n",
    "\n",
    "        stream, _, bwd_func = _get_indrnn_cuda(x.device)\n",
    "        FUNC = bwd_func\n",
    "        FUNC(args=[\n",
    "            x.contiguous().data_ptr(),\n",
    "            weight_hh.contiguous().data_ptr(),\n",
    "            h0.contiguous().data_ptr(),\n",
    "            h.contiguous().data_ptr(),\n",
    "            grad_h.contiguous().data_ptr(),\n",
    "            length,\n",
    "            batch,\n",
    "            hidden_size,\n",
    "            grad_x.contiguous().data_ptr(),\n",
    "            grad_weight_hh.contiguous().data_ptr(),\n",
    "            grad_h0.contiguous().data_ptr()],\n",
    "            block = (thread_per_block,1,1), grid = (num_block,1,1),\n",
    "            stream=stream\n",
    "        )\n",
    "        return grad_x, grad_weight_hh, grad_h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class IndRNN_onlyrecurrent(nn.Module):\n",
    "    def __init__(self, hidden_size, recurrent_init=None):\n",
    "        super(IndRNN_onlyrecurrent, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.recurrent_init = recurrent_init\n",
    "        self.weight_hh = Parameter(torch.Tensor(hidden_size))        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for name, weight in self.named_parameters():\n",
    "            if \"weight_hh\" in name: nn.init.uniform_(weight, a=0, b=1)\n",
    "\n",
    "    def forward(self, input, h0=None):\n",
    "        assert input.dim() == 2 or input.dim() == 3        \n",
    "        if h0 is None:\n",
    "            h0 = input.data.new(input.size(-2),input.size(-1)).zero_()\n",
    "        elif (h0.size(-1)!=input.size(-1)) or (h0.size(-2)!=input.size(-2)):\n",
    "            raise RuntimeError(\n",
    "                'The initial hidden size must be equal to input_size. Expected {}, got {}'.format(\n",
    "                    h0.size(), input.size()))\n",
    "        IndRNN_Compute = IndRNN_Compute_GPU.apply\n",
    "        #h=IndRNN_Compute(input, self.weight_hh, h0)\n",
    "        return IndRNN_Compute(input, self.weight_hh, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Linear_overtime_module(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,bias=True):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_size, hidden_size, bias=bias)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x.contiguous().view(-1, self.input_size)\n",
    "        y = self.fc(y)\n",
    "        y = y.view(x.size()[0], x.size()[1], self.hidden_size)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# from seqdata.models.core import SeqLinear\n",
    "class IndRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1,batch_first=True):\n",
    "        super().__init__()        \n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "#         self.list_linear=nn.ModuleList([SeqLinear(input_size if i == 0 else hidden_size, hidden_size,hidden_layer=0) for i in range(num_layers)])\n",
    "        self.list_linear=nn.ModuleList([Linear_overtime_module(input_size if i == 0 else hidden_size, hidden_size) for i in range(num_layers)])\n",
    "        self.list_rnn = nn.ModuleList([IndRNN_onlyrecurrent(hidden_size) for i in range(num_layers)])     \n",
    "            \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight_hh' in name:\n",
    "                param.data.uniform_(0,0.99)            \n",
    "            if ('list_linear' in name) and 'weight' in name:#'denselayer' in name and \n",
    "                nn.init.kaiming_uniform_(param, a=8, mode='fan_in')#\n",
    "            if 'bias' in name:\n",
    "                param.data.fill_(0.0)\n",
    "    \n",
    "    def forward(self, inp, hid=None):\n",
    "#         import pdb;pdb.set_trace()\n",
    "        new_hid = []\n",
    "        if self.batch_first: inp = inp.transpose(0,1)\n",
    "        for i, (lin,rnn) in enumerate(zip(self.list_linear,self.list_rnn)):\n",
    "            inp = lin(inp)\n",
    "            inp = rnn(inp , None if hid is None else hid[i])\n",
    "            new_hid.append(inp[-1])\n",
    "            \n",
    "        if self.batch_first: inp = inp.transpose(0,1)\n",
    "        return inp, torch.stack(new_hid, 0)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0.16634128987789154, 0.03484616428613663, '00:06']\n"
     ]
    }
   ],
   "source": [
    "class SimpleIndRNN(nn.Module):\n",
    "    \n",
    "    @delegates(IndRNN, keep=True)\n",
    "    def __init__(self,input_size,output_size,num_layers=1,hidden_size=100,linear_layers=0,**kwargs):\n",
    "        super().__init__()\n",
    "        self.rnn = IndRNN(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,**kwargs)\n",
    "        self.final = Linear_overtime_module(hidden_size,output_size)\n",
    "    def forward(self, x, h_init=None):\n",
    "        out,_ = self.rnn(x,h_init)\n",
    "        out = self.final(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = SimpleIndRNN(2,1,4)\n",
    "lrn = Learner(db,model,loss_func=nn.MSELoss()).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class IndRNNRegularizer(Callback):\n",
    "    \"Callback that adds ||max(0,|U|-U_bound)|| to the loss, only useful for IndRNN\"\n",
    "    run_before=TrainEvalCallback\n",
    "    def __init__(self, clip_limit = 0.99):\n",
    "        super().__init__()\n",
    "        self.clip_limit = clip_limit\n",
    "        self.zero = None\n",
    "        \n",
    "    def after_loss(self):\n",
    "#         ||max(0,|U|-U_bound)||\n",
    "        if not self.training: return\n",
    "        if self.zero is None: self.zero = list(self.model.parameters())[0].new_tensor(0.)\n",
    "        \n",
    "        for m in self.model.modules():\n",
    "            if type(m) == IndRNN_onlyrecurrent:\n",
    "#                 import pdb;pdb.set_trace()\n",
    "                self.learn.loss_grad += torch.max(self.zero,m.weight_hh.abs()-self.clip_limit).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_models.ipynb.\n",
      "Converted 01a_IndRNN.ipynb.\n",
      "Converted 02_learner.ipynb.\n",
      "Converted 03_dataloaders.ipynb.\n",
      "Converted 11_dualrnn.ipynb.\n",
      "Converted 12_TensorQuaternions.ipynb.\n",
      "Converted 13_HPOpt.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#| include: false\n",
    "from nbdev.export import *\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
