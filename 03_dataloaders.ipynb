{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataloaders\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%matplotlib notebook\n",
    "from fastai2.callback.progress import *\n",
    "from fastai2.callback.tracker import *\n",
    "from fastai2.callback.schedule import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from seqdata.core import *\n",
    "from seqdata.model import *\n",
    "from seqdata.learner import *\n",
    "from fastai2.basics import *\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataloaders\n",
    "> Pytorch Modules for Training Models for sequential data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncated Backpropagation Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tbptt dataloader needs to split the minibatches that are created in several smaller minibatches that will be returned sequentially before the next minibatch may be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates()\n",
    "class TbpttDl(TfmdDL):\n",
    "\n",
    "    def __init__(self, dataset, sub_seq_len=None,max_batches=None, seq_len = None ,shuffle=True,num_workers=0, **kwargs):\n",
    "        store_attr(self,'sub_seq_len,max_batches,seq_len')\n",
    "        super().__init__(dataset=dataset, shuffle=shuffle, num_workers=num_workers, **kwargs)\n",
    "\n",
    "        self.rnn_reset = sub_seq_len is None #always reset stateful rnns if there are no subsequences\n",
    "    @property\n",
    "    def n_sub_seq(self):\n",
    "        if self.seq_len is None: self.seq_len = self.do_item(0)[0].shape[0]\n",
    "        return math.ceil(self.seq_len / self.sub_seq_len)\n",
    "        \n",
    "    def __len__(self):\n",
    "        l = super().__len__()\n",
    "        if self.sub_seq_len is not None: l *= self.n_sub_seq\n",
    "        if self.max_batches is not None: l = min(l,self.max_batches)\n",
    "        return l\n",
    "    \n",
    "    def create_batches(self, samps):\n",
    "        yield from self._tbptt_generator(super().create_batches(samps))\n",
    "        \n",
    "    def _tbptt_generator(self,batch_iter):\n",
    "        '''generator function that splits batches in smaller windows and truncates batch count if max_batches is set'''\n",
    "        for idx,b in enumerate(batch_iter):\n",
    "            if self.sub_seq_len is None:\n",
    "                self.rnn_reset = True\n",
    "                if self.max_batches is not None and idx >= self.max_batches: return\n",
    "                yield b\n",
    "            else:\n",
    "                for i in range(self.n_sub_seq):\n",
    "                    if self.max_batches is not None and ((idx*self.n_sub_seq)+i) >= self.max_batches: return\n",
    "                    self.rnn_reset = i == 0\n",
    "                    #it is importan to retain the tuple type, or future transforms may now work\n",
    "                    trunc_b = tuple([retain_type(x[:,i*self.sub_seq_len:(i+1)*self.sub_seq_len],x) for x in b])\n",
    "                    yield trunc_b\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = DataBlock(blocks=(SequenceBlock.from_hdf(['current','voltage'],TensorSequencesInput,clm_shift=[-1,-1]),\n",
    "                        SequenceBlock.from_hdf(['voltage'],TensorSequencesOutput,clm_shift=[1])),\n",
    "                 get_items=CreateDict([DfHDFCreateWindows(win_sz=1000+1,stp_sz=1000,clm='current')]),\n",
    "                 splitter=ApplyToDict(ParentSplitter()))\n",
    "db = seq.dataloaders(get_hdf_files('test_data/'),dl_type=TbpttDl,sub_seq_len=10,max_batches=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 10, 2]), torch.Size([64, 1000, 2]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.train.one_batch()[0].shape,db.valid.one_batch()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_workers has to be 0. If there are parallel workers, the order of minibatches will be corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TBPTT_Reset_Callback\n",
    "The stateful model needs to reset its hidden state, when a new sequence begins. The callback reads the reset flag and acts accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TbpttResetCB(Callback):\n",
    "    \"`Callback` resets the rnn model with every new sequence for tbptt\"\n",
    "        \n",
    "    def begin_batch(self):\n",
    "        dl = self.learn.dls.train if self.training else self.learn.dls.valid\n",
    "#         if not self.training: import pdb; pdb.set_trace()\n",
    "        if hasattr(dl,'rnn_reset')and dl.rnn_reset and hasattr(self.model,'reset'):\n",
    "            self.model.reset()\n",
    "        \n",
    "    def after_fit(self): \n",
    "        if hasattr(self.model,'reset'): self.model.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai2.learner.Learner at 0x7f8778937668>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrn = RNNLearner(db,num_layers=1,rnn_type='gru',stateful=False,metrics=[SkipNLoss(fun_rmse,100)])\n",
    "lrn.add_cb(TbpttResetCB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>14.254913</td>\n",
       "      <td>14.269542</td>\n",
       "      <td>3.765729</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrn.fit_one_cycle(1,lr_max=3e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.train.max_batches = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.train.sub_seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.725590</td>\n",
       "      <td>0.018130</td>\n",
       "      <td>0.110914</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrn.fit_one_cycle(1,lr_max=3e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Sampling Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weighted sampling dataloader for nonuniforly distributed data. A factory method receives the base Dataloader class and returns the inherited weighted sampling dataloader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WeightedDL_Factory(cls):\n",
    "    assert issubclass(cls, TfmdDL)\n",
    "    \n",
    "    class WeightedDL(cls):\n",
    "        def __init__(self, dataset, wgts=None,shuffle=True, **kwargs):\n",
    "            super().__init__(dataset=dataset, shuffle=True, **kwargs)\n",
    "            wgts = array([1.]*len(dataset) if wgts is None else wgts)\n",
    "            self.wgts = wgts/wgts.sum()\n",
    "\n",
    "        def get_idxs(self):\n",
    "            if self.n==0: return []\n",
    "            if not self.shuffle: return super().get_idxs()\n",
    "            return list(np.random.choice(self.n, self.n, p=self.wgts))\n",
    "    return WeightedDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = WeightedDL_Factory(TfmdDL)([1,2]*5,bs=10,wgts=[2,1]*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 1, 1, 1, 1, 2, 1, 1, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_model.ipynb.\n",
      "Converted 02_learner.ipynb.\n",
      "Converted 03_dataloaders.ipynb.\n",
      "Converted 11_dualrnn.ipynb.\n",
      "Converted 12_TensorQuaternions.ipynb.\n",
      "Converted 13_HPOpt.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
