{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Pytorch Models for Sequential Data\n",
    "output-file: hpopt.html\n",
    "title: Hyperparameter Optimization Module\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tune\n",
    "#| default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tsfast.data import *\n",
    "from tsfast.models import *\n",
    "from tsfast.learner import *\n",
    "\n",
    "from fastai.basics import *\n",
    "from fastai.callback.core import Callback\n",
    "# from fastai.callback.schedule import *\n",
    "# from fastai.callback.rnn import *\n",
    "# from fastai.callback.tracker import *\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import *\n",
    "# from ray.tune.experiment.trial import ExportFormat\n",
    "# from ray import train\n",
    "# from ray.train import Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = get_config().config_file.parent\n",
    "f_path = project_root / 'test_data/WienerHammerstein'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf_files = L([f for f in get_hdf_files(f_path) if '_test.hdf5' not in str(f)])\n",
    "tfm_src = CreateDict([DfHDFCreateWindows(win_sz=400,stp_sz=100,clm='u')])\n",
    "dls = DataBlock(blocks=(SequenceBlock.from_hdf(['u'],TensorSequencesInput),\n",
    "                        SequenceBlock.from_hdf(['y'],TensorSequencesOutput)),\n",
    "                get_items=tfm_src,\n",
    "                splitter=ApplyToDict(FuncSplitter(lambda o: 'valid' in str(o)))).dataloaders(hdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need a log uniform distibution for variables with vast value ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def log_uniform(min_bound, max_bound, base=10):\n",
    "    '''uniform sampling in an exponential range'''\n",
    "    logmin = np.log(min_bound) / np.log(base)\n",
    "    logmax = np.log(max_bound) / np.log(base)\n",
    "    def _sample():\n",
    "        return base**(np.random.uniform(logmin, logmax))\n",
    "    return _sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0004982865057925154,\n",
       " 0.002890895219146445,\n",
       " 0.00029964853403334436,\n",
       " 4.350564451289386e-05,\n",
       " 9.960921143874997e-05]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[log_uniform(1e-8, 1e-2)() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LearnerTrainable(tune.Trainable):\n",
    "\n",
    "    def setup(self, config):\n",
    "        self.create_lrn = ray.get(config['create_lrn'])\n",
    "        self.dls = ray.get(config['dls'])\n",
    "\n",
    "        self.lrn = self.create_lrn(self.dls,config)\n",
    "\n",
    "    def step(self):\n",
    "        with self.lrn.no_bar(): self.lrn.fit(1)\n",
    "        train_loss,valid_loss,rmse = self.lrn.recorder.values[-1]\n",
    "        result = {'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss,\n",
    "                'mean_loss': rmse}\n",
    "        return result\n",
    "\n",
    "    def save_checkpoint(self, tmp_checkpoint_dir):\n",
    "        checkpoint_path = os.path.join(tmp_checkpoint_dir, \"model.pth\")\n",
    "        torch.save(self.lrn.model.state_dict(), checkpoint_path)\n",
    "        return tmp_checkpoint_dir\n",
    "\n",
    "    def load_checkpoint(self, tmp_checkpoint_dir):\n",
    "        checkpoint_path = os.path.join(tmp_checkpoint_dir, \"model.pth\")\n",
    "        self.lrn.model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "    def _export_model(self, export_formats, export_dir):\n",
    "        if export_formats == [ExportFormat.MODEL]:\n",
    "            path = os.path.join(export_dir, \"exported_model\")\n",
    "            torch.save(self.lrn.model.state_dict(), path)\n",
    "            return {ExportFormat.MODEL: path}\n",
    "        else:\n",
    "            raise ValueError(\"unexpected formats: \" + str(export_formats))\n",
    "\n",
    "    # the learner class will be recreated with every perturbation, saving the model\n",
    "    # that way the new hyperparameter will be applied\n",
    "    def reset_config(self, new_config):\n",
    "        self.lrn = self.create_lrn(self.dls,new_config)\n",
    "        self.config = new_config\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastai.callback.tracker import SaveModelCallback \n",
    "class CBRaySaveModel(SaveModelCallback):\n",
    "    \"A `TrackerCallback` that saves the model's best during training in a tune checkpoint directory\"\n",
    "    \n",
    "    def _save(self, name):\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            file = os.path.join(temp_checkpoint_dir,name+'.pth')\n",
    "            save_model(file, self.learn.model,opt=None)\n",
    "            self.last_saved_path = file\n",
    "            \n",
    "    #final checkpoint\n",
    "    def after_fit(self, **kwargs):\n",
    "        self._save(f'{self.fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from multiprocessing.managers import SharedMemoryManager\n",
    "def stop_shared_memory_managers(obj):\n",
    "    \"\"\"\n",
    "    Iteratively finds and stops all SharedMemoryManager instances contained within the provided object.\n",
    "    \"\"\"\n",
    "    visited = set()  # Track visited objects to avoid infinite loops\n",
    "    stack = [obj]  # Use a stack to manage objects to inspect\n",
    "\n",
    "    while stack:\n",
    "        current_obj = stack.pop()\n",
    "        obj_id = id(current_obj)\n",
    "\n",
    "        if obj_id in visited:\n",
    "            continue  # Skip already visited objects\n",
    "        visited.add(obj_id)\n",
    "\n",
    "        # Check if the current object is a SharedMemoryManager and stop it\n",
    "        if isinstance(current_obj, SharedMemoryManager):\n",
    "            current_obj.shutdown()\n",
    "            continue\n",
    "\n",
    "        # If it's a collection, add its items to the stack. Otherwise, add its attributes.\n",
    "        if isinstance(current_obj, dict):\n",
    "            stack.extend(current_obj.keys())\n",
    "            stack.extend(current_obj.values())\n",
    "        elif isinstance(current_obj, (list, set, tuple)):\n",
    "            stack.extend(current_obj)\n",
    "        elif hasattr(current_obj, '__dict__'):  # Check for custom objects with attributes\n",
    "            stack.extend(vars(current_obj).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import gc\n",
    "def learner_optimize(config):\n",
    "    try:\n",
    "        create_lrn = ray.get(config['create_lrn'])\n",
    "        dls = ray.get(config['dls'])\n",
    "        \n",
    "        #Scheduling Parameters for training the Model\n",
    "        lrn_kwargs = {'n_epoch':100,'pct_start':0.5}\n",
    "        for attr in ['n_epoch','pct_start']:\n",
    "            if attr in config: lrn_kwargs[attr] = config[attr]\n",
    "    \n",
    "        lrn = create_lrn(dls,config)\n",
    "        \n",
    "        # load checkpoint data if provided\n",
    "        checkpoint: train.Checkpoint = train.get_checkpoint()\n",
    "        if checkpoint:\n",
    "            with checkpoint.as_directory() as checkpoint_dir:\n",
    "                lrn.model.load_state_dict(torch.load(checkpoint_dir + 'model.pth'))\n",
    "        \n",
    "        lrn.lr = config['lr'] if 'lr' in config else 3e-3\n",
    "        lrn.add_cb(CBRayReporter() if 'reporter' not in config else ray.get(config['reporter'])())\n",
    "        # lrn.add_cb(CBRaySaveModel()) #the model saving now has to be done by the reporter callback\n",
    "        with lrn.no_bar(): \n",
    "            ray.get(config['fit_method'])(lrn,**lrn_kwargs)\n",
    "    finally:\n",
    "        #cleanup shared memory even when earlystopping occurs\n",
    "        stop_shared_memory_managers(lrn)\n",
    "        del lrn\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrainSpecificEpoch(Callback):\n",
    "    \"Skip training up to `epoch`\"\n",
    "    order = 70\n",
    "    \n",
    "    def __init__(self, epoch:int):\n",
    "        self._skip_to = epoch\n",
    "\n",
    "    def before_epoch(self):\n",
    "        print(self.epoch)\n",
    "        # if self.epoch < self._skip_to:\n",
    "        #     raise CancelEpochException\n",
    "        # if self.epoch > self._skip_to:\n",
    "        # raise CancelFitException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableModel(tune.Trainable):\n",
    "    def setup(self, config):\n",
    "        # Assuming create_lrn and dls are accessible here or passed in config\n",
    "        self.create_lrn = ray.get(config['create_lrn'])\n",
    "        self.dls = ray.get(config['dls'])\n",
    "        self.config = config\n",
    "\n",
    "        self.lrn = self.create_lrn(self.dls, config)\n",
    "\n",
    "        self.lrn.lr = config['lr'] if 'lr' in config else 3e-3\n",
    "        if 'wd' in config: self.lrn.wd = config['wd']\n",
    "        self._setup_callbacks()\n",
    "\n",
    "        if 'reporter' not in self.config:\n",
    "            self.lrn.add_cb(CBRayReporter())\n",
    "        else:\n",
    "            self.lrn.add_cb(ray.get(self.config['reporter'])())\n",
    "\n",
    "        if self.lrn.opt is None: self.lrn.create_opt()\n",
    "        self.lrn.opt.set_hyper('lr', self.lrn.lr)\n",
    "        lr = np.array([h['lr'] for h in self.lrn.opt.hypers])\n",
    "        pct_start = config['pct_start'] if 'pct_start' in config else 0.3\n",
    "        self.n_epoch = config['n_epoch'] if 'n_epoch' in config else 10\n",
    "        lr_scheds = {'lr': combined_cos(pct_start, lr, lr, lr/div_final)}\n",
    "        self.steps=0\n",
    "\n",
    "    def step(self):\n",
    "\n",
    "        self.fit(self.n_epoch, cbs=TrainSpecificEpoch(self.steps)+ParamScheduler(scheds)+L(cbs), wd=wd)\n",
    "        self.steps += 1\n",
    "\n",
    "        \n",
    "        scores = self.lrn.recorder.values[-1]\n",
    "        metrics = {\n",
    "            'train_loss': scores[0],\n",
    "            'valid_loss': scores[1]\n",
    "        }        \n",
    "        for metric,value in zip(self.learn.metrics,scores[2:]):\n",
    "            m_name = metric.name if hasattr(metric,'name') else str(metric)\n",
    "            metrics[m_name] = value\n",
    "        return metrics\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_dir):\n",
    "        file = os.path.join(temp_checkpoint_dir,'model.pth')\n",
    "        save_model(file, self.learn.model,opt=None) \n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        self.lrn.model.load_state_dict(torch.load(checkpoint_dir + 'model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableModel(tune.Trainable):\n",
    "    def setup(self, config):\n",
    "        # Assuming create_lrn and dls are accessible here or passed in config\n",
    "        self.create_lrn = ray.get(config['create_lrn'])\n",
    "        self.dls = ray.get(config['dls'])\n",
    "        self.config = config\n",
    "        self.lrn_kwargs = {'n_epoch': 100, 'pct_start': 0.5}\n",
    "\n",
    "        for attr in ['n_epoch', 'pct_start']:\n",
    "            if attr in config:\n",
    "                self.lrn_kwargs[attr] = config[attr]\n",
    "\n",
    "        self.lrn = self.create_lrn(self.dls, config)\n",
    "        self.lrn.lr = config['lr'] if 'lr' in config else 3e-3\n",
    "\n",
    "\n",
    "    def step(self):\n",
    "        print(self.iteration)\n",
    "        # fit_kwargs = {**self.lrn_kwargs,**{'cbs':TrainSpecificEpoch(self.iteration)}}\n",
    "        # fit_kwargs = {**self.lrn_kwargs,**{'cbs':SkipToEpoch(self.iteration)}}\n",
    "        # fit_kwargs = self.lrn_kwargs\n",
    "        with self.lrn.no_bar(): \n",
    "            # ray.get(self.config['fit_method'])(self.lrn,**fit_kwargs)\n",
    "            # self.lrn.fit_flat_cos(**fit_kwargs)\n",
    "            self.lrn.fit_flat_cos(self.lrn_kwargs['n_epoch'],cbs=TrainSpecificEpoch(self.iteration))\n",
    "\n",
    "        \n",
    "        metrics = {\n",
    "            'train_loss': 1,#scores[0],\n",
    "            'valid_loss': 1,#scores[1],\n",
    "             tune.result.DONE: self.iteration >= self.lrn_kwargs['n_epoch']-1\n",
    "        }  \n",
    "        \n",
    "        # scores = self.lrn.recorder.values[-1]\n",
    "        # metrics = {\n",
    "        #     'train_loss': scores[0],\n",
    "        #     'valid_loss': scores[1],\n",
    "        #      tune.result.DONE: self.epoch_iter >= self.lrn_kwargs['n_epoch']\n",
    "        # }        \n",
    "        # for metric,value in zip(self.lrn.metrics,scores[2:]):\n",
    "        #     m_name = metric.name if hasattr(metric,'name') else str(metric)\n",
    "        #     metrics[m_name] = value\n",
    "        return metrics\n",
    "\n",
    "    def save_checkpoint(self, checkpoint_dir):\n",
    "        file = os.path.join(temp_checkpoint_dir,'model.pth')\n",
    "        save_model(file, self.learn.model,opt=None) \n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        self.lrn.model.load_state_dict(torch.load(checkpoint_dir + 'model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataBlock(blocks=(SequenceBlock.from_hdf(['u'],TensorSequencesInput),\n",
    "                    SequenceBlock.from_hdf(['y'],TensorSequencesOutput)),\n",
    "            get_items=tfm_src,\n",
    "            splitter=ApplyToDict(FuncSplitter(lambda o: 'valid' in str(o)))).dataloaders(hdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mutation config dictionary consists of functions that sample from a distribution. In order to retrieve a dictionary with one realisation we need the function sample_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample_config(config):\n",
    "    ret_conf = config.copy()\n",
    "    for k in ret_conf:\n",
    "        ret_conf[k]=ret_conf[k]()\n",
    "    return ret_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CBRayReporter(Callback):\n",
    "    \"`Callback` reports progress after every epoch to the ray tune logger\"\n",
    "    \n",
    "    order=70 #order has to be >50, to be executed after the recorder callback\n",
    "\n",
    "    def after_epoch(self):\n",
    "        # train_loss,valid_loss,rmse = self.learn.recorder.values[-1]\n",
    "        # metrics = {\n",
    "        #     'train_loss': train_loss,\n",
    "        #     'valid_loss': valid_loss,\n",
    "        #     'mean_loss': rmse,\n",
    "        # }\n",
    "        scores = self.learn.recorder.values[-1]\n",
    "        metrics = {\n",
    "            'train_loss': scores[0],\n",
    "            'valid_loss': scores[1]\n",
    "        }\n",
    "        for metric,value in zip(self.learn.metrics,scores[2:]):\n",
    "            m_name = metric.name if hasattr(metric,'name') else str(metric)\n",
    "            metrics[m_name] = value\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            file = os.path.join(temp_checkpoint_dir,'model.pth')\n",
    "            #the model has to be saved to the checkpoint directory on creation\n",
    "            #that is why a seperate callback for model saving is not trivial\n",
    "            save_model(file, self.learn.model,opt=None) \n",
    "            ray.train.report(metrics, checkpoint=Checkpoint.from_directory(temp_checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HPOptimizer():\n",
    "    def __init__(self,create_lrn,dls):\n",
    "        self.create_lrn = create_lrn\n",
    "        self.dls = dls\n",
    "        self.analysis = None\n",
    "    \n",
    "    @delegates(ray.init)\n",
    "    def start_ray(self,**kwargs):\n",
    "        ray.shutdown()\n",
    "        ray.init(**kwargs)\n",
    "        \n",
    "    def stop_ray(self):\n",
    "        ray.shutdown()\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    @delegates(tune.run, keep=True)\n",
    "    def optimize(self,config,optimize_func=learner_optimize,resources_per_trial={\"gpu\": 1.0},verbose=1,**kwargs):\n",
    "        config['create_lrn'] = ray.put(self.create_lrn)\n",
    "        #dls are large objects, letting ray handle the copying process makes it much faster\n",
    "        config['dls'] = ray.put(self.dls) \n",
    "        if 'fit_method' not in config: config['fit_method'] = ray.put(Learner.fit_flat_cos)\n",
    "\n",
    "        kwargs.setdefault('keep_checkpoints_num', 1)#keep only the last checkpoint\n",
    "\n",
    "        self.analysis = tune.run(\n",
    "            optimize_func,\n",
    "            config=config,\n",
    "            resources_per_trial=resources_per_trial,\n",
    "            verbose=verbose,\n",
    "            **kwargs)\n",
    "        return self.analysis\n",
    "        \n",
    "    @delegates(tune.run, keep=True)\n",
    "    def optimize_pbt(self,opt_name,num_samples,config,mut_conf,perturbation_interval=2,\n",
    "                 stop={\"training_iteration\": 40 },\n",
    "                 resources_per_trial={\"gpu\": 1 },\n",
    "                 resample_probability=0.25,\n",
    "                 quantile_fraction=0.25,\n",
    "                 **kwargs):\n",
    "        self.mut_conf = mut_conf\n",
    "        \n",
    "        config['create_lrn'] = ray.put(self.create_lrn)\n",
    "        #dls are large objects, letting ray handle the copying process makes it much faster\n",
    "        config['dls'] = ray.put(self.dls) \n",
    "        \n",
    "        kwargs.setdefault('keep_checkpoints_num', 2)\n",
    "        \n",
    "\n",
    "        \n",
    "        scheduler = PopulationBasedTraining(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"mean_loss\",\n",
    "        mode=\"min\",\n",
    "        perturbation_interval=perturbation_interval,\n",
    "        resample_probability=resample_probability,\n",
    "        quantile_fraction=quantile_fraction,\n",
    "        hyperparam_mutations=mut_conf)\n",
    "        \n",
    "        self.analysis = tune.run(\n",
    "            LearnerTrainable,\n",
    "            name=opt_name,\n",
    "            scheduler=scheduler,\n",
    "            reuse_actors=True,\n",
    "            verbose=1,\n",
    "            stop=stop,\n",
    "            checkpoint_score_attr=\"mean_loss\",\n",
    "            num_samples=num_samples,\n",
    "            resources_per_trial=resources_per_trial,\n",
    "            config=config,\n",
    "            **kwargs)\n",
    "        return self.analysis\n",
    "    \n",
    "    def best_model(self):\n",
    "        if self.analysis is None: raise Exception\n",
    "        model = self.create_lrn(self.dls,sample_config(self.mut_conf)).model\n",
    "        f_path = ray.get(self.analysis.get_best_trial('mean_loss',mode='min').checkpoint.value)\n",
    "        model.load_state_dict(torch.load(f_path))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Population Based Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lrn(dls,config):\n",
    "    lr = config['lr']\n",
    "    alpha = config['alpha']\n",
    "    beta = config['beta']\n",
    "    weight_p = config['weight_p']\n",
    "    \n",
    "    lrn = RNNLearner(dls)\n",
    "    lrn.lr = lr\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-06-11 21:33:44</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:09.03        </td></tr>\n",
       "<tr><td>Memory:      </td><td>193.8/251.7 GiB    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      PopulationBasedTraining: 0 checkpoints, 0 perturbs<br>Logical resource usage: 0/112 CPUs, 0.5/4 GPUs (0.0/1.0 accelerator_type:RTX)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc                   </th><th style=\"text-align: right;\">      alpha</th><th style=\"text-align: right;\">       beta</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  weight_p</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  valid_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LearnerTrainable_7ea8a_00000</td><td>TERMINATED</td><td>141.23.125.123:1771411</td><td style=\"text-align: right;\">0.56202    </td><td style=\"text-align: right;\">5.66608e-05</td><td style=\"text-align: right;\">0.00119934 </td><td style=\"text-align: right;\"> 0.138491 </td><td style=\"text-align: right;\">0.25008 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.55589</td><td style=\"text-align: right;\">   0.0582699</td><td style=\"text-align: right;\">   0.0626094</td></tr>\n",
       "<tr><td>LearnerTrainable_7ea8a_00001</td><td>TERMINATED</td><td>141.23.125.123:1771412</td><td style=\"text-align: right;\">0.00415602 </td><td style=\"text-align: right;\">0.00034347 </td><td style=\"text-align: right;\">0.000367016</td><td style=\"text-align: right;\"> 0.0511449</td><td style=\"text-align: right;\">0.247538</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.99096</td><td style=\"text-align: right;\">   0.0560226</td><td style=\"text-align: right;\">   0.061328 </td></tr>\n",
       "<tr><td>LearnerTrainable_7ea8a_00002</td><td>TERMINATED</td><td>141.23.125.123:1771413</td><td style=\"text-align: right;\">0.0807741  </td><td style=\"text-align: right;\">4.65182e-05</td><td style=\"text-align: right;\">0.0011368  </td><td style=\"text-align: right;\"> 0.301749 </td><td style=\"text-align: right;\">0.251558</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.52386</td><td style=\"text-align: right;\">   0.0593689</td><td style=\"text-align: right;\">   0.0633444</td></tr>\n",
       "<tr><td>LearnerTrainable_7ea8a_00003</td><td>TERMINATED</td><td>141.23.125.123:1771414</td><td style=\"text-align: right;\">1.16104e-05</td><td style=\"text-align: right;\">0.000607968</td><td style=\"text-align: right;\">0.0033143  </td><td style=\"text-align: right;\"> 0.126769 </td><td style=\"text-align: right;\">0.247707</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.91868</td><td style=\"text-align: right;\">   0.0585945</td><td style=\"text-align: right;\">   0.061402 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pheenix/miniconda3/envs/env_tsfast/lib/python3.11/site-packages/ray/tune/schedulers/pbt.py:480: UserWarning: Using `CheckpointConfig.num_to_keep <= 2` with PBT can lead to restoration problems when checkpoint are deleted too early for other trials to exploit them. If this happens, increase the value of `num_to_keep`.\n",
      "  warnings.warn(\n",
      "\u001b[36m(LearnerTrainable pid=1771412)\u001b[0m /home/pheenix/miniconda3/envs/env_tsfast/lib/python3.11/site-packages/fastai/torch_core.py:137: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "\u001b[36m(LearnerTrainable pid=1771412)\u001b[0m   t = torch.as_tensor(x, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(LearnerTrainable pid=1771412)\u001b[0m [0, 0.05602257698774338, 0.06132800504565239, 0.24753788113594055, '00:01']\n",
      "\u001b[36m(LearnerTrainable pid=1771414)\u001b[0m [0, 0.05859454348683357, 0.06140201538801193, 0.2477065473794937, '00:01']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 21:33:44,602\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/pheenix/ray_results/pbt_test' in 0.0083s.\n",
      "2024-06-11 21:33:44,609\tINFO tune.py:1041 -- Total run time: 9.07 seconds (9.03 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\n",
    "            \"lr\": tune.loguniform(1e-2, 1e-4),\n",
    "            \"alpha\": tune.loguniform(1e-5, 10),\n",
    "            \"beta\": tune.loguniform(1e-5, 10),\n",
    "            \"weight_p\": tune.uniform(0, 0.5)}\n",
    "mut_conf = {# distribution for resampling\n",
    "            \"lr\": log_uniform(1e-8, 1e-2),\n",
    "            \"alpha\": log_uniform(1e-5, 10),\n",
    "            \"beta\": log_uniform(1e-5, 10),\n",
    "            \"weight_p\": lambda: np.random.uniform(0, 0.5)}\n",
    "\n",
    "hp_opt = HPOptimizer(create_lrn,dls)\n",
    "hp_opt.start_ray()\n",
    "hp_opt.optimize_pbt('pbt_test',4,config,mut_conf,perturbation_interval=1,\n",
    "                 stop={\"training_iteration\": 1 },\n",
    "                 resources_per_trial={\"gpu\": 0.5},\n",
    "                 storage_path=str(Path.home() / 'ray_results'))#no cpu count is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hp_opt.best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lrn(dls,config):\n",
    "    dls = DataBlock(blocks=(SequenceBlock.from_hdf(['u'],TensorSequencesInput),\n",
    "                        SequenceBlock.from_hdf(['y'],TensorSequencesOutput)),\n",
    "                get_items=tfm_src,\n",
    "                splitter=ApplyToDict(FuncSplitter(lambda o: 'valid' in str(o)))).dataloaders(hdf_files)\n",
    "    lrn = RNNLearner(dls,hidden_size=config['hidden_size'],metrics=[fun_rmse,mse])\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_opt = HPOptimizer(create_lrn,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"hidden_size\": tune.grid_search([10,20,50,100]),\n",
    "    'n_epoch':10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(LearnerTrainable pid=1771413)\u001b[0m [0, 0.05936885625123978, 0.06334440410137177, 0.25155845284461975, '00:02']\n",
      "\u001b[36m(LearnerTrainable pid=1771411)\u001b[0m [0, 0.05826989933848381, 0.06260943412780762, 0.25007978081703186, '00:02']\n"
     ]
    }
   ],
   "source": [
    "# hp_opt.optimize(optimize_func=TrainableModel,\n",
    "#                 resources_per_trial={\"gpu\": 4},\n",
    "#                 config=search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_best_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhp_opt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_config\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_best_config'"
     ]
    }
   ],
   "source": [
    "# hp_opt.analysis.get_best_config('mean_loss',mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
