{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp learner\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from seqdata.core import *\n",
    "from seqdata.models.core import *\n",
    "from fastai.basics import *\n",
    "from fastai.callback.progress import *\n",
    "from fastai.callback.tracker import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner\n",
    "> Pytorch Modules for Training Models for sequential data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = DataBlock(blocks=(SequenceBlock.from_hdf(['current','voltage'],TensorSequencesInput,clm_shift=[-1,-1]),\n",
    "                        SequenceBlock.from_hdf(['voltage'],TensorSequencesOutput,clm_shift=[1])),\n",
    "                 get_items=CreateDict([DfHDFCreateWindows(win_sz=1000+1,stp_sz=1000,clm='current')]),\n",
    "                 splitter=ApplyToDict(ParentSplitter()))\n",
    "db = seq.dataloaders(get_hdf_files('test_data/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>13.732953</td>\n",
       "      <td>12.560694</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SimpleRNN(2,1)\n",
    "lrn = Learner(db,model,loss_func=nn.MSELoss()).fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GradientClipping(Callback):\n",
    "    \"`Callback` cutts of the gradient of every minibtch at `clip_val`\"\n",
    "    def __init__(self, clip_val=10): self.clip_val = clip_val\n",
    "\n",
    "    def after_backward(self):\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.284286</td>\n",
       "      <td>0.149260</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),cbs=GradientClipping(10)).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class WeightClipping(Callback):\n",
    "    \"`Callback` that clips the weights of a given module at `clip_limit` after every iteration\"\n",
    "    def __init__(self, module, clip_limit = 1):\n",
    "        self.module = module\n",
    "        self.clip_limit = clip_limit\n",
    "\n",
    "    def after_batch(self):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        for p in self.module.parameters():\n",
    "            p.data.clamp_(-self.clip_limit,self.clip_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.166133</td>\n",
       "      <td>0.152843</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),cbs=WeightClipping(model,clip_limit=1)).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SkipFirstNCallback(Callback):\n",
    "    \"`Callback` skips first n samples from prediction and target, optionally `with_loss`\"\n",
    "    def __init__(self, n_skip = 0):\n",
    "        self.n_skip = n_skip\n",
    "\n",
    "    def after_pred(self):\n",
    "        self.learn.pred = self.pred[:,self.n_skip:]\n",
    "#         import pdb; pdb.set_trace()\n",
    "        if isinstance(self.yb, tuple):\n",
    "            self.learn.yb = tuple([y[:,self.n_skip:] for y in self.yb])\n",
    "        else:\n",
    "            self.learn.yb = self.yb[:,self.n_skip:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VarySeqLen(Callback):\n",
    "    \"`Callback` varies sequence length of every mini batch\"\n",
    "    def __init__(self, min_len = 50):\n",
    "        self.min_len = min_len\n",
    "\n",
    "    def begin_batch(self):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        lx = self.xb[0].shape[1]\n",
    "        ly = self.yb[0].shape[1]\n",
    "        lim = random.randint(self.min_len,ly)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        if ly < lx:\n",
    "            self.learn.xb = tuple([x[:,:-(ly-lim)] for x in self.xb])\n",
    "        else:\n",
    "            self.learn.xb = tuple([x[:,:lim] for x in self.xb])\n",
    "            \n",
    "        self.learn.yb = tuple([y[:,:lim] for y in self.yb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.122882</td>\n",
       "      <td>0.092171</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),cbs=VarySeqLen(10)).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.callback.hook import *\n",
    "@delegates()\n",
    "class TimeSeriesRegularizer(HookCallback):\n",
    "    \"Callback that adds AR and TAR to the loss, calculated by output of provided layer\"\n",
    "    run_before=TrainEvalCallback\n",
    "    def __init__(self,alpha=0.0, beta=0.0,dim = None,detach=False, **kwargs):\n",
    "        super().__init__(detach=detach,**kwargs)\n",
    "        store_attr('alpha,beta,dim')\n",
    "        \n",
    "    def hook(self, m, i, o): \n",
    "#         import pdb; pdb.set_trace()\n",
    "        if type(o) is torch.Tensor:\n",
    "            self.out = o\n",
    "        else:\n",
    "            self.out = o[0]\n",
    "        \n",
    "        #find time axis if not already provided\n",
    "        if self.dim is None:\n",
    "            self.dim = np.argmax([0,self.out.shape[1],self.out.shape[2]])\n",
    "    \n",
    "    def after_loss(self):\n",
    "        if not self.training: return\n",
    "        \n",
    "        h = self.out.float()\n",
    "        \n",
    "        if self.alpha != 0.:  \n",
    "            l_a = float(self.alpha) * h.pow(2).mean()\n",
    "            self.learn.loss = self.learn.loss+l_a \n",
    "            \n",
    "        if self.beta != 0. and h.shape[self.dim]>1:\n",
    "            h_diff = (h[:,1:] - h[:,:-1]) if self.dim == 1 else (h[:,:,1:] - h[:,:,:-1])\n",
    "            l_b = float(self.beta) * h_diff.pow(2).mean()\n",
    "            self.learn.loss = self.learn.loss+l_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ARInitCB(Callback):\n",
    "    '''Adds the target variable to the input tuple for autoregression'''\n",
    "    def begin_batch(self):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        self.learn.xb = tuple([*self.xb,*self.yb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.117019</td>\n",
       "      <td>0.103256</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss()).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from matplotlib.lines import Line2D\n",
    "def plot_grad_flow(named_parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    *modified version of https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063/8*\n",
    "    \n",
    "    Call multiple time for transparent overlays, representing the mean gradients\n",
    "    '''\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "#             pdb.set_trace()\n",
    "            ave_grads.append(0 if p.grad is None else p.grad.abs().mean())\n",
    "            max_grads.append(0 if p.grad is None else p.grad.abs().max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"Gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class CB_PlotGradient(Callback):\n",
    "    '''Plot the Gradient Distribution for every trainable parameter'''\n",
    "    \n",
    "    def __init__(self, n_draws=20): self.n_draws = n_draws\n",
    "    \n",
    "    def begin_fit(self):\n",
    "        '''Create a new figure to plot in'''\n",
    "        plt.figure()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    def after_backward(self):\n",
    "        '''plot the gradient for every layer of the current minibatch'''\n",
    "        # plotting n_draws times at the whole training\n",
    "        if self.iter % (max(self.n_epoch*self.n_iter//self.n_draws,1)) == 0:\n",
    "#         if self.iter == self.n_iter-1:\n",
    "            plot_grad_flow(self.learn.model.named_parameters())\n",
    "#             print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.088547</td>\n",
       "      <td>0.078645</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),cbs=CB_PlotGradient()).fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import functools\n",
    "\n",
    "def ignore_nan(func):\n",
    "    '''remove nan values from tensors before function execution, reduces tensor to a flat array, apply to functions such as mse'''\n",
    "    @functools.wraps(func)\n",
    "    def ignore_nan_decorator(*args, **kwargs):\n",
    "#         mask = ~torch.isnan(args[-1]) #nan mask of target tensor\n",
    "#         args = tuple([x[mask] for x in args]) #remove nan values\n",
    "        mask = ~torch.isnan(args[-1][...,-1]) #nan mask of target tensor\n",
    "        args = tuple([x[mask,:] for x in args]) #remove nan values\n",
    "        return func(*args, **kwargs)\n",
    "    return ignore_nan_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "y_t = torch.ones(32,n,6)\n",
    "y_t[:,20]=np.nan\n",
    "y_p = torch.ones(32,n,6)*1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1000, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~torch.isnan(y_t)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1000, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.isnan(mse(y_p,y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "mse_nan = ignore_nan(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(mse_nan(y_p,y_t),0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import functools\n",
    "\n",
    "def float64_func(func):\n",
    "    '''calculate function internally with float64 and convert the result back'''\n",
    "    @functools.wraps(func)\n",
    "    def float64_func_decorator(*args, **kwargs):\n",
    "        typ = args[0].dtype\n",
    "        args = tuple([x.double() if issubclass(type(x),Tensor ) else x for x in args]) #remove nan values\n",
    "        return func(*args, **kwargs).type(typ)\n",
    "    return float64_func_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.060234</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Float but expected Double\nException raised from compute_types at /pytorch/aten/src/ATen/native/TensorIterator.cpp:183 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7fd04958d1e2 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: at::TensorIterator::compute_types(at::TensorIteratorConfig const&) + 0x259 (0x7fd034cc1849 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #2: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x6b (0x7fd034cc4feb in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #3: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7fd034cc565d in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #4: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x18a (0x7fd034b2a2ba in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: <unknown function> + 0xf2b190 (0x7fcffa16a190 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x90 (0x7fd034b26ce0 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0xf2b230 (0x7fcffa16a230 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #8: <unknown function> + 0xf4d4b6 (0x7fcffa18c4b6 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #9: at::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x119 (0x7fd034fe9949 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0x2e03469 (0x7fd036c42469 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0xa9ac76 (0x7fd0348d9c76 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: at::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x119 (0x7fd034fe9949 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1af (0x7fd036b7e0cf in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: <unknown function> + 0x3375bb7 (0x7fd0371b4bb7 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7fd0371b0400 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7fd0371b0fa1 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7fd0371a9119 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7fd04a33fdea in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #19: <unknown function> + 0xc819d (0x7fd06503d19d in /home/daniel/miniconda3/envs/fastaiv2/bin/../lib/libstdc++.so.6)\nframe #20: <unknown function> + 0x76db (0x7fd06885e6db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #21: clone + 0x3f (0x7fd068587a3f in /lib/x86_64-linux-gnu/libc.so.6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-6b30b6a11beb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat64_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastcore/logargs.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0minit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'init_args'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minst\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mto_return\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_hypers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelFitException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end_cleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_end_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;34m;\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m;\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mlog_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cbs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;34m;\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m;\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelTrainException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;34m;\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m;\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_one_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelBatchException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;34m;\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m;\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before_backward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_backward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Float but expected Double\nException raised from compute_types at /pytorch/aten/src/ATen/native/TensorIterator.cpp:183 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7fd04958d1e2 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: at::TensorIterator::compute_types(at::TensorIteratorConfig const&) + 0x259 (0x7fd034cc1849 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #2: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x6b (0x7fd034cc4feb in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #3: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7fd034cc565d in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #4: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x18a (0x7fd034b2a2ba in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: <unknown function> + 0xf2b190 (0x7fcffa16a190 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x90 (0x7fd034b26ce0 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0xf2b230 (0x7fcffa16a230 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #8: <unknown function> + 0xf4d4b6 (0x7fcffa18c4b6 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #9: at::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x119 (0x7fd034fe9949 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0x2e03469 (0x7fd036c42469 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0xa9ac76 (0x7fd0348d9c76 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: at::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x119 (0x7fd034fe9949 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1af (0x7fd036b7e0cf in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: <unknown function> + 0x3375bb7 (0x7fd0371b4bb7 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7fd0371b0400 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7fd0371b0fa1 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7fd0371a9119 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7fd04a33fdea in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #19: <unknown function> + 0xc819d (0x7fd06503d19d in /home/daniel/miniconda3/envs/fastaiv2/bin/../lib/libstdc++.so.6)\nframe #20: <unknown function> + 0x76db (0x7fd06885e6db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #21: clone + 0x3f (0x7fd068587a3f in /lib/x86_64-linux-gnu/libc.so.6)\n"
     ]
    }
   ],
   "source": [
    "Learner(db,model,loss_func=float64_func(nn.MSELoss())).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SkipNLoss(fn,n_skip=0):\n",
    "    '''Loss-Function modifier that skips the first n samples of sequential data'''\n",
    "    @functools.wraps(fn)\n",
    "    def _inner( input, target):\n",
    "        return fn(input[:,n_skip:],target[:,n_skip:])\n",
    "    \n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.056675</td>\n",
       "      <td>0.036849</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=SkipNLoss(nn.MSELoss(),n_skip=30)).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fun_rmse(inp, targ): \n",
    "    '''rmse loss function defined as a function not as a AccumMetric'''\n",
    "    return torch.sqrt(F.mse_loss(inp, targ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.077923</td>\n",
       "      <td>0.073593</td>\n",
       "      <td>0.199043</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),metrics=SkipNLoss(fun_rmse,n_skip=30)).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def norm_rmse(inp, targ):\n",
    "    '''rmse loss function defined as a function not as a AccumMetric'''\n",
    "    return fun_rmse(inp, targ)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>norm_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.070596</td>\n",
       "      <td>0.054236</td>\n",
       "      <td>15.523600</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),metrics=SkipNLoss(norm_rmse,n_skip=30)).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mean_vaf(inp,targ):\n",
    "    return (1-((targ-inp).var()/targ.var()))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_vaf</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.066412</td>\n",
       "      <td>0.061802</td>\n",
       "      <td>95.607208</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),metrics=SkipNLoss(mean_vaf,n_skip=30)).fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Learner Models\n",
    "Create Learner with different kinds of models with fitting Parameters and regularizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_inp_out_size(db):\n",
    "    '''returns input and output size of a timeseries databunch'''\n",
    "    tup = db.one_batch()\n",
    "    inp = tup[0].shape[-1]\n",
    "    out = tup[1].shape[-1]\n",
    "    return inp,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(get_inp_out_size(db),(2,1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Learner\n",
    "The Learners include model specific optimizations. Removing the first n_skip samples of the loss function of transient time, greatly improves training stability. In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(SimpleRNN, keep=True)\n",
    "def RNNLearner(db,loss_func=nn.MSELoss(),metrics=[fun_rmse],n_skip=0,cbs=None,**kwargs):\n",
    "    inp,out = get_inp_out_size(db)\n",
    "    model = SimpleRNN(inp,out,**kwargs)\n",
    "  \n",
    "    skip = partial(SkipNLoss,n_skip=n_skip)\n",
    "        \n",
    "    metrics= [skip(f) for f in metrics]\n",
    "    loss_func = skip(loss_func)\n",
    "        \n",
    "    lrn = Learner(db,model,loss_func=loss_func,opt_func=ranger,metrics=metrics,cbs=cbs)\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>13.791986</td>\n",
       "      <td>13.874659</td>\n",
       "      <td>3.719144</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RNNLearner(db,rnn_type='indrnn').fit(1,1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN Learner\n",
    "Performs better on multi input data. Higher beta values allow a way smoother prediction. Way faster then RNNs in prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(TCN, keep=True)\n",
    "def TCNLearner(db,hl_depth=3,loss_func=nn.MSELoss(),metrics=[fun_rmse],n_skip=0,cbs=None,**kwargs):\n",
    "    inp,out = get_inp_out_size(db)\n",
    "    n_skip = 2**hl_depth if n_skip is None else n_skip\n",
    "    model = TCN(inp,out,hl_depth,**kwargs)\n",
    "  \n",
    "    skip = partial(SkipNLoss,n_skip=n_skip)\n",
    "        \n",
    "    metrics= [skip(f) for f in metrics]\n",
    "    loss_func = skip(loss_func)\n",
    "        \n",
    "    lrn = Learner(db,model,loss_func=loss_func,opt_func=ranger,metrics=metrics,cbs=cbs)\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>13.440915</td>\n",
       "      <td>13.361773</td>\n",
       "      <td>3.654490</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TCNLearner(db).fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRNN Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(CRNN, keep=True)\n",
    "def CRNNLearner(db,loss_func=nn.MSELoss(),metrics=[fun_rmse],n_skip=0,cbs=None,**kwargs):\n",
    "    inp,out = get_inp_out_size(db)\n",
    "    model = CRNN(inp,out,**kwargs)\n",
    "  \n",
    "    skip = partial(SkipNLoss,n_skip=n_skip)\n",
    "        \n",
    "    metrics= [skip(f) for f in metrics]\n",
    "    loss_func = skip(loss_func)\n",
    "        \n",
    "    lrn = Learner(db,model,loss_func=loss_func,opt_func=ranger,metrics=metrics,cbs=cbs)\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8.565171</td>\n",
       "      <td>8.032252</td>\n",
       "      <td>2.815738</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CRNNLearner(db,rnn_type='indrnn').fit(1,3e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(TCN, keep=True)\n",
    "def AR_TCNLearner(db,hl_depth=3,alpha=1,beta=1,early_stop=0,metrics=None,n_skip=None,**kwargs):\n",
    "    n_skip = 2**hl_depth if n_skip is None else n_skip\n",
    "    skip = partial(SkipNLoss,n_skip=n_skip)\n",
    "    \n",
    "    inp,out = get_inp_out_size(db)\n",
    "    model = AR_Model(TCN(inp+out,out,hl_depth,**kwargs),ar=False,rf=n_skip)\n",
    "    model.init_normalize(db.one_batch())\n",
    "    \n",
    "    cbs=[ARInitCB(),TimeSeriesRegularizer(alpha=alpha,beta=beta,modules=[model.model.conv_layers[-1]]),SaveModelCallback()]\n",
    "    if early_stop > 0:\n",
    "        cbs += [EarlyStoppingCallback(patience=early_stop)]\n",
    "        \n",
    "    if metrics is None: metrics=SkipNLoss(fun_rmse,n_skip)\n",
    "        \n",
    "    lrn = Learner(db,model,loss_func=nn.MSELoss(),opt_func=ranger,metrics=metrics,cbs=cbs)\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(AR_RNN, keep=True)\n",
    "def AR_RNNLearner(db,alpha=0,beta=0,early_stop=0,metrics=None,n_skip=0,fname='model',**kwargs):\n",
    "    skip = partial(SkipNLoss,n_skip=n_skip)\n",
    "    \n",
    "    inp,out = get_inp_out_size(db)\n",
    "    model = AR_Model(AR_RNN(inp+out,out,**kwargs),ar=False,hs=True)\n",
    "    model.init_normalize(db.one_batch())\n",
    "    \n",
    "    cbs=[ARInitCB(),TimeSeriesRegularizer(alpha=alpha,beta=beta,modules=[model.model.rnn]),SaveModelCallback()]\n",
    "    if early_stop > 0:\n",
    "        cbs += [EarlyStoppingCallback(patience=early_stop)]\n",
    "        \n",
    "    if metrics is None: metrics=SkipNLoss(fun_rmse,n_skip)\n",
    "        \n",
    "    lrn = Learner(db,model,loss_func=nn.MSELoss(),opt_func=ranger,metrics=metrics,cbs=cbs)\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_models.ipynb.\n",
      "Converted 01a_IndRNN.ipynb.\n",
      "Converted 02_learner.ipynb.\n",
      "Converted 03_dataloaders.ipynb.\n",
      "Converted 11_dualrnn.ipynb.\n",
      "Converted 12_TensorQuaternions.ipynb.\n",
      "Converted 13_HPOpt.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
