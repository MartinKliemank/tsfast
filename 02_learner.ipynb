{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp learner\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from seqdata.core import *\n",
    "from seqdata.models.core import *\n",
    "from fastai.basics import *\n",
    "from fastai.callback.progress import *\n",
    "from fastai.callback.tracker import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner\n",
    "> Pytorch Modules for Training Models for sequential data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = DataBlock(blocks=(SequenceBlock.from_hdf(['current','voltage'],TensorSequencesInput,clm_shift=[-1,-1]),\n",
    "                        SequenceBlock.from_hdf(['voltage'],TensorSequencesOutput,clm_shift=[1])),\n",
    "                 get_items=CreateDict([DfHDFCreateWindows(win_sz=1000+1,stp_sz=1000,clm='current')]),\n",
    "                 splitter=ApplyToDict(ParentSplitter()))\n",
    "db = seq.dataloaders(get_hdf_files('test_data/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>12.698597</td>\n",
       "      <td>11.738781</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SimpleRNN(2,1)\n",
    "lrn = Learner(db,model,loss_func=nn.MSELoss()).fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GradientClipping(Callback):\n",
    "    \"`Callback` cutts of the gradient of every minibtch at `clip_val`\"\n",
    "    def __init__(self, clip_val=10): self.clip_val = clip_val\n",
    "\n",
    "    def after_backward(self):\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8.170834</td>\n",
       "      <td>3.417894</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),cbs=GradientClipping(10)).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class WeightClipping(Callback):\n",
    "    \"`Callback` that clips the weights of a given module at `clip_limit` after every iteration\"\n",
    "    def __init__(self, module, clip_limit = 1):\n",
    "        self.module = module\n",
    "        self.clip_limit = clip_limit\n",
    "\n",
    "    def after_batch(self):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        for p in self.module.parameters():\n",
    "            p.data.clamp_(-self.clip_limit,self.clip_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.220595</td>\n",
       "      <td>0.277792</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),cbs=WeightClipping(model,clip_limit=1)).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SkipFirstNCallback(Callback):\n",
    "    \"`Callback` skips first n samples from prediction and target, optionally `with_loss`\"\n",
    "    def __init__(self, n_skip = 0):\n",
    "        self.n_skip = n_skip\n",
    "\n",
    "    def after_pred(self):\n",
    "        if self.training:\n",
    "            dl = self.learn.dls.train\n",
    "            if (hasattr(dl,'rnn_reset') and dl.rnn_reset) or not hasattr(dl,'rnn_reset'): # if tbptt is used, only skip loss in the first minibatch\n",
    "                self.learn.pred = self.pred[:,self.n_skip:]\n",
    "        #         import pdb; pdb.set_trace()\n",
    "                if isinstance(self.yb, tuple):\n",
    "                    self.learn.yb = tuple([y[:,self.n_skip:] for y in self.yb])\n",
    "                else:\n",
    "                    self.learn.yb = self.yb[:,self.n_skip:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VarySeqLen(Callback):\n",
    "    \"`Callback` varies sequence length of every mini batch\"\n",
    "    def __init__(self, min_len = 50):\n",
    "        self.min_len = min_len\n",
    "\n",
    "    def before_batch(self):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        lx = self.xb[0].shape[1]\n",
    "        ly = self.yb[0].shape[1]\n",
    "        lim = random.randint(self.min_len,ly)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        if ly < lx:\n",
    "            self.learn.xb = tuple([x[:,:-(ly-lim)] for x in self.xb])\n",
    "        else:\n",
    "            self.learn.xb = tuple([x[:,:lim] for x in self.xb])\n",
    "            \n",
    "        self.learn.yb = tuple([y[:,:lim] for y in self.yb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.272021</td>\n",
       "      <td>0.260153</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),cbs=VarySeqLen(10)).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.callback.hook import *\n",
    "@delegates()\n",
    "class TimeSeriesRegularizer(HookCallback):\n",
    "    \"Callback that adds AR and TAR to the loss, calculated by output of provided layer\"\n",
    "    run_before=TrainEvalCallback\n",
    "    def __init__(self,alpha=0.0, beta=0.0,dim = None,detach=False, **kwargs):\n",
    "        super().__init__(detach=detach,**kwargs)\n",
    "        store_attr('alpha,beta,dim')\n",
    "        \n",
    "    def hook(self, m, i, o): \n",
    "#         import pdb; pdb.set_trace()\n",
    "        if isinstance(o,torch.Tensor):\n",
    "            self.out = o\n",
    "        else:\n",
    "            self.out = o[0]\n",
    "        \n",
    "        #find time axis if not already provided\n",
    "        if self.dim is None:\n",
    "            self.dim = np.argmax([0,self.out.shape[1],self.out.shape[2]])\n",
    "    \n",
    "    def after_loss(self):\n",
    "        if not self.training: return\n",
    "        \n",
    "        h = self.out.float()\n",
    "        \n",
    "        if self.alpha != 0.:  \n",
    "            l_a = float(self.alpha) * h.pow(2).mean()\n",
    "            self.learn.loss = self.learn.loss+l_a \n",
    "            \n",
    "        if self.beta != 0. and h.shape[self.dim]>1:\n",
    "            h_diff = (h[:,1:] - h[:,:-1]) if self.dim == 1 else (h[:,:,1:] - h[:,:,:-1])\n",
    "            l_b = float(self.beta) * h_diff.pow(2).mean()\n",
    "            self.learn.loss = self.learn.loss+l_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ARInitCB(Callback):\n",
    "    '''Adds the target variable to the input tuple for autoregression'''\n",
    "    def before_batch(self):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        self.learn.xb = tuple([*self.xb,*self.yb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.146504</td>\n",
       "      <td>0.108184</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss()).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from matplotlib.lines import Line2D\n",
    "def plot_grad_flow(named_parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    *modified version of https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063/8*\n",
    "    \n",
    "    Call multiple time for transparent overlays, representing the mean gradients\n",
    "    '''\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "#             pdb.set_trace()\n",
    "            ave_grads.append(0 if p.grad is None else p.grad.abs().mean())\n",
    "            max_grads.append(0 if p.grad is None else p.grad.abs().max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"Gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class CB_PlotGradient(Callback):\n",
    "    '''Plot the Gradient Distribution for every trainable parameter'''\n",
    "    \n",
    "    def __init__(self, n_draws=20): self.n_draws = n_draws\n",
    "    \n",
    "    def begin_fit(self):\n",
    "        '''Create a new figure to plot in'''\n",
    "        plt.figure()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    def after_backward(self):\n",
    "        '''plot the gradient for every layer of the current minibatch'''\n",
    "        # plotting n_draws times at the whole training\n",
    "        if self.iter % (max(self.n_epoch*self.n_iter//self.n_draws,1)) == 0:\n",
    "#         if self.iter == self.n_iter-1:\n",
    "            plot_grad_flow(self.learn.model.named_parameters())\n",
    "#             print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.127609</td>\n",
       "      <td>0.093049</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),cbs=CB_PlotGradient()).fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import functools\n",
    "\n",
    "def ignore_nan(func):\n",
    "    '''remove nan values from tensors before function execution, reduces tensor to a flat array, apply to functions such as mse'''\n",
    "    @functools.wraps(func)\n",
    "    def ignore_nan_decorator(*args, **kwargs):\n",
    "#         mask = ~torch.isnan(args[-1]) #nan mask of target tensor\n",
    "#         args = tuple([x[mask] for x in args]) #remove nan values\n",
    "        mask = ~torch.isnan(args[-1][...,-1]) #nan mask of target tensor\n",
    "        args = tuple([x[mask,:] for x in args]) #remove nan values\n",
    "        return func(*args, **kwargs)\n",
    "    return ignore_nan_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "y_t = torch.ones(32,n,6)\n",
    "y_t[:,20]=np.nan\n",
    "y_p = torch.ones(32,n,6)*1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1000, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~torch.isnan(y_t)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1000, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.isnan(mse(y_p,y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "mse_nan = ignore_nan(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(mse_nan(y_p,y_t),0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import functools\n",
    "\n",
    "def float64_func(func):\n",
    "    '''calculate function internally with float64 and convert the result back'''\n",
    "    @functools.wraps(func)\n",
    "    def float64_func_decorator(*args, **kwargs):\n",
    "        typ = args[0].dtype\n",
    "        args = tuple([x.double() if issubclass(type(x),Tensor ) else x for x in args]) #remove nan values\n",
    "        return func(*args, **kwargs).type(typ)\n",
    "    return float64_func_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.073600</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Float but expected Double\nException raised from compute_types at /pytorch/aten/src/ATen/native/TensorIterator.cpp:183 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7fe200dd61e2 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: at::TensorIterator::compute_types(at::TensorIteratorConfig const&) + 0x259 (0x7fe23ccc1849 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #2: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x6b (0x7fe23ccc4feb in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #3: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7fe23ccc565d in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #4: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x18a (0x7fe23cb2a2ba in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: <unknown function> + 0xf2b190 (0x7fe20216a190 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x90 (0x7fe23cb26ce0 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0xf2b230 (0x7fe20216a230 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #8: <unknown function> + 0xf4d4b6 (0x7fe20218c4b6 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #9: at::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x119 (0x7fe23cfe9949 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0x2e03469 (0x7fe23ec42469 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0xa9ac76 (0x7fe23c8d9c76 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: at::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x119 (0x7fe23cfe9949 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1af (0x7fe23eb7e0cf in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: <unknown function> + 0x3375bb7 (0x7fe23f1b4bb7 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7fe23f1b0400 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7fe23f1b0fa1 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7fe23f1a9119 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7fe258b3cdea in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #19: <unknown function> + 0xc819d (0x7fe26b87d19d in /home/daniel/miniconda3/envs/fastaiv2/bin/../lib/libstdc++.so.6)\nframe #20: <unknown function> + 0x76db (0x7fe26f09e6db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #21: clone + 0x3f (0x7fe26edc7a3f in /lib/x86_64-linux-gnu/libc.so.6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6b30b6a11beb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat64_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastcore/logargs.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0minit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'init_args'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minst\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mto_return\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_hypers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelFitException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end_cleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_end_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;34m;\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m;\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelEpochException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mlog_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cbs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;34m;\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m;\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelTrainException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;34m;\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m;\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_one_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelBatchException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;34m;\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m;\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'before_backward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_backward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Float but expected Double\nException raised from compute_types at /pytorch/aten/src/ATen/native/TensorIterator.cpp:183 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7fe200dd61e2 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: at::TensorIterator::compute_types(at::TensorIteratorConfig const&) + 0x259 (0x7fe23ccc1849 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #2: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x6b (0x7fe23ccc4feb in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #3: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7fe23ccc565d in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #4: at::native::mse_loss_backward_out(at::Tensor&, at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x18a (0x7fe23cb2a2ba in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: <unknown function> + 0xf2b190 (0x7fe20216a190 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: at::native::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x90 (0x7fe23cb26ce0 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0xf2b230 (0x7fe20216a230 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #8: <unknown function> + 0xf4d4b6 (0x7fe20218c4b6 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #9: at::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x119 (0x7fe23cfe9949 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0x2e03469 (0x7fe23ec42469 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0xa9ac76 (0x7fe23c8d9c76 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: at::mse_loss_backward(at::Tensor const&, at::Tensor const&, at::Tensor const&, long) + 0x119 (0x7fe23cfe9949 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: torch::autograd::generated::MseLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1af (0x7fe23eb7e0cf in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: <unknown function> + 0x3375bb7 (0x7fe23f1b4bb7 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7fe23f1b0400 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7fe23f1b0fa1 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7fe23f1a9119 in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7fe258b3cdea in /home/daniel/miniconda3/envs/fastaiv2/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #19: <unknown function> + 0xc819d (0x7fe26b87d19d in /home/daniel/miniconda3/envs/fastaiv2/bin/../lib/libstdc++.so.6)\nframe #20: <unknown function> + 0x76db (0x7fe26f09e6db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #21: clone + 0x3f (0x7fe26edc7a3f in /lib/x86_64-linux-gnu/libc.so.6)\n"
     ]
    }
   ],
   "source": [
    "Learner(db,model,loss_func=float64_func(nn.MSELoss())).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SkipNLoss(fn,n_skip=0):\n",
    "    '''Loss-Function modifier that skips the first n samples of sequential data'''\n",
    "    @functools.wraps(fn)\n",
    "    def _inner( input, target):\n",
    "        return fn(input[:,n_skip:],target[:,n_skip:])\n",
    "    \n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.092071</td>\n",
       "      <td>0.059101</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=SkipNLoss(nn.MSELoss(),n_skip=30)).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fun_rmse(inp, targ): \n",
    "    '''rmse loss function defined as a function not as a AccumMetric'''\n",
    "    return torch.sqrt(F.mse_loss(inp, targ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.095200</td>\n",
       "      <td>0.074323</td>\n",
       "      <td>0.197204</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),metrics=SkipNLoss(fun_rmse,n_skip=30)).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def norm_rmse(inp, targ):\n",
    "    '''rmse loss function defined as a function not as a AccumMetric'''\n",
    "    return fun_rmse(inp, targ)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>norm_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.090974</td>\n",
       "      <td>0.063468</td>\n",
       "      <td>17.684565</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),metrics=SkipNLoss(norm_rmse,n_skip=30)).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mean_vaf(inp,targ):\n",
    "    return (1-((targ-inp).var()/targ.var()))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_vaf</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.079336</td>\n",
       "      <td>0.059446</td>\n",
       "      <td>89.616798</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),metrics=SkipNLoss(mean_vaf,n_skip=30)).fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Learner Models\n",
    "Create Learner with different kinds of models with fitting Parameters and regularizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_inp_out_size(db):\n",
    "    '''returns input and output size of a timeseries databunch'''\n",
    "    tup = db.one_batch()\n",
    "    inp = tup[0].shape[-1]\n",
    "    out = tup[1].shape[-1]\n",
    "    return inp,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(get_inp_out_size(db),(2,1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Learner\n",
    "The Learners include model specific optimizations. Removing the first n_skip samples of the loss function of transient time, greatly improves training stability. In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(SimpleRNN, keep=True)\n",
    "def RNNLearner(db,loss_func=nn.MSELoss(),metrics=[fun_rmse],n_skip=0,cbs=None,**kwargs):\n",
    "    inp,out = get_inp_out_size(db)\n",
    "    model = SimpleRNN(inp,out,**kwargs)\n",
    "  \n",
    "    skip = partial(SkipNLoss,n_skip=n_skip)\n",
    "        \n",
    "    metrics= [skip(f) for f in metrics]\n",
    "    loss_func = skip(loss_func)\n",
    "        \n",
    "    lrn = Learner(db,model,loss_func=loss_func,opt_func=ranger,metrics=metrics,cbs=cbs)\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>14.619395</td>\n",
       "      <td>14.713695</td>\n",
       "      <td>3.821392</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndRNN loaded for gpu cuda:0\n"
     ]
    }
   ],
   "source": [
    "RNNLearner(db,rnn_type='indrnn').fit(1,1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN Learner\n",
    "Performs better on multi input data. Higher beta values allow a way smoother prediction. Way faster then RNNs in prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(TCN, keep=True)\n",
    "def TCNLearner(db,hl_depth=3,loss_func=nn.MSELoss(),metrics=[fun_rmse],n_skip=0,cbs=None,**kwargs):\n",
    "    inp,out = get_inp_out_size(db)\n",
    "    n_skip = 2**hl_depth if n_skip is None else n_skip\n",
    "    model = TCN(inp,out,hl_depth,**kwargs)\n",
    "  \n",
    "    skip = partial(SkipNLoss,n_skip=n_skip)\n",
    "        \n",
    "    metrics= [skip(f) for f in metrics]\n",
    "    loss_func = skip(loss_func)\n",
    "        \n",
    "    lrn = Learner(db,model,loss_func=loss_func,opt_func=ranger,metrics=metrics,cbs=cbs)\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>12.552464</td>\n",
       "      <td>12.902330</td>\n",
       "      <td>3.535440</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TCNLearner(db).fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRNN Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(CRNN, keep=True)\n",
    "def CRNNLearner(db,loss_func=nn.MSELoss(),metrics=[fun_rmse],n_skip=0,cbs=None,**kwargs):\n",
    "    inp,out = get_inp_out_size(db)\n",
    "    model = CRNN(inp,out,**kwargs)\n",
    "  \n",
    "    skip = partial(SkipNLoss,n_skip=n_skip)\n",
    "        \n",
    "    metrics= [skip(f) for f in metrics]\n",
    "    loss_func = skip(loss_func)\n",
    "        \n",
    "    lrn = Learner(db,model,loss_func=loss_func,opt_func=ranger,metrics=metrics,cbs=cbs)\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CRNNLearner(db,rnn_type='indrnn').fit(1,3e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(TCN, keep=True)\n",
    "def AR_TCNLearner(db,hl_depth=3,alpha=1,beta=1,early_stop=0,metrics=None,n_skip=None,**kwargs):\n",
    "    n_skip = 2**hl_depth if n_skip is None else n_skip\n",
    "    skip = partial(SkipNLoss,n_skip=n_skip)\n",
    "    \n",
    "    inp,out = get_inp_out_size(db)\n",
    "    model = AR_Model(TCN(inp+out,out,hl_depth,**kwargs),ar=False,rf=n_skip)\n",
    "    model.init_normalize(db.one_batch())\n",
    "    \n",
    "    cbs=[ARInitCB(),TimeSeriesRegularizer(alpha=alpha,beta=beta,modules=[model.model.conv_layers[-1]]),SaveModelCallback()]\n",
    "    if early_stop > 0:\n",
    "        cbs += [EarlyStoppingCallback(patience=early_stop)]\n",
    "        \n",
    "    if metrics is None: metrics=SkipNLoss(fun_rmse,n_skip)\n",
    "        \n",
    "    lrn = Learner(db,model,loss_func=nn.MSELoss(),opt_func=ranger,metrics=metrics,cbs=cbs)\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(SimpleRNN, keep=True)\n",
    "def AR_RNNLearner(db,alpha=0,beta=0,early_stop=0,metrics=None,n_skip=0,fname='model',**kwargs):\n",
    "    skip = partial(SkipNLoss,n_skip=n_skip)\n",
    "    \n",
    "    inp,out = get_inp_out_size(db)\n",
    "    model = AR_Model(SimpleRNN(inp+out,out,**kwargs),ar=False,hs=True)\n",
    "    model.init_normalize(db.one_batch())\n",
    "    \n",
    "    cbs=[ARInitCB(),TimeSeriesRegularizer(alpha=alpha,beta=beta,modules=[model.model.rnn]),SaveModelCallback()]\n",
    "    if early_stop > 0:\n",
    "        cbs += [EarlyStoppingCallback(patience=early_stop)]\n",
    "        \n",
    "    if metrics is None: metrics=SkipNLoss(fun_rmse,n_skip)\n",
    "        \n",
    "    lrn = Learner(db,model,loss_func=nn.MSELoss(),opt_func=ranger,metrics=metrics,cbs=cbs)\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_models.ipynb.\n",
      "Converted 01a_IndRNN.ipynb.\n",
      "Converted 02_learner.ipynb.\n",
      "Converted 03_dataloaders.ipynb.\n",
      "Converted 11_dualrnn.ipynb.\n",
      "Converted 12_TensorQuaternions.ipynb.\n",
      "Converted 13_HPOpt.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
