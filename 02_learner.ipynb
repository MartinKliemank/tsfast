{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp learner\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from seqdata.core import *\n",
    "from seqdata.models.core import *\n",
    "from fastai2.basics import *\n",
    "from fastai2.callback.progress import *\n",
    "from fastai2.callback.tracker import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner\n",
    "> Pytorch Modules for Training Models for sequential data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = DataBlock(blocks=(SequenceBlock.from_hdf(['current','voltage'],TensorSequencesInput,clm_shift=[-1,-1]),\n",
    "                        SequenceBlock.from_hdf(['voltage'],TensorSequencesOutput,clm_shift=[1])),\n",
    "                 get_items=CreateDict([DfHDFCreateWindows(win_sz=1000+1,stp_sz=1000,clm='current')]),\n",
    "                 splitter=ApplyToDict(ParentSplitter()))\n",
    "db = seq.dataloaders(get_hdf_files('test_data/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>12.830972</td>\n",
       "      <td>11.714725</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SimpleRNN(2,1)\n",
    "lrn = Learner(db,model,loss_func=nn.MSELoss()).fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GradientClipping(Callback):\n",
    "    \"`Callback` cutts of the gradient of every minibtch at `clip_val`\"\n",
    "    def __init__(self, clip_val=10): self.clip_val = clip_val\n",
    "\n",
    "    def after_backward(self):\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class WeightClipping(Callback):\n",
    "    \"`Callback` that clips the weights of a given module at `clip_limit` after every iteration\"\n",
    "    def __init__(self, module, clip_limit = 1):\n",
    "        self.module = module\n",
    "        self.clip_limit = clip_limit\n",
    "\n",
    "    def after_batch(self):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        for p in self.module.parameters():\n",
    "            p.data.clamp_(-self.clip_limit,self.clip_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SkipFirstNCallback(Callback):\n",
    "    \"`Callback` skips first n samples from prediction and target, optionally `with_loss`\"\n",
    "    def __init__(self, n_skip = 0):\n",
    "        self.n_skip = n_skip\n",
    "\n",
    "    def after_pred(self):\n",
    "        self.learn.pred = self.pred[:,self.n_skip:]\n",
    "#         import pdb; pdb.set_trace()\n",
    "        if isinstance(self.yb, tuple):\n",
    "            self.learn.yb = tuple([y[:,self.n_skip:] for y in self.yb])\n",
    "        else:\n",
    "            self.learn.yb = self.yb[:,self.n_skip:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VarySeqLen(Callback):\n",
    "    \"`Callback` varies sequence length of every mini batch\"\n",
    "    def __init__(self, min_len = 50):\n",
    "        self.min_len = min_len\n",
    "\n",
    "    def begin_batch(self):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        lx = self.xb[0].shape[1]\n",
    "        ly = self.yb[0].shape[1]\n",
    "        lim = random.randint(self.min_len,ly)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        if ly < lx:\n",
    "            self.learn.xb = tuple([x[:,:-(ly-lim)] for x in self.xb])\n",
    "        else:\n",
    "            self.learn.xb = tuple([x[:,:lim] for x in self.xb])\n",
    "            \n",
    "        self.learn.yb = tuple([y[:,:lim] for y in self.yb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.361042</td>\n",
       "      <td>4.533768</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),cbs=VarySeqLen(10)).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.callback.hook import *\n",
    "@delegates()\n",
    "class TimeSeriesRegularizer(HookCallback):\n",
    "    \"Callback that adds AR and TAR to the loss, calculated by output of provided layer\"\n",
    "    run_before=TrainEvalCallback\n",
    "    def __init__(self,alpha=0.0, beta=0.0,dim = None,detach=False, **kwargs):\n",
    "        super().__init__(detach=detach,**kwargs)\n",
    "        store_attr(self,'alpha,beta,dim')\n",
    "        \n",
    "    def hook(self, m, i, o): \n",
    "#         import pdb; pdb.set_trace()\n",
    "        if type(o) is torch.Tensor:\n",
    "            self.out = o\n",
    "        else:\n",
    "            self.out = o[0]\n",
    "        \n",
    "        #find time axis if not already provided\n",
    "        if self.dim is None:\n",
    "            self.dim = np.argmax([0,self.out.shape[1],self.out.shape[2]])\n",
    "    \n",
    "    def after_loss(self):\n",
    "        if not self.training: return\n",
    "        \n",
    "        h = self.out.float()\n",
    "        \n",
    "        if self.alpha != 0.:  \n",
    "            l_a = float(self.alpha) * h.pow(2).mean()\n",
    "            self.learn.loss = self.learn.loss+l_a \n",
    "            \n",
    "        if self.beta != 0. and h.shape[self.dim]>1:\n",
    "            h_diff = (h[:,1:] - h[:,:-1]) if self.dim == 1 else (h[:,:,1:] - h[:,:,:-1])\n",
    "            l_b = float(self.beta) * h_diff.pow(2).mean()\n",
    "            self.learn.loss = self.learn.loss+l_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ARInitCB(Callback):\n",
    "    '''Adds the target variable to the input tuple for autoregression'''\n",
    "    def begin_batch(self):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        self.learn.xb = tuple([*self.xb,*self.yb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.270075</td>\n",
       "      <td>0.300428</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss()).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from matplotlib.lines import Line2D\n",
    "def plot_grad_flow(named_parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    *modified version of https://discuss.pytorch.org/t/check-gradient-flow-in-network/15063/8*\n",
    "    \n",
    "    Call multiple time for transparent overlays, representing the mean gradients\n",
    "    '''\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "#             pdb.set_trace()\n",
    "            ave_grads.append(0 if p.grad is None else p.grad.abs().mean())\n",
    "            max_grads.append(0 if p.grad is None else p.grad.abs().max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"Gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CB_PlotGradient(Callback):\n",
    "    '''Plot the Gradient Distribution for every trainable parameter'''\n",
    "    def begin_fit(self,n_draws=20):\n",
    "        '''Create a new figure to plot in'''\n",
    "        self.n_draws =n_draws\n",
    "        plt.figure()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    def after_backward(self):\n",
    "        '''plot the gradient for every layer of the current minibatch'''\n",
    "        # plotting n_draws times at the whole training\n",
    "        if self.iter % (max(self.n_epoch*self.n_iter//self.n_draws,1)) == 0:\n",
    "#         if self.iter == self.n_iter-1:\n",
    "            plot_grad_flow(self.learn.model.named_parameters())\n",
    "#             print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.179843</td>\n",
       "      <td>0.091173</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),cbs=CB_PlotGradient()).fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import functools\n",
    "\n",
    "def ignore_nan(func):\n",
    "    '''remove nan values from tensors before function execution, reduces tensor to a flat array, apply to functions such as mse'''\n",
    "    @functools.wraps(func)\n",
    "    def ignore_nan_decorator(*args, **kwargs):\n",
    "#         mask = ~torch.isnan(args[-1]) #nan mask of target tensor\n",
    "#         args = tuple([x[mask] for x in args]) #remove nan values\n",
    "        mask = ~torch.isnan(args[-1][...,-1]) #nan mask of target tensor\n",
    "        args = tuple([x[mask,:] for x in args]) #remove nan values\n",
    "        return func(*args, **kwargs)\n",
    "    return ignore_nan_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "y_t = torch.ones(32,n,6)\n",
    "y_t[:,20]=np.nan\n",
    "y_p = torch.ones(32,n,6)*1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1000, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~torch.isnan(y_t)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1000, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.isnan(mse(y_p,y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "mse_nan = ignore_nan(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(mse_nan(y_p,y_t),0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import functools\n",
    "\n",
    "def float64_func(func):\n",
    "    '''calculate function internally with float64 and convert the result back'''\n",
    "    @functools.wraps(func)\n",
    "    def float64_func_decorator(*args, **kwargs):\n",
    "        typ = args[0].dtype\n",
    "        args = tuple([x.type(torch.float64) if issubclass(type(x),Tensor ) else x for x in args]) #remove nan values\n",
    "        return func(*args, **kwargs).type(typ)\n",
    "    return float64_func_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.078034</td>\n",
       "      <td>0.062594</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=float64_func(nn.MSELoss())).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SkipNLoss(fn,n_skip=0):\n",
    "    '''Loss-Function modifier that skips the first n samples of sequential data'''\n",
    "    @functools.wraps(fn)\n",
    "    def _inner( input, target):\n",
    "        return fn(input[:,n_skip:],target[:,n_skip:])\n",
    "    \n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.125872</td>\n",
       "      <td>0.094163</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=SkipNLoss(nn.MSELoss(),n_skip=30)).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fun_rmse(inp, targ): \n",
    "    '''rmse loss function defined as a function not as a AccumMetric'''\n",
    "    return torch.sqrt(F.mse_loss(inp, targ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.116710</td>\n",
       "      <td>0.092909</td>\n",
       "      <td>0.206432</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),metrics=SkipNLoss(fun_rmse,n_skip=30)).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def norm_rmse(inp, targ):\n",
    "    '''rmse loss function defined as a function not as a AccumMetric'''\n",
    "    return fun_rmse(inp, targ)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>norm_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.120062</td>\n",
       "      <td>0.094240</td>\n",
       "      <td>20.664818</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),metrics=SkipNLoss(norm_rmse,n_skip=30)).fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mean_vaf(inp,targ):\n",
    "    return (1-((targ-inp).var()/targ.var()))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>mean_vaf</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.109881</td>\n",
       "      <td>0.084180</td>\n",
       "      <td>86.037331</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Learner(db,model,loss_func=nn.MSELoss(),metrics=SkipNLoss(mean_vaf,n_skip=30)).fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Learner Models\n",
    "Create Learner with different kinds of models with fitting Parameters and regularizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_inp_out_size(db):\n",
    "    '''returns input and output size of a timeseries databunch'''\n",
    "    tup = db.one_batch()\n",
    "    inp = tup[0].shape[-1]\n",
    "    out = tup[1].shape[-1]\n",
    "    return inp,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(get_inp_out_size(db),(2,1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Learner\n",
    "The Learners include model specific optimizations. Removing the first n_skip samples of the loss function of transient time, greatly improves training stability. In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(SimpleRNN, keep=True)\n",
    "def RNNLearner(db,loss_func=nn.MSELoss(),metrics=[fun_rmse],n_skip=0,cbs=None,**kwargs):\n",
    "    inp,out = get_inp_out_size(db)\n",
    "    model = SimpleRNN(inp,out,**kwargs)\n",
    "  \n",
    "    skip = partial(SkipNLoss,n_skip=n_skip)\n",
    "        \n",
    "    metrics= [skip(f) for f in metrics]\n",
    "    loss_func = skip(loss_func)\n",
    "        \n",
    "    lrn = Learner(db,model,loss_func=loss_func,opt_func=ranger,metrics=metrics,cbs=cbs)\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndRNN loaded for gpu cuda:0\n"
     ]
    }
   ],
   "source": [
    "RNNLearner(db,rnn_type='indrnn').fit(1,1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN Learner\n",
    "Performs better on multi input data. Higher beta values allow a way smoother prediction. Way faster then RNNs in prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(TCN, keep=True)\n",
    "def TCNLearner(db,hl_depth=3,loss_func=nn.MSELoss(),metrics=[fun_rmse],n_skip=0,cbs=None,**kwargs):\n",
    "    inp,out = get_inp_out_size(db)\n",
    "    n_skip = 2**hl_depth if n_skip is None else n_skip\n",
    "    model = TCN(inp,out,hl_depth,**kwargs)\n",
    "  \n",
    "    skip = partial(SkipNLoss,n_skip=n_skip)\n",
    "        \n",
    "    metrics= [skip(f) for f in metrics]\n",
    "    loss_func = skip(loss_func)\n",
    "        \n",
    "    lrn = Learner(db,model,loss_func=loss_func,opt_func=ranger,metrics=metrics,cbs=cbs)\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11.619610</td>\n",
       "      <td>11.575568</td>\n",
       "      <td>3.400748</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TCNLearner(db).fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRNN Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(CRNN, keep=True)\n",
    "def CRNNLearner(db,loss_func=nn.MSELoss(),metrics=[fun_rmse],n_skip=0,cbs=None,**kwargs):\n",
    "    inp,out = get_inp_out_size(db)\n",
    "    model = CRNN(inp,out,**kwargs)\n",
    "  \n",
    "    skip = partial(SkipNLoss,n_skip=n_skip)\n",
    "        \n",
    "    metrics= [skip(f) for f in metrics]\n",
    "    loss_func = skip(loss_func)\n",
    "        \n",
    "    lrn = Learner(db,model,loss_func=loss_func,opt_func=ranger,metrics=metrics,cbs=cbs)\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>fun_rmse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>183.274597</td>\n",
       "      <td>3.166660</td>\n",
       "      <td>1.695407</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CRNNLearner(db,rnn_type='indrnn').fit(1,3e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(TCN, keep=True)\n",
    "def AR_TCNLearner(db,hl_depth=3,alpha=1,beta=1,early_stop=0,metrics=None,n_skip=None,**kwargs):\n",
    "    n_skip = 2**hl_depth if n_skip is None else n_skip\n",
    "    skip = partial(SkipNLoss,n_skip=n_skip)\n",
    "    \n",
    "    inp,out = get_inp_out_size(db)\n",
    "    model = AR_Model(TCN(inp+out,out,hl_depth,**kwargs),ar=False,rf=n_skip)\n",
    "    model.init_normalize(db.one_batch())\n",
    "    \n",
    "    cbs=[ARInitCB(),TimeSeriesRegularizer(alpha=alpha,beta=beta,modules=[model.model.conv_layers[-1]]),SaveModelCallback()]\n",
    "    if early_stop > 0:\n",
    "        cbs += [EarlyStoppingCallback(patience=early_stop)]\n",
    "        \n",
    "    if metrics is None: metrics=SkipNLoss(fun_rmse,n_skip)\n",
    "        \n",
    "    lrn = Learner(db,model,loss_func=nn.MSELoss(),opt_func=ranger,metrics=metrics,cbs=cbs)\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(AR_RNN, keep=True)\n",
    "def AR_RNNLearner(db,alpha=0,beta=0,early_stop=0,metrics=None,n_skip=0,fname='model',**kwargs):\n",
    "    skip = partial(SkipNLoss,n_skip=n_skip)\n",
    "    \n",
    "    inp,out = get_inp_out_size(db)\n",
    "    model = AR_Model(AR_RNN(inp+out,out,**kwargs),ar=False,hs=True)\n",
    "    model.init_normalize(db.one_batch())\n",
    "    \n",
    "    cbs=[ARInitCB(),TimeSeriesRegularizer(alpha=alpha,beta=beta,modules=[model.model.rnn]),SaveModelCallback()]\n",
    "    if early_stop > 0:\n",
    "        cbs += [EarlyStoppingCallback(patience=early_stop)]\n",
    "        \n",
    "    if metrics is None: metrics=SkipNLoss(fun_rmse,n_skip)\n",
    "        \n",
    "    lrn = Learner(db,model,loss_func=nn.MSELoss(),opt_func=ranger,metrics=metrics,cbs=cbs)\n",
    "    return lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_models.ipynb.\n",
      "Converted 01a_IndRNN.ipynb.\n",
      "Converted 02_learner.ipynb.\n",
      "Converted 03_dataloaders.ipynb.\n",
      "Converted 11_dualrnn.ipynb.\n",
      "Converted 12_TensorQuaternions.ipynb.\n",
      "Converted 13_HPOpt.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
