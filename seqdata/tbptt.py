#AUTOGENERATED! DO NOT EDIT! File to edit: dev/03_tbptt_dl.ipynb (unless otherwise specified).

__all__ = ['TbpttDl', 'TbpttResetCB']

#Cell
from .core import *
from .model import *
from .learner import *
from fastai2.basics import *

import math

#Cell
@delegates()
class TbpttDl(TfmdDL):

    def __init__(self, dataset, sub_seq_len=None, seq_len = None ,shuffle=True,num_workers=0, **kwargs):
        self.n_sub_seq = None
        super().__init__(dataset=dataset, shuffle=shuffle, num_workers=num_workers, **kwargs)
        store_attr(self,'sub_seq_len')

        if sub_seq_len is not None:
            if seq_len is None: seq_len = self.do_item(0)[0].shape[0]
            self.n_sub_seq = math.ceil(seq_len / sub_seq_len)

        self.rnn_reset = self.n_sub_seq is None #always reset stateful rnns if there are no subsequences

    def __len__(self):
        if self.n_sub_seq is None:
            return super().__len__()
        else:
            return super().__len__()*self.n_sub_seq

    def create_batches(self, samps):
        batch_iter = super().create_batches(samps)
        if self.n_sub_seq is None:
            return batch_iter
        else:
            return self._tbptt_generator(batch_iter)

    def _tbptt_generator(self,batch_iter):
        for b in batch_iter:
            tmp_b = b
            for i in range(self.n_sub_seq):
                self.rnn_reset = i == 0
                #it is importan to retain the tuple type, or future transforms may now work
                trunc_b = tuple([retain_type(x[:,i*self.sub_seq_len:(i+1)*self.sub_seq_len],x) for x in b])
                yield trunc_b

#Cell
class TbpttResetCB(Callback):
    "`Callback` resets the rnn model with every new sequence for tbptt"

    def begin_batch(self):
        dl = self.learn.dls.train if self.training else self.learn.dls.valid
#         if not self.training: import pdb; pdb.set_trace()
        if hasattr(dl,'rnn_reset')and dl.rnn_reset: self.model.reset()