#AUTOGENERATED! DO NOT EDIT! File to edit: dev/11_ProDiag.ipynb (unless otherwise specified).

__all__ = ['ProDiagTrainer', 'ProgDiag']

#Cell
from .core import *
from .model import *
from .learner import *
from fastai2.basics import *
from fastai2.callback.progress import *
from fastai2.callback.schedule import *
from fastai2.text.models.qrnn import *

#Cell
class ProDiagTrainer(Callback):
    "`Callback` that regroups lr adjustment to seq_len, AR and TAR."
    def __init__(self, init_size:int=50,alpha=1e6,beta=1):
        self.init_size = init_size
        self.alpha = alpha
        self.beta = beta

#     def begin_batch(self):
#         self.learn.yb = tuple([y[:,self.init_size:] for y in self.yb])

    def after_pred(self):
#         import pdb; pdb.set_trace()
        p,self.pred_diag,self.est_hidden,self.pred_hidden=self.pred
        self.learn.pred = p

    def after_loss(self):
        if not self.training: return
#         import pdb; pdb.set_trace()
        self.learn.loss += self.beta*self.learn.loss_func(self.pred_diag,*self.yb)

        hidden_loss = ((self.est_hidden-self.pred_hidden)/
                       (self.est_hidden.norm()+self.pred_hidden.norm())).pow(2).mean()
        self.learn.loss += self.alpha * hidden_loss

#Cell
class ProgDiag(nn.Module):
    def __init__(self,input_size,output_size,init_size,hidden_size=100,
                 rnn='gru',tcn_layer=0,rnn_layer=1,linear_layer = 1):
        super().__init__()

        self.out_sz = output_size
        self.init_size = init_size
        self.tcn_layer = tcn_layer

        rnn_kwargs = dict(hidden_size=hidden_size,num_layers=rnn_layer,rnn=rnn,ret_full_hidden=True)

        if tcn_layer > 0:
            self.diag_tcn = TCN(input_size,hidden_size,hl_depth=tcn_layer,hl_width=hidden_size)
            self.prog_tcn = TCN(input_size-output_size,hidden_size,hl_depth=tcn_layer,hl_width=hidden_size)

            self.diag_state = RNN(hidden_size,**rnn_kwargs)
            self.prog_state = RNN(hidden_size,**rnn_kwargs)
        else:
            self.diag_state = RNN(input_size,**rnn_kwargs)
            self.prog_state = RNN(input_size-output_size,**rnn_kwargs)

        self.estimator = SeqLinear(hidden_size,output_size,hidden_layer=linear_layer)

    def forward(self, x,init_state = None):
        #TCN Layer
        if self.tcn_layer > 0:
            x_diag = self.diag_tcn(x)
            x_prog = self.prog_tcn(x[:,:,:-self.out_sz])[:,self.init_size:]
        else:
            x_diag = x
            x_prog = x[:,self.init_size:,:-self.out_sz]

        #RNN Layer
        out_diag,h_diag = self.diag_state(x_diag)
        h_prog_init = h_diag[:,:,self.init_size-1,:]
#         import pdb; pdb.set_trace()
        out_prog,h_prog = self.prog_state(x_prog,h_prog_init)

        #Shared Linear Layer
        est_diag = self.estimator(out_diag[:,self.init_size:])
        est_prog = self.estimator(out_prog)

#         out = torch.cat([est_diag,est_prog],dim=2)
#         import pdb; pdb.set_trace()
        return est_prog,est_diag, h_diag[:,:,self.init_size:,:],h_prog