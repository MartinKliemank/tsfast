# AUTOGENERATED! DO NOT EDIT! File to edit: 03_dataloaders.ipynb (unless otherwise specified).

__all__ = ['TbpttDl', 'TbpttResetCB', 'WeightedDL_Factory', 'uniform_p_of_category']

# Cell
from .core import *
from .model import *
from .learner import *
from fastai2.basics import *

import math

# Cell
@delegates()
class TbpttDl(TfmdDL):

    def __init__(self, dataset, sub_seq_len=None,max_batches=None, seq_len = None ,shuffle=True,num_workers=0, **kwargs):
        store_attr(self,'sub_seq_len,max_batches,seq_len')
        super().__init__(dataset=dataset, shuffle=shuffle, num_workers=num_workers, **kwargs)

        self.rnn_reset = sub_seq_len is None #always reset stateful rnns if there are no subsequences
    @property
    def n_sub_seq(self):
        if self.seq_len is None: self.seq_len = self.do_item(0)[0].shape[0]
        return math.ceil(self.seq_len / self.sub_seq_len)

    def __len__(self):
        l = super().__len__()
        if self.sub_seq_len is not None: l *= self.n_sub_seq
        if self.max_batches is not None: l = min(l,self.max_batches)
        return l

    def create_batches(self, samps):
        yield from self._tbptt_generator(super().create_batches(samps))

    def _tbptt_generator(self,batch_iter):
        '''generator function that splits batches in smaller windows and truncates batch count if max_batches is set'''
        for idx,b in enumerate(batch_iter):
            if self.sub_seq_len is None:
                self.rnn_reset = True
                if self.max_batches is not None and idx >= self.max_batches: return
                yield b
            else:
                for i in range(self.n_sub_seq):
                    if self.max_batches is not None and ((idx*self.n_sub_seq)+i) >= self.max_batches: return
                    self.rnn_reset = i == 0
                    #it is importan to retain the tuple type, or future transforms may now work
                    trunc_b = tuple([retain_type(x[:,i*self.sub_seq_len:(i+1)*self.sub_seq_len],x) for x in b])
                    yield trunc_b


# Cell
class TbpttResetCB(Callback):
    "`Callback` resets the rnn model with every new sequence for tbptt"

    def begin_batch(self):
        dl = self.learn.dls.train if self.training else self.learn.dls.valid
#         if not self.training: import pdb; pdb.set_trace()
        if hasattr(dl,'rnn_reset')and dl.rnn_reset and hasattr(self.model,'reset'):
            self.model.reset()

    def after_fit(self):
        if hasattr(self.model,'reset'): self.model.reset()

# Cell
def WeightedDL_Factory(cls):
    '''
    Weighted Dataloader that provides control over sampling probabilities.
    wgts: probability array with probability for every item
            gets extracted from the pandas 'p_sample' column if given.
            Otherwise uniform sampling will be enabled

    '''
    assert issubclass(cls, TfmdDL)

    class WeightedDL(cls):
        def __init__(self, dataset, wgts=None,shuffle=True, **kwargs):
#             import pdb;pdb.set_trace()

            if wgts is None:
                self.wgts = array([1/(len(dataset))]*len(dataset))
                #self.items need to be assigned, but super.init needs wgts allready assigned
                super().__init__(dataset=dataset, shuffle=True, **kwargs)
                if  (type(self.items) is list and
                    type(self.items[0]) is dict and
                    'p_sample' in self.items[0].keys()):
                    self.wgts = np.array([x['p_sample'] for x in self.items])
                    self.wgts = self.wgts/self.wgts.sum()
                else:
                    print('No wgts provided for WeightedDL. Was that intentional?')
            else:
                self.wgts = wgts/np.sum(wgts)
                super().__init__(dataset=dataset, shuffle=True, **kwargs)

        def get_idxs(self):
            if self.n==0: return []
            if not self.shuffle: return super().get_idxs()
            return list(np.random.choice(self.n, self.n, p=self.wgts))
    return WeightedDL

# Cell
def uniform_p_of_category(cat_name):
    '''Scales sampling weights for an even distribution between every category'''
    def _inner(df):
        counts = df[cat_name].value_counts()
        sample_prob =  1/counts
        sample_prob.name = 'p_sample'
        return df.merge(sample_prob,left_on=cat_name,right_index=True)

    return _inner