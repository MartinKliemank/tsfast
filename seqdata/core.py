#AUTOGENERATED! DO NOT EDIT! File to edit: dev/00_core.ipynb (unless otherwise specified).

__all__ = ['get_hdf_files', 'hdf_extensions', 'apply_df_tfms', 'CreateDict', 'DfHDFCreateWindows', 'calc_shift_offsets',
           'hdf_extract_sequence', 'Memoize', 'HDF2Sequence', 'hdf2scalars', 'TensorSequences', 'TensorSequencesInput',
           'TensorSequencesOutput', 'toTensorSequencesInput', 'toTensorSequencesOutput', 'TensorScalars',
           'TensorScalarsInput', 'TensorScalarsOutput', 'SeqSlice', 'SeqNoiseInjection', 'encodes', 'decodes',
           'ParentSplitter', 'PercentageSplitter', 'ApplyToDict', 'pad_sequence', 'SequenceBlock', 'Seq2SeqDS',
           'Seq2SeqDataloaders', 'plot_seqs_single_figure', 'plot_seqs_multi_figures']

#Cell
from fastai2.data.all import *
import h5py

#Cell
hdf_extensions = ['.hdf5']
def get_hdf_files(path,recurse=True, folders=None):
    "Get hdf5 files in `path` recursively, only in `folders`, if specified."
    return get_files(path, extensions=hdf_extensions, recurse=recurse, folders=folders)

#Cell
def apply_df_tfms(f_list,pd_tfms = None):
    '''Create Pandas Dataframe out of a list of items, with a list of df transforms applied'''
    df = pd.DataFrame(data=f_list.items,columns=['path'])
    if pd_tfms is not None:
        for t in pd_tfms:
            df = t(df)
    return df

#Cell
def CreateDict(pd_tfms = None):
    '''Create List of Dictionarys out of a list of items, with a list of df transforms applied'''
    def _inner(f_list):
        return apply_df_tfms(f_list,pd_tfms).to_dict(orient='records')
    return _inner

#Cell
def DfHDFCreateWindows(win_sz,stp_sz, clm, fixed_start = False, fixed_end = False):
    '''create windows of sequences, splits sequence into multiple items'''
    def _inner(df):
        if fixed_start and fixed_end: raise Exception

        lst_df = [] #new dataframe for every row
        for idx, row in df.iterrows():
            with h5py.File(row.path,'r') as f:
                #TODO make clm optional
#                 if clm == '':
#                     clm = list(f.keys())[0]
                f_len = max(f[clm].shape)

                n_win = ((f_len-win_sz)//stp_sz)+1
                tmp_df = df.iloc[[idx]*n_win]; #duplicate the row of the df multiple times by reference
                lst_idx = np.arange(n_win)

                #every row is a reference so we need to suppress the warning messages while copying
                pd.options.mode.chained_assignment = None
                tmp_df['l_slc'] = None if fixed_start else lst_idx*stp_sz
                tmp_df['r_slc'] = None if fixed_end else lst_idx*stp_sz + win_sz
                pd.options.mode.chained_assignment = 'warn'

                lst_df.append(tmp_df)

        res_df = pd.concat(lst_df)
        return res_df

    return _inner

#Cell
def calc_shift_offsets(clm_shift):
    clm_shift = array(clm_shift)
    l_offs = -min(clm_shift.min(),0)
    r_offs = -max(clm_shift.max(),0)
    l_shift = clm_shift+l_offs
    r_shift = clm_shift+r_offs
    dim_red = l_offs-r_offs
    return l_shift,r_shift,dim_red

#Cell
def hdf_extract_sequence(hdf_path,clms,dataset = None, l_slc = None, r_slc= None):
    with h5py.File(hdf_path,'r') as f:
        ds = f if dataset is None else f[dataset]
        l_array = [ds[n][l_slc:r_slc] for n in clms]
        seq = np.vstack(l_array).T
        return seq

#Cell
class Memoize:
    def __init__(self, fn):
        self.fn = fn
        self.memo = {}

    def __call__(self, *args):
        if args not in self.memo:
            self.memo[args] = self.fn(*args)
        return self.memo[args]

#Cell
from functools import lru_cache

class HDF2Sequence(Transform):

    def __init__(self, clm_names,clm_shift=None,truncate_sz=None,to_cls=noop,cached=True):
        if not clm_shift is None:
            assert len(clm_shift)==len(clm_names) and all(isinstance(n, int) for n in clm_shift)
            self.l_shift,self.r_shift,_ = calc_shift_offsets(clm_shift)

        self._exseq = Memoize(self._hdf_extract_sequence) if cached else self._hdf_extract_sequence

        store_attr(self,'clm_names,clm_shift,truncate_sz,to_cls,cached')

    def _hdf_extract_sequence(self,hdf_path,dataset = None, l_slc = None, r_slc= None):
        with h5py.File(hdf_path,'r') as f:
            ds = f if dataset is None else f[dataset]
            l_array = [ds[n][l_slc:r_slc] for n in self.clm_names]
            seq = np.vstack(l_array).T
            return seq

    def _extract_dict_sequence(self,item):
        if isinstance(item,dict):
            path = item['path']
            dataset = item['dataset'] if 'dataset' in item else None
            l_slc = item['l_slc'] if 'l_slc' in item else None
            r_slc = item['r_slc'] if 'r_slc' in item else None

            if self.cached:
                seq = self._exseq(path,dataset)[l_slc:r_slc]
            else:
                seq = self._exseq(path,dataset,l_slc,r_slc)
        else:
            seq = self._exseq(str(item))

        #shift clms of result by given value
        if not self.clm_shift is None:
            l_seq = seq.shape[0]
            seq = np.vstack([seq[self.l_shift[i]:l_seq+self.r_shift[i],i] for i in range(seq.shape[1])]).T

        if not self.truncate_sz is None:
            seq = seq[truncate_sz:]

        #it is important to slice first and then do the class conversion
        return self.to_cls(seq)

    def encodes(self, item)->None:
        return self._extract_dict_sequence(item)

#Cell
def hdf2scalars(hdf_path,c_names):
    with h5py.File(hdf_path,'r') as f:
#         import pdb; pdb.set_trace()
#         l_array = [f[n][:][:,None] for n in c_names]
#         seq = np.concatenate(l_array,axis=1)
        return None

#Cell
class TensorSequences(Tensor):#TensorBase
#     def __init__(self,x,c_names=None, **kwargs):
#         super().__init__()
#         self.c_names = c_names

    def show(self, ctx=None, **kwargs):
#         import pdb; pdb.set_trace()
        ax = ctx
        if ax is None: _,ax = plt.subplots()
        ax.plot(self)
#         if title is not None: ax.set_title(title)
        return ax

    @classmethod
    def from_hdf(cls,clm_names,cached=True):
        return HDF2Sequence(clm_names=clm_names,cached=cached,to_cls=cls)

class TensorSequencesInput(TensorSequences): pass
class TensorSequencesOutput(TensorSequences): pass

#Cell
@Transform
def toTensorSequencesInput(o): return TensorSequencesInput(o)
@Transform
def toTensorSequencesOutput(o): return TensorSequencesOutput(o)

#Cell
class TensorScalars(Tensor): pass
class TensorScalarsInput(TensorScalars): pass
class TensorScalarsOutput(TensorScalars): pass

#Cell
class SeqSlice(Transform):
    '''Take a slice from an array-like object. Useful for e.g. shifting input and output'''
    def __init__(self, l_slc=None,r_slc=None):
        self.l_slc,self.r_slc = l_slc,r_slc

    def encodes(self, o): return o[self.l_slc:self.r_slc]

#Cell
class SeqNoiseInjection(Transform):
    split_idx=0
    '''Adds normal distributed noise to the tensor sequence with seperate mean and std for every signal'''
    def __init__(self, std=1e-1,mean=0.):
        self.std,self.mean = tensor(std),tensor(mean)

    def setups(self, dl:DataLoader):
        #check the tensor type of your input
        #TODO: include scalar type case
        x,*_ = dl.one_batch()
        self.std = to_device(self.std,x.device)
        self.mean = to_device(self.mean,x.device)

    def encodes(self, o:TensorSequencesInput):
        #expand creates a view on a tensor and is therefore very fast compared to copy
        return o+torch.normal(mean=self.mean.expand_as(o),
                              std=self.std.expand_as(o))

#Cell
@Normalize
def encodes(self, x:TensorSequencesInput):
    return (x-self.mean) / self.std

@Normalize
def decodes(self, x:TensorSequencesInput):
    f = to_cpu if x.device.type=='cpu' else noop
    return (x*f(self.std) + f(self.mean))

#Cell
def _parent_idxs(items, name): return mask2idxs(Path(o).parent.name == name for o in items)

def ParentSplitter(train_name='train', valid_name='valid'):
    "Split `items` from the parent folder names (`train_name` and `valid_name`)."
    def _inner(o, **kwargs):
        return _parent_idxs(o, train_name),_parent_idxs(o, valid_name)
    return _inner

#Cell
def PercentageSplitter(pct=0.8):
    "Split `items` in order in relative quantity."
    def _inner(o, **kwargs):
        split_idx=int(len(o)*pct)
        return L(range(split_idx)),L(range(split_idx,len(o)))
    return _inner

#Cell
def ApplyToDict(fn,key='path'):
    return lambda x:fn([i[key] for i in x])

#Cell
def pad_sequence(batch):
    '''collate_fn for padding of sequences of different lengths, use in before_batch of databunch, still quite slow'''
    #takes list of tuples as input, returns list of tuples
    sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)

    pad_func = partial(torch.nn.utils.rnn.pad_sequence,batch_first=True)
    padded_tensors = [pad_func([x[tup] for x in sorted_batch]) for tup in range(len(batch[0]))]
    padded_list = [retain_types(tuple([tup[entry] for tup in padded_tensors]),batch[0]) for entry in range(len(batch))]
    #retain types is important for decoding later back to source items
#     import pdb; pdb.set_trace()

    return padded_list

#Cell
class SequenceBlock(TransformBlock):
    def __init__(self, seq_extract,padding=False):
        return super().__init__(type_tfms=[seq_extract],
                                batch_tfms=[Normalize(axes=[0,1])],
                                dls_kwargs={} if not padding else {'before_batch': pad_sequence})

    @classmethod
    @delegates(HDF2Sequence, keep=True)
    def from_hdf(cls, clm_names, seq_cls=TensorSequencesInput,padding=False, **kwargs):
        return cls(HDF2Sequence(clm_names,to_cls=seq_cls,**kwargs), padding)


#Cell
from torch.utils.data import Dataset,ConcatDataset
class Seq2SeqDS(Dataset):
    #Datensatz aus HDF5 Datei

    def __init__(self, hdf_file, u_clms,y_clms, win_sz,stp_sz):
        super().__init__()
        store_attr(self,'u_clms,y_clms,win_sz,stp_sz')

        self.u = hdf_extract_sequence(hdf_file,u_clms)
        self.y = hdf_extract_sequence(hdf_file,y_clms)

#         if clm_shift is not None:
#             assert len(clm_shift[0])==len(u_clms) and len(clm_shift[1])==len(y_clms)
#             l_shift,r_shift,dim_red = calc_shift_offsets(clm_shift)

        self.len = (self.u.shape[0]-self.win_sz)//self.stp_sz or 1

    def __len__(self):
        return self.len

    def __getitem__(self, idx):
        if not (0 <= idx < len(self)): raise IndexError
#         import pdb; pdb.set_trace()
        i = idx*self.stp_sz + self.win_sz
        return (TensorSequencesInput(self.u[(i-self.win_sz+1):i+1]),
                TensorSequencesOutput(self.y[(i-self.win_sz+1):i+1]))

#Cell
@delegates(TfmdDL, keep=True)
def Seq2SeqDataloaders(items,u_clms,y_clms, win_sz,stp_sz=1,splitter=None,**kwargs):
    if splitter is None:
        lst_items = [items]
    else:
        lst_items = [items[idx] for idx in splitter(items)]

    dss = [ConcatDataset([Seq2SeqDS(f,u_clms,y_clms, win_sz,stp_sz) for f in itms])
          for itms in lst_items]
    if not 'after_batch' in kwargs: kwargs['after_batch'] = [to_device,Normalize(axes=[0,1])]

    dls = [TfmdDL(ds,shuffle=(i == 0),**kwargs) for i,ds in enumerate(dss)]
    return DataLoaders(*dls)

#Cell
def plot_seqs_single_figure(n_samples,samples,outs=None):
    rows=max(1,((n_samples-1) // 3)+1)
    cols=min(3,n_samples)
    fig = plt.figure(figsize=(9,2*cols))
    outer_grid = fig.add_gridspec(rows, cols)
#     import pdb; pdb.set_trace()
    n_targ = samples[0][1].shape[1]
    for i in range(n_samples):
        in_signals = samples[i][0]
        targ_signals = samples[i][1]
        if outs is not None: out_signals = outs[i][0]
        inner_grid = outer_grid[i].subgridspec(n_targ+1, 1)
        for j in range(n_targ):
            ax = fig.add_subplot(inner_grid[j])
            ax.plot(targ_signals[:,j])
            if outs is not None: ax.plot(out_signals[:,j])
            ax.label_outer()

        ax = fig.add_subplot(inner_grid[n_targ])
        ax.plot(in_signals)
    plt.tight_layout()

#Cell
def plot_seqs_multi_figures(n_samples,samples,outs=None):
    n_targ = samples[0][1].shape[1]
    for i in range(n_samples):
        fig = plt.figure(figsize=(9,3))
        axs = fig.subplots(nrows=n_targ+1,sharex=True)
        in_signals = samples[i][0]
        targ_signals = samples[i][1]
        if outs is not None:  out_signals = outs[i][0]
        for j in range(n_targ):
            ax = axs[j]
            ax.plot(targ_signals[:,j])
            if outs is not None:   ax.plot(out_signals[:,j])

        axs[-1].plot(in_signals)
        plt.tight_layout()

#Cell
@typedispatch
def show_batch(x:TensorSequences, y:TensorSequences, samples, ctxs=None, max_n=6, **kwargs):
    n_samples = min(len(samples), max_n)
    if n_samples > 3:
        #if there are more then 3 samples to plot then put them in a single figure
        plot_seqs_single_figure(n_samples,samples)
    else:
        #if there are less then 3 samples to plot then put each in its own figure
        plot_seqs_multi_figures(n_samples,samples)
    return ctxs

#Cell
@typedispatch
def show_results(x:TensorSequences, y:TensorSequences, samples, outs, ctxs=None, max_n=2, **kwargs):
    n_samples = min(len(samples), max_n)
    if n_samples > 3:
        #if there are more then 3 samples to plot then put them in a single figure
        plot_seqs_single_figure(n_samples,samples,outs)
    else:
        #if there are less then 3 samples to plot then put each in its own figure
        plot_seqs_multi_figures(n_samples,samples,outs)
    return ctxs