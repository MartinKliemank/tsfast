#AUTOGENERATED! DO NOT EDIT! File to edit: dev/02_learner.ipynb (unless otherwise specified).

__all__ = ['SkipFirstNCallback', 'VarySeqLen', 'TimeSeriesRegularizer', 'ProDiagTrainer', 'SkipNLoss', 'fun_rmse',
           'norm_rmse', 'get_inp_out_size', 'GRULearner', 'QRNNLearner', 'TCNLearner']

#Cell
from .core import *
from .model import *
from fastai2.basics import *
from fastai2.callback.progress import *
from fastai2.callback.tracker import *

#Cell
class SkipFirstNCallback(Callback):
    "`Callback` skips first n samples from prediction and target, optionally `with_loss`"
    def __init__(self, n_skip = 0):
        self.n_skip = n_skip

    def after_pred(self):
        self.learn.pred = self.pred[:,self.n_skip:]
#         import pdb; pdb.set_trace()
        if isinstance(self.yb, tuple):
            self.learn.yb = tuple([y[:,self.n_skip:] for y in self.yb])
        else:
            self.learn.yb = self.yb[:,self.n_skip:]


#Cell
class VarySeqLen(Callback):
    "`Callback` varies sequence length of every mini batch"
    def __init__(self, min_len = 50):
        self.min_len = min_len

    def begin_batch(self):
#         import pdb; pdb.set_trace()
        l_targ = self.xb[0].shape[1]
        lim = random.randint(self.min_len,l_targ)
#         if isinstance(self.xb, tuple):
        self.learn.xb = tuple([x[:,lim:] for x in self.xb])
#         else:
#             self.learn.xb = self.xb[:,lim:]

#         if isinstance(self.yb, tuple):
        self.learn.yb = tuple([y[:,lim:] for y in self.yb])
#         else:
#             self.learn.yb = self.yb[:,lim:]

#Cell
from fastai2.callback.hook import *
@delegates()
class TimeSeriesRegularizer(HookCallback):
    "Callback that adds AR and TAR to the loss, calculated by output of provided layer"
    run_before=TrainEvalCallback
    def __init__(self,alpha=0.0, beta=0.0,dim = None,detach=False, **kwargs):
        super().__init__(detach=detach,**kwargs)
        store_attr(self,'alpha,beta,dim')

    def hook(self, m, i, o):
#         import pdb; pdb.set_trace()
        if type(o) is torch.Tensor:
            self.out = o
        else:
            self.out = o[0]

        #find time axis if not already provided
        if self.dim is None:
            self.dim = np.argmax([0,self.out.shape[1],self.out.shape[2]])

    def after_loss(self):
        if not self.training: return

        h = self.out.float()

        if self.alpha != 0.:
            l_a = self.alpha * h.pow(2).mean()
            self.learn.loss += l_a

        if self.beta != 0. and h.shape[self.dim]>1:
            h_diff = (h[:,1:] - h[:,:-1]) if self.dim == 1 else (h[:,:,1:] - h[:,:,:-1])
            l_b = self.beta * h_diff.pow(2).mean()
            self.learn.loss += l_b

#Cell
class ProDiagTrainer(Callback):
    "`Callback` that regroups lr adjustment to seq_len, AR and TAR."
    def __init__(self, init_size:int=50,alpha=1e6):
        self.init_size = init_size
        self.alpha = alpha

#     def on_epoch_begin(self, **kwargs):
#         "Reset the hidden state of the model."
#         self.learn.model.reset()

    def begin_batch(self):
        self.learn.yb = tuple([y[:,self.init_size:].expand(-1,-1,2) for y in self.yb])

    def after_pred(self):
#         import pdb; pdb.set_trace()
        p,self.est_hidden,self.pred_hidden=self.pred
        self.learn.pred = p

    def after_loss(self):
        hidden_loss = ((self.est_hidden-self.pred_hidden)/
                       (self.est_hidden.norm()+self.pred_hidden.norm())).pow(2).mean()
        self.learn.loss += self.alpha * hidden_loss

#Cell
def SkipNLoss(fn,n_skip=0):
    '''Loss-Function modifier that skips the first n samples of sequential data'''
    def _inner( input, target):
        return fn(input[:,n_skip:],target[:,n_skip:])

    #checking if fn has the attribute name leads sometimes to false -> try_catch instead
    try:
        _inner.__name__ = fn.__name__
    except:
        pass

    return _inner

#Cell
def fun_rmse(inp, targ):
    '''rmse loss function defined as a function not as a AccumMetric'''
    return torch.sqrt(F.mse_loss(inp, targ))

#Cell
def norm_rmse(inp, targ):
    '''rmse loss function defined as a function not as a AccumMetric'''
    return fun_rmse(inp, targ)*100

#Cell
def get_inp_out_size(db):
    '''returns input and output size of a timeseries databunch'''
    tup = db.one_batch()
    inp = tup[0].shape[-1]
    out = tup[1].shape[-1]
    return inp,out

#Cell
@delegates(SimpleGRU, keep=True)
def GRULearner(db,alpha=1,beta=1,early_stop=0,metrics=None,n_skip=0,**kwargs):
    inp,out = get_inp_out_size(db)
    model = SimpleGRU(inp,out,**kwargs)

    cbs=[TimeSeriesRegularizer(alpha=alpha,beta=beta,modules=[model.rnn]),SaveModelCallback()]
    if early_stop > 0:
        cbs += [EarlyStoppingCallback(patience=early_stop)]

    if metrics is None: metrics=SkipNLoss(fun_rmse,n_skip)

    lrn = Learner(db,model,loss_func=nn.MSELoss(),opt_func=ranger,metrics=metrics,cbs=cbs)
    return lrn

#Cell
@delegates(SimpleQRNN, keep=True)
def QRNNLearner(db,alpha=1,beta=1,early_stop=0,metrics=None,n_skip=0,**kwargs):
    inp,out = get_inp_out_size(db)
    model = SimpleQRNN(inp,out,**kwargs)

    cbs=[TimeSeriesRegularizer(alpha=alpha,beta=beta,modules=[model.rnn]),SaveModelCallback()]
    if early_stop > 0:
        cbs += [EarlyStoppingCallback(patience=early_stop)]

    if metrics is None: metrics=SkipNLoss(fun_rmse,n_skip)

    lrn = Learner(db,model,loss_func=nn.MSELoss(),opt_func=ranger,metrics=metrics,cbs=cbs)
    return lrn

#Cell
@delegates(TCN, keep=True)
def TCNLearner(db,hl_depth=3,alpha=1,beta=1,early_stop=0,metrics=None,n_skip=None,**kwargs):
    n_skip = 2**hl_depth if n_skip is None else n_skip
    skip = partial(SkipNLoss,n_skip=n_skip)

    inp,out = get_inp_out_size(db)
    model = TCN(inp,out,hl_depth,**kwargs)

    cbs=[TimeSeriesRegularizer(alpha=alpha,beta=beta,modules=[model.conv_layers[-1]]),SaveModelCallback()]
    if early_stop > 0:
        cbs += [EarlyStoppingCallback(patience=early_stop)]

    if metrics is None: metrics=SkipNLoss(fun_rmse,n_skip)

    lrn = Learner(db,model,loss_func=skip(nn.MSELoss()),opt_func=ranger,metrics=metrics,cbs=cbs)
    return lrn