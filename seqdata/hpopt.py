# AUTOGENERATED! DO NOT EDIT! File to edit: 13_HPOpt.ipynb (unless otherwise specified).

__all__ = ['log_uniform', 'LearnerTrainable', 'learner_optimize', 'sample_config', 'CBRayReporter', 'HPOptimizer']

# Cell
from .core import *
from .model import *
from .learner import *
from fastai2.basics import *
from fastai2.callback.schedule import *
from fastai2.callback.rnn import *
from fastai2.callback.tracker import *

import ray
from ray import tune
from ray.tune import Trainable
from ray.tune.schedulers import *
from ray.tune.trial import ExportFormat

# Cell
def log_uniform(min_bound, max_bound, base=10):
    '''uniform sampling in an exponential range'''
    logmin = np.log(min_bound) / np.log(base)
    logmax = np.log(max_bound) / np.log(base)
    def _sample():
        return base**(np.random.uniform(logmin, logmax))
    return _sample

# Cell
class LearnerTrainable(tune.Trainable):

    def _setup(self, config):
        self.create_lrn = config['create_lrn']
        self.dls = ray.get(config['dls'])

        self.lrn = self.create_lrn(self.dls,config)

    def _train(self):
        with self.lrn.no_bar(): self.lrn.fit(1)
        train_loss,valid_loss,rmse = self.lrn.recorder.values[-1]
        return {'train_loss': train_loss,
                'valid_loss': valid_loss,
                'mean_loss': rmse}

    def _save(self, checkpoint_dir):
        checkpoint_path = os.path.join(checkpoint_dir, "model.pth")
        torch.save(self.lrn.model.state_dict(), checkpoint_path)
        return checkpoint_path

    def _restore(self, checkpoint_path):
        self.lrn.model.load_state_dict(torch.load(checkpoint_path))

    def _export_model(self, export_formats, export_dir):
        if export_formats == [ExportFormat.MODEL]:
            path = os.path.join(export_dir, "exported_model")
            torch.save(self.lrn.model.state_dict(), path)
            return {ExportFormat.MODEL: path}
        else:
            raise ValueError("unexpected formats: " + str(export_formats))

    # the learner class will be recreated with every perturbation, saving the model
    # that way the new hyperparameter will be applied
    def reset_config(self, new_config):
        model_state = self.lrn.model.state_dict()
#         import pdb; pdb.set_trace()
        self.lrn = self.create_lrn(self.dls,new_config)

        #restore trainable parameters, keeping the new hyperparameters in the model like dropout
        self.lrn.model.load_state_dict(model_state)

        self.config = new_config
        return True

# Cell
def learner_optimize(config):
        create_lrn = config['create_lrn']
        dls = ray.get(config['dls'])

        #Scheduling Parameters for training the Model
        lrn_kwargs = {'n_epoch':100,'pct_start':0.5}
        for attr in ['n_epoch','pct_start']:
            if attr in config: lrn_kwargs[attr] = config[attr]

        lrn = create_lrn(dls,config)
        lrn.lr = config['lr'] if 'lr' in config else 3e-3
        lrn.add_cb(CBRayReporter())
        with lrn.no_bar():
            config['fit_method'](lrn,**lrn_kwargs)

# Cell
def sample_config(config):
    ret_conf = config.copy()
    for k in ret_conf:
        ret_conf[k]=ret_conf[k]()
    return ret_conf

# Cell
class CBRayReporter(Callback):
    "`Callback` reports progress after every epoch to the ray tune logger"

    def after_epoch(self):
        train_loss,valid_loss,rmse = self.learn.recorder.values[-1]
        tune.track.log(train_loss=train_loss,
                        valid_loss=valid_loss,
                        mean_loss=rmse)


# Cell
class HPOptimizer():
    def __init__(self,create_lrn,dls):
        self.create_lrn = create_lrn
        self.dls = dls
        self.analysis = None

    @delegates(ray.init)
    def start_ray(self,**kwargs):
        ray.shutdown()
        ray.init(**kwargs)

    def stop_ray(self):
        ray.shutdown()




    @delegates(tune.run, keep=True)
    def optimize(self,config,resources_per_trial={"gpu": 1.0},verbose=1,**kwargs):
        config['create_lrn'] = self.create_lrn
        #dls are large objects, letting ray handle the copying process makes it much faster
        config['dls'] = ray.put(self.dls)

        if 'fit_method' not in config: config['fit_method'] = Learner.fit_flat_cos

        self.analysis = tune.run(
            learner_optimize,
            config=config,
            resources_per_trial=resources_per_trial,
            verbose=verbose,
            **kwargs)
        return self.analysis

    @delegates(tune.run, keep=True)
    def optimize_pbt(self,opt_name,num_samples,config,mut_conf,freq=2,
                 stop={"training_iteration": 40 },
                 resources_per_trial={"gpu": 1 },
                 resample_probability=0.25,
                 quantile_fraction=0.25,
                 **kwargs):
        self.mut_conf = mut_conf

        config['create_lrn'] = self.create_lrn
        #dls are large objects, letting ray handle the copying process makes it much faster
        config['dls'] = ray.put(self.dls)



        scheduler = PopulationBasedTraining(
        time_attr="training_iteration",
        metric="mean_loss",
        mode="min",
        perturbation_interval=freq,
        resample_probability=resample_probability,
        quantile_fraction=quantile_fraction,
        hyperparam_mutations=mut_conf)

        self.analysis = tune.run(
            LearnerTrainable,
            name=opt_name,
            scheduler=scheduler,
            reuse_actors=True,
            verbose=1,
            stop=stop,
            checkpoint_score_attr="mean_loss",
            checkpoint_freq=freq,
            keep_checkpoints_num=4,
            num_samples=num_samples,
            resources_per_trial=resources_per_trial,
            config=config,
            **kwargs)
        return self.analysis

    def best_model(self):
        if self.analysis is None: raise Exception
        model = self.create_lrn(self.dls,sample_config(self.mut_conf)).model
        f_path = self.analysis.get_best_trial('mean_loss').checkpoint.value
        model.load_state_dict(torch.load(f_path))
        return model